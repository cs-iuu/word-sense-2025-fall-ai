{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNPOylVQsRHYo3MNy/L79hB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cs-iuu/word-sense-2025-fall-ai/blob/main/notebooks/10.2.Embeddings_token_level_Mongolian.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10.2 Token-level Contextual Embeddings & Similarity (Mongolian)\n",
        "\n",
        "This notebook demonstrates how contextual embeddings can be created for Mongolian text.\n",
        "\n",
        "This notebook uses a multilingual sentence-transformer (MiniLM) tokenizer + transformer encoder to produce token-level contextual embeddings, and visualizes token-to-token similarities between two sentences.\n",
        "\n",
        "• Run in Google Colab (CPU is fine; GPU speeds up model loading & embedding).\n",
        "• If you want to process many sentences or large texts, consider batching and/or using a GPU runtime.\n",
        "• The model (paraphrase-multilingual-MiniLM-L12-v2) is multilingual and performs reasonably for Mongolian; for better Mongolian coverage you can swap in a Mongolian-specific mBERT variant if available."
      ],
      "metadata": {
        "id": "Gyy7tnorkYPD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install dependencies"
      ],
      "metadata": {
        "id": "l2ENjWdHksvn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LM8EPVsRkXQI"
      },
      "outputs": [],
      "source": [
        "# Install required libraries (run once)\n",
        "!pip install -q sentence-transformers transformers torch torchvision torchaudio --upgrade\n",
        "!pip install -q seaborn umap-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "0b-jeusAkzYi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "import textwrap\n",
        "\n",
        "# plotting defaults\n",
        "plt.rcParams['figure.figsize'] = (8,6)\n",
        "sns.set(style=\"whitegrid\")"
      ],
      "metadata": {
        "id": "rLHMzhvOkph9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load tokenizer and model"
      ],
      "metadata": {
        "id": "OiNR2Fvxk8rA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# hugging face multi-lingual model\n",
        "MODEL_NAME = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
        "\n",
        "# Option: use a HuggingFace transformer directly (e.g., bert-base-multilingual-cased)\n",
        "# if you want raw token embeddings from BERT layers.\n",
        "# HF_MODEL = \"bert-base-multilingual-cased\"\n",
        "\n",
        "print(\"Loading model:\", MODEL_NAME)\n",
        "sentence_model = SentenceTransformer(MODEL_NAME)  # for sentence pooling usage (not required for token-level)\n",
        "# But to get token-level hidden states, use AutoTokenizer + AutoModel\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "transformer = AutoModel.from_pretrained(MODEL_NAME)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "transformer.to(device)\n",
        "print(\"Using device:\", device)\n"
      ],
      "metadata": {
        "id": "8GdyO3FHk3Va"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test sentence"
      ],
      "metadata": {
        "id": "42nhGP1wlygY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample Mongolian sentences (you can edit these or paste your own).\n",
        "sent_a = \"Би сургуульд явж байна.\"\n",
        "sent_b = \"Сурагч сургуульд суралцдаг.\"\n",
        "\n",
        "print(\"Sentence A:\", sent_a)\n",
        "print(\"Sentence B:\", sent_b)\n"
      ],
      "metadata": {
        "id": "CapQNMKLl0Vy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_display(sentence):\n",
        "    # Tokenize with tokenizer (returns input_ids with special tokens)\n",
        "    encoded = tokenizer(sentence, return_tensors=\"pt\")\n",
        "    token_ids = encoded[\"input_ids\"][0].tolist()\n",
        "    tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
        "    # Show tokens and original text wrapping\n",
        "    print(\"Original sentence:\")\n",
        "    print(textwrap.fill(sentence, 80))\n",
        "    print(\"\\nTokens:\")\n",
        "    for i, t in enumerate(tokens):\n",
        "        print(f\"{i:02d}: {t}\")\n",
        "    return encoded, tokens\n",
        "\n",
        "enc_a, tokens_a = tokenize_and_display(sent_a)\n",
        "print(\"\\n---\\n\")\n",
        "enc_b, tokens_b = tokenize_and_display(sent_b)\n"
      ],
      "metadata": {
        "id": "LEU7kW9Kl63U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Token-level embeddings"
      ],
      "metadata": {
        "id": "dgjN6mm0mKX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def get_token_embeddings(encoded_inputs, model, device=\"cpu\", layer=-1):\n",
        "    \"\"\"\n",
        "    Returns token-level embeddings (last_hidden_state) for a single encoded input.\n",
        "    encoded_inputs: output of tokenizer(..., return_tensors='pt')\n",
        "    model: AutoModel\n",
        "    layer: which layer's outputs to use. -1 = last_hidden_state (after final layer)\n",
        "    \"\"\"\n",
        "    # move inputs to device\n",
        "    encoded_inputs = {k: v.to(device) for k,v in encoded_inputs.items()}\n",
        "    out = model(**encoded_inputs, output_hidden_states=True, return_dict=True)\n",
        "    # last_hidden_state shape: (batch, seq_len, hidden_dim)\n",
        "    # if you want a specific layer's outputs, use out.hidden_states[layer_index]\n",
        "    # hidden_states is tuple(layer0_embedding, layer1_output, ..., layerN_output)\n",
        "    if layer == -1:\n",
        "        token_embeddings = out.last_hidden_state.squeeze(0)  # (seq_len, hidden_dim)\n",
        "    else:\n",
        "        # pick layer index (0 is embedding, 1..N are transformer layer outputs)\n",
        "        token_embeddings = out.hidden_states[layer].squeeze(0)\n",
        "    return token_embeddings.cpu().numpy()\n",
        "\n",
        "emb_a = get_token_embeddings(enc_a, transformer, device=device, layer=-1)\n",
        "emb_b = get_token_embeddings(enc_b, transformer, device=device, layer=-1)\n",
        "\n",
        "print(\"Emb A shape:\", emb_a.shape)\n",
        "print(\"Emb B shape:\", emb_b.shape)\n"
      ],
      "metadata": {
        "id": "58rOzz50mBcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize token similarity heatmap"
      ],
      "metadata": {
        "id": "f9q4gKbYmUeK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_token_similarity(emb1, tokens1, emb2, tokens2, figsize=(10,8), cmap=\"viridis\"):\n",
        "    # Compute cosine similarity matrix (seq_len1 x seq_len2)\n",
        "    sim = cosine_similarity(emb1, emb2)  # shape (len(tokens1), len(tokens2))\n",
        "    plt.figure(figsize=figsize)\n",
        "    sns.heatmap(sim, xticklabels=tokens2, yticklabels=tokens1, cmap=cmap, annot=False)\n",
        "    plt.title(\"Token-to-Token Cosine Similarity\")\n",
        "    plt.xlabel(\"Sentence B tokens\")\n",
        "    plt.ylabel(\"Sentence A tokens\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    return sim\n",
        "\n",
        "sim_matrix = plot_token_similarity(emb_a, tokens_a, emb_b, tokens_b)\n",
        "print(\"Similarity matrix shape:\", sim_matrix.shape)\n"
      ],
      "metadata": {
        "id": "kgOq2krBmOzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ignore special tokens & group subwords for word-level mapping"
      ],
      "metadata": {
        "id": "XutFTsZMmpdU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper to drop special tokens (like [CLS], [SEP]) and group subwords to reconstruct word-level embeddings\n",
        "def tokens_to_words(tokens):\n",
        "    \"\"\"\n",
        "    Returns a tuple (word_list, token_index_groups)\n",
        "    token_index_groups is a list of lists; each inner list indexes tokens belonging to that word.\n",
        "    This tries to merge '##' subword tokens (WordPiece style) into single words.\n",
        "    \"\"\"\n",
        "    words = []\n",
        "    groups = []\n",
        "    current = []\n",
        "    current_word = \"\"\n",
        "    for idx, t in enumerate(tokens):\n",
        "        # skip special tokens\n",
        "        if t in tokenizer.all_special_tokens:\n",
        "            if current:\n",
        "                groups.append(current)\n",
        "                words.append(current_word)\n",
        "                current, current_word = [], \"\"\n",
        "            continue\n",
        "\n",
        "        # WordPiece style: tokens that start with '##' are continuations\n",
        "        if t.startswith(\"##\"):\n",
        "            current.append(idx)\n",
        "            current_word += t.replace(\"##\", \"\")\n",
        "        else:\n",
        "            if current:\n",
        "                groups.append(current)\n",
        "                words.append(current_word)\n",
        "            current = [idx]\n",
        "            current_word = t\n",
        "    if current:\n",
        "        groups.append(current)\n",
        "        words.append(current_word)\n",
        "    return words, groups\n",
        "\n",
        "words_a, groups_a = tokens_to_words(tokens_a)\n",
        "words_b, groups_b = tokens_to_words(tokens_b)\n",
        "\n",
        "print(\"Words A:\", words_a)\n",
        "print(\"Groups A:\", groups_a)\n",
        "print(\"Words B:\", words_b)\n",
        "print(\"Groups B:\", groups_b)\n"
      ],
      "metadata": {
        "id": "7L04eVd-mVMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Aggregate token embeddings into word embeddings (mean pooling) & compute word-level similarity"
      ],
      "metadata": {
        "id": "rWxSIFnUm4WF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def aggregate_word_embeddings(token_embs, token_groups):\n",
        "    \"\"\"\n",
        "    token_embs: numpy array (seq_len, dim)\n",
        "    token_groups: list of lists of token indices per word\n",
        "    returns: (num_words, dim)\n",
        "    \"\"\"\n",
        "    word_embs = []\n",
        "    for grp in token_groups:\n",
        "        vecs = [token_embs[i] for i in grp]\n",
        "        word_embs.append(np.mean(vecs, axis=0))\n",
        "    return np.vstack(word_embs)\n",
        "\n",
        "word_embs_a = aggregate_word_embeddings(emb_a, groups_a)\n",
        "word_embs_b = aggregate_word_embeddings(emb_b, groups_b)\n",
        "\n",
        "print(\"Word embeddings shapes:\", word_embs_a.shape, word_embs_b.shape)\n",
        "\n",
        "# Compute word-level similarity matrix\n",
        "word_sim = cosine_similarity(word_embs_a, word_embs_b)\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(word_sim, xticklabels=words_b, yticklabels=words_a, cmap=\"coolwarm\", annot=True, fmt=\".2f\")\n",
        "plt.title(\"Word-level Cosine Similarity (mean pooled subword embeddings)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "PyEiYw00mzKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2D Visualization (t-SNE)"
      ],
      "metadata": {
        "id": "cZtpfLzenElw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_2d_tokens(embs_list, labels_list, title=\"2D token visualization (t-SNE)\", perplexity=5):\n",
        "    \"\"\"\n",
        "    embs_list: list of (N_i, dim) numpy arrays\n",
        "    labels_list: list of token lists (strings)\n",
        "    \"\"\"\n",
        "    # stack all token embs\n",
        "    all_embs = np.vstack(embs_list)\n",
        "    # reduce dimension\n",
        "    tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42, init='pca', learning_rate='auto')\n",
        "    reduced = tsne.fit_transform(all_embs)\n",
        "    # split back\n",
        "    splits = np.cumsum([e.shape[0] for e in embs_list])[:-1]\n",
        "    parts = np.split(reduced, splits)\n",
        "    # plot\n",
        "    plt.figure(figsize=(8,6))\n",
        "    colors = [\"C0\",\"C1\",\"C2\",\"C3\",\"C4\"]\n",
        "    for idx, part in enumerate(parts):\n",
        "        xs, ys = part[:,0], part[:,1]\n",
        "        plt.scatter(xs, ys, label=f\"Sentence {idx+1}\", color=colors[idx%len(colors)])\n",
        "        for i, txt in enumerate(labels_list[idx]):\n",
        "            plt.annotate(txt, (xs[i], ys[i]), fontsize=9)\n",
        "    plt.legend()\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "# Use token-level embeddings and tokens (filter out special tokens if desired)\n",
        "plot_2d_tokens([emb_a, emb_b], [tokens_a, tokens_b], title=\"t-SNE of Token Embeddings (A & B)\", perplexity=3)\n"
      ],
      "metadata": {
        "id": "ZlFysIWfm5jo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y0MQNfuenDmt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}