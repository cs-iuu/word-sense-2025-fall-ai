{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMimRfS8fG+Z8/zEu6HTvwL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cs-iuu/word-sense-2025-fall-ai/blob/main/notebooks/13.word_sense_induction_en.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word Sense Induction\n",
        "\n",
        "WSI (Word Sense Induction) using BERT"
      ],
      "metadata": {
        "id": "-cKtBt3GeGHf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "fqrhyU8IeBHv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ZhMPq5cdc5tr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "# from transformers import BertTokenizer, BertModel\n",
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "# --- New Imports ---\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "nltk.download('averaged_perceptron_tagger_eng', quiet=True) # Explicitly download the English tagger\n",
        "nltk.download('stopwords', quiet=True)\n",
        "# --------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load text"
      ],
      "metadata": {
        "id": "Orcecj-He_dO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Setup and Corpus Definition (Same as before) ---\n",
        "# large_corpus = [\n",
        "#     \"I went to the bank to deposit a large sum of money.\",\n",
        "#     \"The fisherman cast his line from the grassy river bank.\",\n",
        "#     \"The central bank announced a new interest rate policy.\",\n",
        "#     \"We use a construction crane to lift heavy steel beams.\",\n",
        "#     \"A small red star twinkled brightly in the night sky.\",\n",
        "#     \"The famous movie star walked the red carpet.\",\n",
        "#     \"The huge crane bird waded through the shallow marsh.\",\n",
        "#     \"The harbor crane loaded the containers onto the ship.\"\n",
        "# ]\n",
        "# clean_corpus = [s.replace('**', '') for s in large_corpus]\n",
        "\n",
        "file_path = \"/content/Bible_NT.en-kjv.txt\"\n",
        "\n",
        "clean_corpus = []\n",
        "with open(file_path, 'r') as f:\n",
        "    for line in f:\n",
        "        clean_corpus.append(line.strip())\n",
        "\n",
        "N_SENTENCES = len(clean_corpus)\n"
      ],
      "metadata": {
        "id": "FnwNCN0pfurj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define functions"
      ],
      "metadata": {
        "id": "iT75a4QchVz8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### for corpus processing function using BERT"
      ],
      "metadata": {
        "id": "KtDsXTm9eaGm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load BERT Model (Same as before)\n",
        "# MODEL_NAME = 'bert-base-uncased'\n",
        "# tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
        "# model = BertModel.from_pretrained(MODEL_NAME)\n",
        "# Use a smaller and faster model\n",
        "MODEL_NAME = 'distilbert-base-uncased'\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = DistilBertModel.from_pretrained(MODEL_NAME)\n",
        "model.eval()\n",
        "\n",
        "embeddings_store = []\n",
        "index_data = []\n",
        "\n",
        "print(f\"--- Starting Stage 1: Indexing All Content Words for {N_SENTENCES} Sentences ---\")\n",
        "\n",
        "# Define which POS tags are considered content words (Nouns, Verbs, Adjectives, Adverbs)\n",
        "# POS tags: NN (Noun), VB (Verb), JJ (Adjective), RB (Adverb)\n",
        "CONTENT_TAGS_PREFIX = ('NN', 'VB', 'JJ', 'RB')\n",
        "STOP_WORDS = set(stopwords.words('english'))\n",
        "\n",
        "# --- 2. The Modified Pre-calculation Function ---\n",
        "def process_corpus_general(corpus, model, tokenizer):\n",
        "    \"\"\"Processes the corpus and indexes all content words found.\"\"\"\n",
        "    for sent_idx, text in enumerate(corpus):\n",
        "        # A. Get BERT Hidden States (Same as before)\n",
        "        encoded_input = tokenizer(text, return_tensors='pt', padding='max_length', truncation=True)\n",
        "        input_ids = encoded_input['input_ids']\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids)\n",
        "            full_hidden_state = outputs[0].squeeze(0).numpy()\n",
        "\n",
        "        embeddings_store.append(full_hidden_state)\n",
        "\n",
        "        # B. Identify Content Words using NLTK (The New Step)\n",
        "\n",
        "        # NLTK tokenization and POS tagging\n",
        "        nltk_tokens = nltk.word_tokenize(text)\n",
        "        tagged_tokens = nltk.pos_tag(nltk_tokens)\n",
        "\n",
        "        # Filter for content words\n",
        "        content_words = [(word.lower(), tag) for word, tag in tagged_tokens\n",
        "                         if word.isalpha() and word.lower() not in STOP_WORDS and tag.startswith(CONTENT_TAGS_PREFIX)]\n",
        "\n",
        "        # C. Build the Index for Each Content Word\n",
        "        bert_tokens = tokenizer.convert_ids_to_tokens(input_ids.squeeze(0))\n",
        "\n",
        "        for word, _ in content_words:\n",
        "            # 1. Find all BERT token indices corresponding to this content word\n",
        "            # Note: We search the BERT tokens, not the NLTK tokens\n",
        "            target_indices = [i for i, token in enumerate(bert_tokens)\n",
        "                              if word in token or word.capitalize() in token]\n",
        "\n",
        "            if target_indices:\n",
        "                index_data.append({\n",
        "                    'target_word': word,\n",
        "                    'sentence_id': sent_idx,\n",
        "                    'token_indices': target_indices,\n",
        "                    'sentence': text\n",
        "                })"
      ],
      "metadata": {
        "id": "ZNcnXUoqecC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Batch version of corpus processing"
      ],
      "metadata": {
        "id": "0p-UaZZl6V_a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "CG9ldT1o7l99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Check if a CUDA-enabled GPU is available\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"‚úÖ Using GPU: {torch.cuda.get_device_name(0)}\")\n",
        "# else:\n",
        "#     device = torch.device(\"cpu\")\n",
        "#     print(\"‚ö†Ô∏è Using CPU. Processing will be slower.\")\n",
        "\n",
        "# BATCH_SIZE = 32"
      ],
      "metadata": {
        "id": "PnlO3Q5-7izZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = 'distilbert-base-uncased'\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = DistilBertModel.from_pretrained(MODEL_NAME)\n",
        "# model.eval()\n",
        "\n",
        "embeddings_store = []\n",
        "index_data = []\n",
        "\n",
        "print(f\"--- Starting Stage 1: Indexing All Content Words for {N_SENTENCES} Sentences ---\")\n",
        "\n",
        "# Define which POS tags are considered content words (Nouns, Verbs, Adjectives, Adverbs)\n",
        "# POS tags: NN (Noun), VB (Verb), JJ (Adjective), RB (Adverb)\n",
        "CONTENT_TAGS_PREFIX = ('NN', 'VB', 'JJ', 'RB')\n",
        "STOP_WORDS = set(stopwords.words('english'))\n",
        "model.to(device)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "yr23aP8_8LXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# --- Define File Paths ---\n",
        "HDF5_EMBEDDINGS_FILE = 'corpus_embeddings.h5'\n",
        "INDEX_FILE = 'corpus_index.pkl' # Use a simple file for the index metadata\n",
        "BATCH_SIZE = 16 # Use a low batch size to be safe\n",
        "\n",
        "# Initialize (or create) the HDF5 file and the index list\n",
        "# Delete files if they exist to start fresh\n",
        "try:\n",
        "    os.remove(HDF5_EMBEDDINGS_FILE)\n",
        "    os.remove(INDEX_FILE)\n",
        "except OSError:\n",
        "    pass\n",
        "\n",
        "# Initialize an empty list to collect index data (this stays small)\n",
        "index_data = []\n",
        "\n",
        "# Open the HDF5 file for writing\n",
        "h5f = h5py.File(HDF5_EMBEDDINGS_FILE, 'w')"
      ],
      "metadata": {
        "id": "SVVwFE4A_bpa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. The Modified Pre-calculation Function (Corrected and Robust) ---\n",
        "def process_corpus_general_batched(corpus, model, tokenizer, batch_size, device, h5f):\n",
        "    \"\"\"\n",
        "    Processes the corpus in batches for fast BERT inference and indexes\n",
        "    all content words, writing embeddings directly to the h5f disk file.\n",
        "    \"\"\"\n",
        "\n",
        "    # We rely on index_data being defined globally outside the function\n",
        "    global index_data\n",
        "\n",
        "    # 1. Chunk the entire corpus into batches\n",
        "    batched_corpus = [corpus[i:i + batch_size] for i in range(0, len(corpus), batch_size)]\n",
        "\n",
        "    # Track the global sentence index across all batches\n",
        "    global_sent_idx = 0\n",
        "\n",
        "    print(f\"Processing {len(corpus)} sentences in {len(batched_corpus)} batches of size {batch_size}...\")\n",
        "\n",
        "    # 2. Process each batch\n",
        "    for batch_texts in batched_corpus:\n",
        "\n",
        "        # A. Get BERT Hidden States for the entire batch\n",
        "        encoded_input = tokenizer(\n",
        "            batch_texts,\n",
        "            return_tensors='pt',\n",
        "            padding='max_length',\n",
        "            truncation=True\n",
        "        ).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**encoded_input)\n",
        "            full_hidden_states = outputs[0].cpu().numpy() # Shape: (batch_size, seq_len, hidden_size)\n",
        "\n",
        "        # 3. Process each sentence's result from the batch for indexing\n",
        "        for sent_in_batch, text in enumerate(batch_texts):\n",
        "\n",
        "            # Extract the ID tensor and ensure it's a CPU-based list/array for token conversion\n",
        "            input_ids_tensor = encoded_input['input_ids'][sent_in_batch].cpu()\n",
        "\n",
        "            # --- CRITICAL CHANGE: DISK WRITE ---\n",
        "            embedding_array = full_hidden_states[sent_in_batch]\n",
        "            # Write the array to HDF5 with a unique dataset name\n",
        "            h5f.create_dataset(f'sent_{global_sent_idx}', data=embedding_array, compression=\"gzip\")\n",
        "\n",
        "\n",
        "            # 4. Identify Content Words using NLTK (CPU-bound)\n",
        "            nltk_tokens = nltk.word_tokenize(text)\n",
        "            tagged_tokens = nltk.pos_tag(nltk_tokens)\n",
        "\n",
        "            # NOTE: Assuming STOP_WORDS and CONTENT_TAGS_PREFIX are globally defined\n",
        "            content_words = [(word.lower(), tag) for word, tag in tagged_tokens\n",
        "                             if word.isalpha() and word.lower() not in STOP_WORDS and tag.startswith(CONTENT_TAGS_PREFIX)]\n",
        "\n",
        "            # Convert IDs to BERT tokens using the list of IDs\n",
        "            bert_tokens = tokenizer.convert_ids_to_tokens(input_ids_tensor.tolist())\n",
        "\n",
        "            # 5. Build the Index for Each Content Word\n",
        "            for word, _ in content_words:\n",
        "                target_indices = [i for i, token in enumerate(bert_tokens)\n",
        "                                  if word in token or word.capitalize() in token]\n",
        "\n",
        "                if target_indices:\n",
        "                    index_data.append({\n",
        "                        'target_word': word,\n",
        "                        'sentence_id': global_sent_idx,\n",
        "                        'token_indices': target_indices,\n",
        "                        'sentence': text\n",
        "                    })\n",
        "\n",
        "            global_sent_idx += 1"
      ],
      "metadata": {
        "id": "4gTIvGWt6U-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## for analysis"
      ],
      "metadata": {
        "id": "Pl7Qj-1yg-YV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "import h5py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# --- File Paths (Must match where you saved them) ---\n",
        "HDF5_EMBEDDINGS_FILE = 'corpus_embeddings.h5'\n",
        "INDEX_FILE = 'corpus_index.pkl'\n",
        "\n",
        "# --- 0. Load the Index (Do this once, before calling the function) ---\n",
        "# index_df = pd.read_pickle(INDEX_FILE)\n",
        "\n",
        "def get_target_vectors_from_store(target_word, index_df):\n",
        "    \"\"\"\n",
        "    Retrieves the contextualized BERT vectors for all occurrences of a target word,\n",
        "    reading the embedding data directly from the HDF5 file.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Query the Index\n",
        "    # Find all rows in the index where the 'target_word' occurs (case-insensitive)\n",
        "    matches = index_df[index_df['target_word'] == target_word.lower()]\n",
        "\n",
        "    if matches.empty:\n",
        "        return []\n",
        "\n",
        "    target_vectors = []\n",
        "\n",
        "    # 2. Access HDF5 File\n",
        "    # Use 'with' to ensure the file is closed automatically\n",
        "    with h5py.File(HDF5_EMBEDDINGS_FILE, 'r') as hf:\n",
        "\n",
        "        # 3. Iterate through matches and extract the specific vector\n",
        "        for _, row in matches.iterrows():\n",
        "            sent_id = row['sentence_id']\n",
        "            token_indices = row['token_indices']\n",
        "\n",
        "            # Retrieve the full embedding array for the sentence from HDF5\n",
        "            # The dataset name is f'sent_{sent_id}'\n",
        "            try:\n",
        "                # Use dataset name indexing (e.g., hf['sent_0']) and load the data [()]\n",
        "                full_sent_embedding = hf[f'sent_{sent_id}'][()]\n",
        "            except KeyError:\n",
        "                print(f\"Warning: Dataset 'sent_{sent_id}' not found in HDF5 file.\")\n",
        "                continue\n",
        "\n",
        "            # The embedding for the word is the average of its sub-token embeddings\n",
        "            word_vector = np.mean(full_sent_embedding[token_indices], axis=0)\n",
        "            target_vectors.append(word_vector)\n",
        "\n",
        "    return target_vectors\n",
        "\n",
        "# Example Call:\n",
        "# all_embeddings_for_apple = get_target_vectors_from_store(\"apple\", index_df)\n",
        "# --- 2. Function to find optimal K and perform Clustering (from previous answer) ---\n",
        "def find_optimal_k_and_cluster(X, max_k=5):\n",
        "    \"\"\"\n",
        "    Finds the optimal K using Silhouette Score and performs K-means,\n",
        "    while safeguarding against having too few samples.\n",
        "    \"\"\"\n",
        "    n_instances = X.shape[0]\n",
        "\n",
        "    # --- Initial Checks ---\n",
        "    if n_instances < 2:\n",
        "        # If there's 0 or 1 instance, clustering is meaningless\n",
        "        print(f\"   --> Warning: Only {n_instances} instance(s) found. Cannot cluster.\")\n",
        "        return 1, np.zeros(n_instances, dtype=int)\n",
        "\n",
        "    # K must be less than the number of instances for Silhouette Score\n",
        "    # The range should be from 2 up to n_instances - 1\n",
        "    k_range = range(2, min(max_k, n_instances - 1) + 1)\n",
        "\n",
        "    # If the range is empty (e.g., n_instances=2, range is just 2, min is 1),\n",
        "    # we can only assign K=1 (no distinct senses found).\n",
        "    if len(k_range) == 0:\n",
        "        print(f\"   --> Warning: Only {n_instances} instances. Defaulting to K=1.\")\n",
        "        return 1, np.zeros(n_instances, dtype=int)\n",
        "\n",
        "    best_k = k_range[0] # Start with the smallest possible K (usually 2)\n",
        "    best_score = -1.0\n",
        "\n",
        "    print(f\"   --> Testing K in range {list(k_range)}...\")\n",
        "\n",
        "    for k in k_range:\n",
        "        kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto')\n",
        "        labels = kmeans.fit_predict(X)\n",
        "\n",
        "        # This calculation is now safe because k is guaranteed to be < n_instances\n",
        "        score = silhouette_score(X, labels)\n",
        "\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_k = k\n",
        "\n",
        "    # Final clustering with the optimal K\n",
        "    kmeans = KMeans(n_clusters=best_k, random_state=42, n_init='auto')\n",
        "    final_labels = kmeans.fit_predict(X)\n",
        "\n",
        "    print(f\"   --> Optimal K determined: {best_k} (Silhouette: {best_score:.4f})\")\n",
        "    return best_k, final_labels\n",
        "\n",
        "# The word 'deposit' from your corpus likely had n_instances = 3.\n",
        "# The old code tried K=3, which failed.\n",
        "# The new code will cap K at min(max_k, 3-1) = min(5, 2) = 2. It will only test K=2."
      ],
      "metadata": {
        "id": "rhgL0_J2hHbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Process & Save the index & embeddings"
      ],
      "metadata": {
        "id": "xHE5rzl8gcoC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # --- Run the Pre-calculation ---\n",
        "start_time = time.time()\n",
        "\n",
        "# IMPORTANT: h5f must be opened before the function call\n",
        "# h5f = h5py.File('corpus_embeddings.h5', 'w')\n",
        "process_corpus_general_batched(clean_corpus, model, tokenizer, BATCH_SIZE, device, h5f)\n",
        "h5f.close() # Close after the loop finishes\n",
        "\n",
        "# --- CRITICAL: SAVE THE INDEX HERE ---\n",
        "import pandas as pd\n",
        "index_df = pd.DataFrame(index_data)\n",
        "INDEX_FILE = 'corpus_index.pkl'\n",
        "index_df.to_pickle(INDEX_FILE)\n",
        "\n",
        "print(f\"Processing complete. Index saved to {INDEX_FILE}\")\n",
        "print(f\"Time taken for Stage 1 (BERT Inference): {time.time() - start_time:.2f} seconds.\")\n",
        "print(f\"Index created for {len(index_df)} instances of ALL content words.\")"
      ],
      "metadata": {
        "id": "kHBNBu4WgbHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test & Analysis"
      ],
      "metadata": {
        "id": "RAFp0NekimXu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# Assuming find_optimal_k_and_cluster is available\n",
        "\n",
        "# --- File Paths (Must match where you saved them) ---\n",
        "HDF5_EMBEDDINGS_FILE = 'corpus_embeddings.h5'\n",
        "INDEX_FILE = 'corpus_index.pkl'\n",
        "\n",
        "# --- 0. Load the Index and Ensure Files Exist ---\n",
        "try:\n",
        "    # Load the index DataFrame from the saved pickle file\n",
        "    index_df = pd.read_pickle(INDEX_FILE)\n",
        "    print(f\"‚úÖ Loaded index with {len(index_df)} word occurrences.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"üõë Error: Index file '{INDEX_FILE}' not found. Did Stage 1 complete successfully?\")\n",
        "    exit()\n",
        "\n",
        "# Filter out words with too few instances to cluster (e.g., less than 2)\n",
        "word_counts = index_df.groupby('target_word').size()\n",
        "plausible_words = word_counts[word_counts >= 2].index.tolist()\n",
        "print(f\"Found {len(plausible_words)} words with 2 or more instances for clustering.\")\n",
        "\n",
        "\n",
        "# --- 1. Define Words to Analyze ---\n",
        "# Use a specific list, or sample from the plausible_words list\n",
        "WORDS_TO_ANALYZE = [\"life\", \"light\", \"spirit\"]\n",
        "\n",
        "print(\"\\n--- Starting Stage 2: Efficient Sense Induction from Disk ---\")\n",
        "\n",
        "for word in WORDS_TO_ANALYZE:\n",
        "    run_start = time.time()\n",
        "\n",
        "    # A. Retrieve vectors quickly\n",
        "    # The function handles reading the correct embedding arrays from the HDF5 file\n",
        "    X_list = get_target_vectors_from_store(word, index_df)\n",
        "\n",
        "    if not X_list:\n",
        "        print(f\"  Skipping '{word}': No instances found or retrieval failed.\")\n",
        "        continue\n",
        "\n",
        "    # Convert the list of vectors to a numpy array for K-means\n",
        "    X = np.array(X_list)\n",
        "\n",
        "    # Extract the original sentences for interpretation\n",
        "    sentences = index_df[index_df['target_word'] == word.lower()]['sentence'].tolist()\n",
        "\n",
        "    # B. Find optimal K and cluster\n",
        "    # Note: max_k=5 is generally a good starting point for WSI\n",
        "    optimal_k, labels = find_optimal_k_and_cluster(X, max_k=5)\n",
        "\n",
        "    # C. Display Results\n",
        "    sense_clusters = {i: [] for i in range(optimal_k)}\n",
        "    for sentence, label in zip(sentences, labels):\n",
        "        sense_clusters[label].append(sentence)\n",
        "\n",
        "    run_end = time.time()\n",
        "    print(f\"\\n## üéØ Induced Senses for '{word}' (Run Time: {run_end - run_start:.4f}s) ##\")\n",
        "\n",
        "    for i, sentences_in_sense in sense_clusters.items():\n",
        "        print(f\"--- Sense Cluster {i+1} ({len(sentences_in_sense)} instances) ---\")\n",
        "\n",
        "        # Display up to 3 example sentences for brevity\n",
        "        for j, sentence in enumerate(sentences_in_sense[:3]):\n",
        "            print(f\"  - {sentence}\")\n",
        "        if len(sentences_in_sense) > 3:\n",
        "             print(\"  - ... (more instances)\")\n",
        "\n",
        "    print(\"-\" * 20)"
      ],
      "metadata": {
        "id": "S2ouPn9cIujp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}