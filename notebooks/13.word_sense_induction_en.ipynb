{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOLtUjN63QKnXZZxgtrCP00",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cs-iuu/word-sense-2025-fall-ai/blob/main/notebooks/13.word_sense_induction_en.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word Sense Induction\n",
        "\n",
        "WSI (Word Sense Induction) using BERT"
      ],
      "metadata": {
        "id": "-cKtBt3GeGHf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "fqrhyU8IeBHv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ZhMPq5cdc5tr",
        "outputId": "792fd7df-3342-421d-ad24-14647082b418"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import torch\n",
        "# from transformers import BertTokenizer, BertModel\n",
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "# --- New Imports ---\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "nltk.download('averaged_perceptron_tagger_eng', quiet=True) # Explicitly download the English tagger\n",
        "nltk.download('stopwords', quiet=True)\n",
        "# --------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load text"
      ],
      "metadata": {
        "id": "Orcecj-He_dO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Setup and Corpus Definition (Same as before) ---\n",
        "# large_corpus = [\n",
        "#     \"I went to the bank to deposit a large sum of money.\",\n",
        "#     \"The fisherman cast his line from the grassy river bank.\",\n",
        "#     \"The central bank announced a new interest rate policy.\",\n",
        "#     \"We use a construction crane to lift heavy steel beams.\",\n",
        "#     \"A small red star twinkled brightly in the night sky.\",\n",
        "#     \"The famous movie star walked the red carpet.\",\n",
        "#     \"The huge crane bird waded through the shallow marsh.\",\n",
        "#     \"The harbor crane loaded the containers onto the ship.\"\n",
        "# ]\n",
        "# clean_corpus = [s.replace('**', '') for s in large_corpus]\n",
        "\n",
        "file_path = \"/content/Bible_NT.en-kjv.txt\"\n",
        "\n",
        "clean_corpus = []\n",
        "with open(file_path, 'r') as f:\n",
        "    for line in f:\n",
        "        clean_corpus.append(line.strip())\n",
        "\n",
        "N_SENTENCES = len(clean_corpus)\n"
      ],
      "metadata": {
        "id": "FnwNCN0pfurj"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define functions"
      ],
      "metadata": {
        "id": "iT75a4QchVz8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Batch version of corpus processing"
      ],
      "metadata": {
        "id": "0p-UaZZl6V_a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "CG9ldT1o7l99"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Check if a CUDA-enabled GPU is available\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"‚úÖ Using GPU: {torch.cuda.get_device_name(0)}\")\n",
        "# else:\n",
        "#     device = torch.device(\"cpu\")\n",
        "#     print(\"‚ö†Ô∏è Using CPU. Processing will be slower.\")\n",
        "\n",
        "# BATCH_SIZE = 32"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnlO3Q5-7izZ",
        "outputId": "dd4636a1-53ef-4729-c641-9225d00c0059"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Using GPU: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = 'distilbert-base-uncased'\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = DistilBertModel.from_pretrained(MODEL_NAME)\n",
        "# model.eval()\n",
        "\n",
        "embeddings_store = []\n",
        "index_data = []\n",
        "\n",
        "print(f\"--- Starting Stage 1: Indexing All Content Words for {N_SENTENCES} Sentences ---\")\n",
        "\n",
        "# Define which POS tags are considered content words (Nouns, Verbs, Adjectives, Adverbs)\n",
        "# POS tags: NN (Noun), VB (Verb), JJ (Adjective), RB (Adverb)\n",
        "CONTENT_TAGS_PREFIX = ('NN', 'VB', 'JJ', 'RB')\n",
        "STOP_WORDS = set(stopwords.words('english'))\n",
        "model.to(device)\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yr23aP8_8LXz",
        "outputId": "29f58a61-2d8d-494f-bec2-4806fe075a4e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Stage 1: Indexing All Content Words for 11180 Sentences ---\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DistilBertModel(\n",
              "  (embeddings): Embeddings(\n",
              "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (transformer): Transformer(\n",
              "    (layer): ModuleList(\n",
              "      (0-5): 6 x TransformerBlock(\n",
              "        (attention): DistilBertSdpaAttention(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        (ffn): FFN(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (activation): GELUActivation()\n",
              "        )\n",
              "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# --- Define File Paths ---\n",
        "HDF5_EMBEDDINGS_FILE = 'corpus_embeddings.h5'\n",
        "INDEX_FILE = 'corpus_index.pkl' # Use a simple file for the index metadata\n",
        "BATCH_SIZE = 16 # Use a low batch size to be safe\n",
        "\n",
        "# Initialize (or create) the HDF5 file and the index list\n",
        "# Delete files if they exist to start fresh\n",
        "try:\n",
        "    os.remove(HDF5_EMBEDDINGS_FILE)\n",
        "    os.remove(INDEX_FILE)\n",
        "except OSError:\n",
        "    pass\n",
        "\n",
        "# Initialize an empty list to collect index data (this stays small)\n",
        "index_data = []\n",
        "\n",
        "# Open the HDF5 file for writing\n",
        "h5f = h5py.File(HDF5_EMBEDDINGS_FILE, 'w')"
      ],
      "metadata": {
        "id": "SVVwFE4A_bpa"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. The Modified Pre-calculation Function (Corrected and Robust) ---\n",
        "def process_corpus_general_batched(corpus, model, tokenizer, batch_size, device, h5f):\n",
        "    \"\"\"\n",
        "    Processes the corpus in batches for fast BERT inference and indexes\n",
        "    all content words, writing embeddings directly to the h5f disk file.\n",
        "    \"\"\"\n",
        "\n",
        "    # We rely on index_data being defined globally outside the function\n",
        "    global index_data\n",
        "\n",
        "    # 1. Chunk the entire corpus into batches\n",
        "    batched_corpus = [corpus[i:i + batch_size] for i in range(0, len(corpus), batch_size)]\n",
        "\n",
        "    # Track the global sentence index across all batches\n",
        "    global_sent_idx = 0\n",
        "\n",
        "    print(f\"Processing {len(corpus)} sentences in {len(batched_corpus)} batches of size {batch_size}...\")\n",
        "\n",
        "    # 2. Process each batch\n",
        "    for batch_texts in batched_corpus:\n",
        "\n",
        "        # A. Get BERT Hidden States for the entire batch\n",
        "        encoded_input = tokenizer(\n",
        "            batch_texts,\n",
        "            return_tensors='pt',\n",
        "            padding='max_length',\n",
        "            truncation=True\n",
        "        ).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**encoded_input)\n",
        "            full_hidden_states = outputs[0].cpu().numpy() # Shape: (batch_size, seq_len, hidden_size)\n",
        "\n",
        "        # 3. Process each sentence's result from the batch for indexing\n",
        "        for sent_in_batch, text in enumerate(batch_texts):\n",
        "\n",
        "            # Extract the ID tensor and ensure it's a CPU-based list/array for token conversion\n",
        "            input_ids_tensor = encoded_input['input_ids'][sent_in_batch].cpu()\n",
        "\n",
        "            # --- CRITICAL CHANGE: DISK WRITE ---\n",
        "            embedding_array = full_hidden_states[sent_in_batch]\n",
        "            # Write the array to HDF5 with a unique dataset name\n",
        "            h5f.create_dataset(f'sent_{global_sent_idx}', data=embedding_array, compression=\"gzip\")\n",
        "\n",
        "\n",
        "            # 4. Identify Content Words using NLTK (CPU-bound)\n",
        "            nltk_tokens = nltk.word_tokenize(text)\n",
        "            tagged_tokens = nltk.pos_tag(nltk_tokens)\n",
        "\n",
        "            # NOTE: Assuming STOP_WORDS and CONTENT_TAGS_PREFIX are globally defined\n",
        "            content_words = [(word.lower(), tag) for word, tag in tagged_tokens\n",
        "                             if word.isalpha() and word.lower() not in STOP_WORDS and tag.startswith(CONTENT_TAGS_PREFIX)]\n",
        "\n",
        "            # Convert IDs to BERT tokens using the list of IDs\n",
        "            bert_tokens = tokenizer.convert_ids_to_tokens(input_ids_tensor.tolist())\n",
        "\n",
        "            # 5. Build the Index for Each Content Word\n",
        "            for word, _ in content_words:\n",
        "                target_indices = [i for i, token in enumerate(bert_tokens)\n",
        "                                  if word in token or word.capitalize() in token]\n",
        "\n",
        "                if target_indices:\n",
        "                    index_data.append({\n",
        "                        'target_word': word,\n",
        "                        'sentence_id': global_sent_idx,\n",
        "                        'token_indices': target_indices,\n",
        "                        'sentence': text\n",
        "                    })\n",
        "\n",
        "            global_sent_idx += 1"
      ],
      "metadata": {
        "id": "4gTIvGWt6U-G"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## for analysis"
      ],
      "metadata": {
        "id": "Pl7Qj-1yg-YV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "import h5py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# --- File Paths (Must match where you saved them) ---\n",
        "HDF5_EMBEDDINGS_FILE = 'corpus_embeddings.h5'\n",
        "INDEX_FILE = 'corpus_index.pkl'\n",
        "\n",
        "# --- 0. Load the Index (Do this once, before calling the function) ---\n",
        "# index_df = pd.read_pickle(INDEX_FILE)\n",
        "\n",
        "def get_target_vectors_from_store(target_word, index_df):\n",
        "    \"\"\"\n",
        "    Retrieves the contextualized BERT vectors for all occurrences of a target word,\n",
        "    reading the embedding data directly from the HDF5 file.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Query the Index\n",
        "    # Find all rows in the index where the 'target_word' occurs (case-insensitive)\n",
        "    matches = index_df[index_df['target_word'] == target_word.lower()]\n",
        "\n",
        "    if matches.empty:\n",
        "        return []\n",
        "\n",
        "    target_vectors = []\n",
        "\n",
        "    # 2. Access HDF5 File\n",
        "    # Use 'with' to ensure the file is closed automatically\n",
        "    with h5py.File(HDF5_EMBEDDINGS_FILE, 'r') as hf:\n",
        "\n",
        "        # 3. Iterate through matches and extract the specific vector\n",
        "        for _, row in matches.iterrows():\n",
        "            sent_id = row['sentence_id']\n",
        "            token_indices = row['token_indices']\n",
        "\n",
        "            # Retrieve the full embedding array for the sentence from HDF5\n",
        "            # The dataset name is f'sent_{sent_id}'\n",
        "            try:\n",
        "                # Use dataset name indexing (e.g., hf['sent_0']) and load the data [()]\n",
        "                full_sent_embedding = hf[f'sent_{sent_id}'][()]\n",
        "            except KeyError:\n",
        "                print(f\"Warning: Dataset 'sent_{sent_id}' not found in HDF5 file.\")\n",
        "                continue\n",
        "\n",
        "            # The embedding for the word is the average of its sub-token embeddings\n",
        "            word_vector = np.mean(full_sent_embedding[token_indices], axis=0)\n",
        "            target_vectors.append(word_vector)\n",
        "\n",
        "    return target_vectors\n",
        "\n",
        "# Example Call:\n",
        "# all_embeddings_for_apple = get_target_vectors_from_store(\"apple\", index_df)\n",
        "# --- 2. Function to find optimal K and perform Clustering (from previous answer) ---\n",
        "def find_optimal_k_and_cluster(X, max_k=5):\n",
        "    \"\"\"\n",
        "    Finds the optimal K using Silhouette Score and performs K-means,\n",
        "    while safeguarding against having too few samples.\n",
        "    \"\"\"\n",
        "    n_instances = X.shape[0]\n",
        "\n",
        "    # --- Initial Checks ---\n",
        "    if n_instances < 2:\n",
        "        # If there's 0 or 1 instance, clustering is meaningless\n",
        "        print(f\"   --> Warning: Only {n_instances} instance(s) found. Cannot cluster.\")\n",
        "        return 1, np.zeros(n_instances, dtype=int)\n",
        "\n",
        "    # K must be less than the number of instances for Silhouette Score\n",
        "    # The range should be from 2 up to n_instances - 1\n",
        "    k_range = range(2, min(max_k, n_instances - 1) + 1)\n",
        "\n",
        "    # If the range is empty (e.g., n_instances=2, range is just 2, min is 1),\n",
        "    # we can only assign K=1 (no distinct senses found).\n",
        "    if len(k_range) == 0:\n",
        "        print(f\"   --> Warning: Only {n_instances} instances. Defaulting to K=1.\")\n",
        "        return 1, np.zeros(n_instances, dtype=int)\n",
        "\n",
        "    best_k = k_range[0] # Start with the smallest possible K (usually 2)\n",
        "    best_score = -1.0\n",
        "\n",
        "    print(f\"   --> Testing K in range {list(k_range)}...\")\n",
        "\n",
        "    for k in k_range:\n",
        "        kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto')\n",
        "        labels = kmeans.fit_predict(X)\n",
        "\n",
        "        # This calculation is now safe because k is guaranteed to be < n_instances\n",
        "        score = silhouette_score(X, labels)\n",
        "\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_k = k\n",
        "\n",
        "    # Final clustering with the optimal K\n",
        "    kmeans = KMeans(n_clusters=best_k, random_state=42, n_init='auto')\n",
        "    final_labels = kmeans.fit_predict(X)\n",
        "\n",
        "    print(f\"   --> Optimal K determined: {best_k} (Silhouette: {best_score:.4f})\")\n",
        "    return best_k, final_labels\n",
        "\n",
        "# The word 'deposit' from your corpus likely had n_instances = 3.\n",
        "# The old code tried K=3, which failed.\n",
        "# The new code will cap K at min(max_k, 3-1) = min(5, 2) = 2. It will only test K=2."
      ],
      "metadata": {
        "id": "rhgL0_J2hHbM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Process & Save the index & embeddings"
      ],
      "metadata": {
        "id": "xHE5rzl8gcoC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # --- Run the Pre-calculation ---\n",
        "start_time = time.time()\n",
        "\n",
        "# IMPORTANT: h5f must be opened before the function call\n",
        "# h5f = h5py.File('corpus_embeddings.h5', 'w')\n",
        "process_corpus_general_batched(clean_corpus, model, tokenizer, BATCH_SIZE, device, h5f)\n",
        "h5f.close() # Close after the loop finishes\n",
        "\n",
        "# --- CRITICAL: SAVE THE INDEX HERE ---\n",
        "import pandas as pd\n",
        "index_df = pd.DataFrame(index_data)\n",
        "INDEX_FILE = 'corpus_index.pkl'\n",
        "index_df.to_pickle(INDEX_FILE)\n",
        "\n",
        "print(f\"Processing complete. Index saved to {INDEX_FILE}\")\n",
        "print(f\"Time taken for Stage 1 (BERT Inference): {time.time() - start_time:.2f} seconds.\")\n",
        "print(f\"Index created for {len(index_df)} instances of ALL content words.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHBNBu4WgbHG",
        "outputId": "a1c611e9-30ae-4e99-8d6e-91d1213b196b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 11180 sentences in 699 batches of size 16...\n",
            "Processing complete. Index saved to corpus_index.pkl\n",
            "Time taken for Stage 1 (BERT Inference): 841.36 seconds.\n",
            "Index created for 63158 instances of ALL content words.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test & Analysis"
      ],
      "metadata": {
        "id": "RAFp0NekimXu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# Assuming find_optimal_k_and_cluster is available\n",
        "\n",
        "# --- File Paths (Must match where you saved them) ---\n",
        "HDF5_EMBEDDINGS_FILE = 'corpus_embeddings.h5'\n",
        "INDEX_FILE = 'corpus_index.pkl'\n",
        "\n",
        "# --- 0. Load the Index and Ensure Files Exist ---\n",
        "try:\n",
        "    # Load the index DataFrame from the saved pickle file\n",
        "    index_df = pd.read_pickle(INDEX_FILE)\n",
        "    print(f\"‚úÖ Loaded index with {len(index_df)} word occurrences.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"üõë Error: Index file '{INDEX_FILE}' not found. Did Stage 1 complete successfully?\")\n",
        "    exit()\n",
        "\n",
        "# Filter out words with too few instances to cluster (e.g., less than 2)\n",
        "word_counts = index_df.groupby('target_word').size()\n",
        "plausible_words = word_counts[word_counts >= 2].index.tolist()\n",
        "print(f\"Found {len(plausible_words)} words with 2 or more instances for clustering.\")\n",
        "\n",
        "\n",
        "# --- 1. Define Words to Analyze ---\n",
        "# Use a specific list, or sample from the plausible_words list\n",
        "WORDS_TO_ANALYZE = [\"life\", \"light\", \"spirit\"]\n",
        "\n",
        "print(\"\\n--- Starting Stage 2: Efficient Sense Induction from Disk ---\")\n",
        "\n",
        "for word in WORDS_TO_ANALYZE:\n",
        "    run_start = time.time()\n",
        "\n",
        "    # A. Retrieve vectors quickly\n",
        "    # The function handles reading the correct embedding arrays from the HDF5 file\n",
        "    X_list = get_target_vectors_from_store(word, index_df)\n",
        "\n",
        "    if not X_list:\n",
        "        print(f\"  Skipping '{word}': No instances found or retrieval failed.\")\n",
        "        continue\n",
        "\n",
        "    # Convert the list of vectors to a numpy array for K-means\n",
        "    X = np.array(X_list)\n",
        "\n",
        "    # Extract the original sentences for interpretation\n",
        "    sentences = index_df[index_df['target_word'] == word.lower()]['sentence'].tolist()\n",
        "\n",
        "    # B. Find optimal K and cluster\n",
        "    # Note: max_k=5 is generally a good starting point for WSI\n",
        "    optimal_k, labels = find_optimal_k_and_cluster(X, max_k=5)\n",
        "\n",
        "    # C. Display Results\n",
        "    sense_clusters = {i: [] for i in range(optimal_k)}\n",
        "    for sentence, label in zip(sentences, labels):\n",
        "        sense_clusters[label].append(sentence)\n",
        "\n",
        "    run_end = time.time()\n",
        "    print(f\"\\n## üéØ Induced Senses for '{word}' (Run Time: {run_end - run_start:.4f}s) ##\")\n",
        "\n",
        "    for i, sentences_in_sense in sense_clusters.items():\n",
        "        print(f\"--- Sense Cluster {i+1} ({len(sentences_in_sense)} instances) ---\")\n",
        "\n",
        "        # Display up to 3 example sentences for brevity\n",
        "        for j, sentence in enumerate(sentences_in_sense[:3]):\n",
        "            print(f\"  - {sentence}\")\n",
        "        if len(sentences_in_sense) > 3:\n",
        "             print(\"  - ... (more instances)\")\n",
        "\n",
        "    print(\"-\" * 20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2ouPn9cIujp",
        "outputId": "c6ab0ad4-f8de-47df-e1a9-7c310edbff72"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Loaded index with 63158 word occurrences.\n",
            "Found 2480 words with 2 or more instances for clustering.\n",
            "\n",
            "--- Starting Stage 2: Efficient Sense Induction from Disk ---\n",
            "   --> Testing K in range [2, 3, 4, 5]...\n",
            "   --> Optimal K determined: 2 (Silhouette: 0.1717)\n",
            "\n",
            "## üéØ Induced Senses for 'life' (Run Time: 2.5488s) ##\n",
            "--- Sense Cluster 1 (176 instances) ---\n",
            "  - dead which sought the young child‚Äôs life. And he arose, and took the young child and his mother, and came\n",
            "  - despise the other. Ye cannot serve God and mammon. Therefore I say unto you, Take no thought for your life, what ye\n",
            "  - shall put on. Is not the life more than meat, and the body than\n",
            "  - ... (more instances)\n",
            "--- Sense Cluster 2 (10 instances) ---\n",
            "  - life for my sake shall find it. He that receiveth you receiveth me, and he that receiveth me\n",
            "  - life eternal: that both he that soweth and he that reapeth may rejoice\n",
            "  - life through Jesus Christ our Lord. Know ye not, brethren, (for I speak to them that know the law,)\n",
            "  - ... (more instances)\n",
            "--------------------\n",
            "   --> Testing K in range [2, 3, 4, 5]...\n",
            "   --> Optimal K determined: 2 (Silhouette: 0.0695)\n",
            "\n",
            "## üéØ Induced Senses for 'light' (Run Time: 1.2279s) ##\n",
            "--- Sense Cluster 1 (47 instances) ---\n",
            "  - Gentiles; The people which sat in darkness saw great light; and\n",
            "  - single, thy whole body shall be full of light. But if thine eye be evil, thy whole body shall be full of\n",
            "  - darkness. If therefore the light that is in thee be darkness, how great is that\n",
            "  - ... (more instances)\n",
            "--- Sense Cluster 2 (48 instances) ---\n",
            "  - to them which sat in the region and shadow of death light is sprung\n",
            "  - nothing, but to be cast out, and to be trodden under foot of men. Ye are the light of the world. A city that is set on an hill\n",
            "  - cannot be hid. Neither do men light a candle, and put it under a bushel, but on\n",
            "  - ... (more instances)\n",
            "--------------------\n",
            "   --> Testing K in range [2, 3, 4, 5]...\n",
            "   --> Optimal K determined: 5 (Silhouette: 0.0683)\n",
            "\n",
            "## üéØ Induced Senses for 'spirit' (Run Time: 3.3377s) ##\n",
            "--- Sense Cluster 1 (72 instances) ---\n",
            "  - well pleased. Then was Jesus led up of the spirit into the wilderness to be\n",
            "  - not see death, before he had seen the Lord‚Äôs Christ. And he came by the Spirit into the temple: and when the parents\n",
            "  - was led by the Spirit into the wilderness, Being forty days\n",
            "  - ... (more instances)\n",
            "--- Sense Cluster 2 (60 instances) ---\n",
            "  - greater than Solomon is here. When the unclean spirit is gone out of a man, he walketh through\n",
            "  - troubled, saying, It is a spirit; and they cried out for fear. But straightway Jesus spake unto them, saying, Be of good cheer;\n",
            "  - saith unto Peter, What, could ye not watch with me one hour? Watch and pray, that ye enter not into temptation: the spirit indeed\n",
            "  - ... (more instances)\n",
            "--- Sense Cluster 3 (43 instances) ---\n",
            "  - taught them, saying, Blessed are the poor in spirit: for theirs is\n",
            "  - well pleased: I will put my spirit upon him, and he shall shew\n",
            "  - him, The son of David. He saith unto them, How then doth David in spirit call him Lord,\n",
            "  - ... (more instances)\n",
            "--- Sense Cluster 4 (18 instances) ---\n",
            "  - Spirit of God descending like a dove, and lighting upon him: And\n",
            "  - spirit and in truth. The woman saith unto him, I know that Messias cometh, which is\n",
            "  - spirit, and they are life. But there are some of you that believe not. For Jesus knew from\n",
            "  - ... (more instances)\n",
            "--- Sense Cluster 5 (68 instances) ---\n",
            "  - speak. For it is not ye that speak, but the Spirit of your Father which\n",
            "  - children cast them out? therefore they shall be your judges. But if I cast out devils by the Spirit of God, then the kingdom\n",
            "  - one that had authority, and not as the scribes. And there was in their synagogue a man with an unclean spirit;\n",
            "  - ... (more instances)\n",
            "--------------------\n"
          ]
        }
      ]
    }
  ]
}