{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOq62RtHwzEWSBRIbsvWj19",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cs-iuu/word-sense-2025-fall-ai/blob/main/notebooks/13.1.word_sense_induction_en.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word Sense Induction (English)\n",
        "\n",
        "WSI (Word Sense Induction) using BERT"
      ],
      "metadata": {
        "id": "-cKtBt3GeGHf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "fqrhyU8IeBHv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ZhMPq5cdc5tr",
        "outputId": "84269fd3-543f-4936-8263-d91401e36940"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import torch\n",
        "# from transformers import BertTokenizer, BertModel\n",
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "# --- New Imports ---\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "nltk.download('averaged_perceptron_tagger_eng', quiet=True) # Explicitly download the English tagger\n",
        "nltk.download('stopwords', quiet=True)\n",
        "# --------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load text"
      ],
      "metadata": {
        "id": "Orcecj-He_dO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Setup and Corpus Definition (Same as before) ---\n",
        "# large_corpus = [\n",
        "#     \"I went to the bank to deposit a large sum of money.\",\n",
        "#     \"The fisherman cast his line from the grassy river bank.\",\n",
        "#     \"The central bank announced a new interest rate policy.\",\n",
        "#     \"We use a construction crane to lift heavy steel beams.\",\n",
        "#     \"A small red star twinkled brightly in the night sky.\",\n",
        "#     \"The famous movie star walked the red carpet.\",\n",
        "#     \"The huge crane bird waded through the shallow marsh.\",\n",
        "#     \"The harbor crane loaded the containers onto the ship.\"\n",
        "# ]\n",
        "# clean_corpus = [s.replace('**', '') for s in large_corpus]\n",
        "\n",
        "file_path = \"/content/les_miserables.txt\"\n",
        "\n",
        "clean_corpus = []\n",
        "with open(file_path, 'r') as f:\n",
        "    for line in f:\n",
        "        clean_corpus.append(line.strip())\n",
        "\n",
        "N_SENTENCES = len(clean_corpus)\n"
      ],
      "metadata": {
        "id": "FnwNCN0pfurj"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define functions"
      ],
      "metadata": {
        "id": "iT75a4QchVz8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Batch version of corpus processing"
      ],
      "metadata": {
        "id": "0p-UaZZl6V_a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "CG9ldT1o7l99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Check if a CUDA-enabled GPU is available\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"‚úÖ Using GPU: {torch.cuda.get_device_name(0)}\")\n",
        "# else:\n",
        "#     device = torch.device(\"cpu\")\n",
        "#     print(\"‚ö†Ô∏è Using CPU. Processing will be slower.\")\n",
        "\n",
        "# BATCH_SIZE = 32"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnlO3Q5-7izZ",
        "outputId": "00e58c07-1fde-4fbc-a8bf-caf098e1baee"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Using GPU: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = 'distilbert-base-uncased'\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = DistilBertModel.from_pretrained(MODEL_NAME)\n",
        "# model.eval()\n",
        "\n",
        "embeddings_store = []\n",
        "index_data = []\n",
        "\n",
        "print(f\"--- Starting Stage 1: Indexing All Content Words for {N_SENTENCES} Sentences ---\")\n",
        "\n",
        "# Define which POS tags are considered content words (Nouns, Verbs, Adjectives, Adverbs)\n",
        "# POS tags: NN (Noun), VB (Verb), JJ (Adjective), RB (Adverb)\n",
        "CONTENT_TAGS_PREFIX = ('NN', 'VB', 'JJ', 'RB')\n",
        "STOP_WORDS = set(stopwords.words('english'))\n",
        "model.to(device)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "yr23aP8_8LXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# --- Define File Paths ---\n",
        "HDF5_EMBEDDINGS_FILE = 'corpus_embeddings.h5'\n",
        "INDEX_FILE = 'corpus_index.pkl' # Use a simple file for the index metadata\n",
        "BATCH_SIZE = 16 # Use a low batch size to be safe\n",
        "\n",
        "# Initialize (or create) the HDF5 file and the index list\n",
        "# Delete files if they exist to start fresh\n",
        "try:\n",
        "    os.remove(HDF5_EMBEDDINGS_FILE)\n",
        "    os.remove(INDEX_FILE)\n",
        "except OSError:\n",
        "    pass\n",
        "\n",
        "# Initialize an empty list to collect index data (this stays small)\n",
        "index_data = []\n",
        "\n",
        "# Open the HDF5 file for writing\n",
        "h5f = h5py.File(HDF5_EMBEDDINGS_FILE, 'w')"
      ],
      "metadata": {
        "id": "SVVwFE4A_bpa"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. The Modified Pre-calculation Function (Corrected and Robust) ---\n",
        "def process_corpus_general_batched(corpus, model, tokenizer, batch_size, device, h5f):\n",
        "    \"\"\"\n",
        "    Processes the corpus in batches for fast BERT inference and indexes\n",
        "    all content words, writing embeddings directly to the h5f disk file.\n",
        "    \"\"\"\n",
        "\n",
        "    # We rely on index_data being defined globally outside the function\n",
        "    global index_data\n",
        "\n",
        "    # 1. Chunk the entire corpus into batches\n",
        "    batched_corpus = [corpus[i:i + batch_size] for i in range(0, len(corpus), batch_size)]\n",
        "\n",
        "    # Track the global sentence index across all batches\n",
        "    global_sent_idx = 0\n",
        "\n",
        "    print(f\"Processing {len(corpus)} sentences in {len(batched_corpus)} batches of size {batch_size}...\")\n",
        "\n",
        "    # 2. Process each batch\n",
        "    for batch_id, batch_texts in enumerate(batched_corpus):\n",
        "        print(f\"Processing batch {batch_id + 1}/{len(batched_corpus)}...\")\n",
        "\n",
        "        # A. Get BERT Hidden States for the entire batch\n",
        "        encoded_input = tokenizer(\n",
        "            batch_texts,\n",
        "            return_tensors='pt',\n",
        "            padding='max_length',\n",
        "            truncation=True\n",
        "        ).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**encoded_input)\n",
        "            full_hidden_states = outputs[0].cpu().numpy() # Shape: (batch_size, seq_len, hidden_size)\n",
        "\n",
        "        # 3. Process each sentence's result from the batch for indexing\n",
        "        for sent_in_batch, text in enumerate(batch_texts):\n",
        "            # print(f\"  Processing sentence {global_sent_idx + sent_in_batch + 1}/{N_SENTENCES}...\")\n",
        "\n",
        "            # Extract the ID tensor and ensure it's a CPU-based list/array for token conversion\n",
        "            input_ids_tensor = encoded_input['input_ids'][sent_in_batch].cpu()\n",
        "\n",
        "            # --- CRITICAL CHANGE: DISK WRITE ---\n",
        "            embedding_array = full_hidden_states[sent_in_batch]\n",
        "            # Write the array to HDF5 with a unique dataset name\n",
        "            h5f.create_dataset(f'sent_{global_sent_idx}', data=embedding_array, compression=\"gzip\")\n",
        "\n",
        "\n",
        "            # 4. Identify Content Words using NLTK (CPU-bound)\n",
        "            nltk_tokens = nltk.word_tokenize(text)\n",
        "            tagged_tokens = nltk.pos_tag(nltk_tokens)\n",
        "\n",
        "            # NOTE: Assuming STOP_WORDS and CONTENT_TAGS_PREFIX are globally defined\n",
        "            content_words = [(word.lower(), tag) for word, tag in tagged_tokens\n",
        "                             if word.isalpha() and word.lower() not in STOP_WORDS and tag.startswith(CONTENT_TAGS_PREFIX)]\n",
        "\n",
        "            # Convert IDs to BERT tokens using the list of IDs\n",
        "            bert_tokens = tokenizer.convert_ids_to_tokens(input_ids_tensor.tolist())\n",
        "\n",
        "            # 5. Build the Index for Each Content Word\n",
        "            for word, _ in content_words:\n",
        "                target_indices = [i for i, token in enumerate(bert_tokens)\n",
        "                                  if word in token or word.capitalize() in token]\n",
        "\n",
        "                if target_indices:\n",
        "                    index_data.append({\n",
        "                        'target_word': word,\n",
        "                        'sentence_id': global_sent_idx,\n",
        "                        'token_indices': target_indices,\n",
        "                        'sentence': text\n",
        "                    })\n",
        "\n",
        "            global_sent_idx += 1"
      ],
      "metadata": {
        "id": "4gTIvGWt6U-G"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## for analysis"
      ],
      "metadata": {
        "id": "Pl7Qj-1yg-YV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "import h5py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# --- File Paths (Must match where you saved them) ---\n",
        "HDF5_EMBEDDINGS_FILE = 'corpus_embeddings.h5'\n",
        "INDEX_FILE = 'corpus_index.pkl'\n",
        "\n",
        "# --- 0. Load the Index (Do this once, before calling the function) ---\n",
        "# index_df = pd.read_pickle(INDEX_FILE)\n",
        "\n",
        "def get_target_vectors_from_store(target_word, index_df):\n",
        "    \"\"\"\n",
        "    Retrieves the contextualized BERT vectors for all occurrences of a target word,\n",
        "    reading the embedding data directly from the HDF5 file.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Query the Index\n",
        "    # Find all rows in the index where the 'target_word' occurs (case-insensitive)\n",
        "    matches = index_df[index_df['target_word'] == target_word.lower()]\n",
        "\n",
        "    if matches.empty:\n",
        "        return []\n",
        "\n",
        "    target_vectors = []\n",
        "\n",
        "    # 2. Access HDF5 File\n",
        "    # Use 'with' to ensure the file is closed automatically\n",
        "    with h5py.File(HDF5_EMBEDDINGS_FILE, 'r') as hf:\n",
        "\n",
        "        # 3. Iterate through matches and extract the specific vector\n",
        "        for _, row in matches.iterrows():\n",
        "            sent_id = row['sentence_id']\n",
        "            token_indices = row['token_indices']\n",
        "\n",
        "            # Retrieve the full embedding array for the sentence from HDF5\n",
        "            # The dataset name is f'sent_{sent_id}'\n",
        "            try:\n",
        "                # Use dataset name indexing (e.g., hf['sent_0']) and load the data [()]\n",
        "                full_sent_embedding = hf[f'sent_{sent_id}'][()]\n",
        "            except KeyError:\n",
        "                print(f\"Warning: Dataset 'sent_{sent_id}' not found in HDF5 file.\")\n",
        "                continue\n",
        "\n",
        "            # The embedding for the word is the average of its sub-token embeddings\n",
        "            word_vector = np.mean(full_sent_embedding[token_indices], axis=0)\n",
        "            target_vectors.append(word_vector)\n",
        "\n",
        "    return target_vectors\n",
        "\n",
        "# Example Call:\n",
        "# all_embeddings_for_apple = get_target_vectors_from_store(\"apple\", index_df)\n",
        "# --- 2. Function to find optimal K and perform Clustering (from previous answer) ---\n",
        "def find_optimal_k_and_cluster(X, max_k=5):\n",
        "    \"\"\"\n",
        "    Finds the optimal K using Silhouette Score and performs K-means,\n",
        "    while safeguarding against having too few samples.\n",
        "    \"\"\"\n",
        "    n_instances = X.shape[0]\n",
        "\n",
        "    # --- Initial Checks ---\n",
        "    if n_instances < 2:\n",
        "        # If there's 0 or 1 instance, clustering is meaningless\n",
        "        print(f\"   --> Warning: Only {n_instances} instance(s) found. Cannot cluster.\")\n",
        "        return 1, np.zeros(n_instances, dtype=int)\n",
        "\n",
        "    # K must be less than the number of instances for Silhouette Score\n",
        "    # The range should be from 2 up to n_instances - 1\n",
        "    k_range = range(2, min(max_k, n_instances - 1) + 1)\n",
        "\n",
        "    # If the range is empty (e.g., n_instances=2, range is just 2, min is 1),\n",
        "    # we can only assign K=1 (no distinct senses found).\n",
        "    if len(k_range) == 0:\n",
        "        print(f\"   --> Warning: Only {n_instances} instances. Defaulting to K=1.\")\n",
        "        return 1, np.zeros(n_instances, dtype=int)\n",
        "\n",
        "    best_k = k_range[0] # Start with the smallest possible K (usually 2)\n",
        "    best_score = -1.0\n",
        "\n",
        "    print(f\"   --> Testing K in range {list(k_range)}...\")\n",
        "\n",
        "    for k in k_range:\n",
        "        kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto')\n",
        "        labels = kmeans.fit_predict(X)\n",
        "\n",
        "        # This calculation is now safe because k is guaranteed to be < n_instances\n",
        "        score = silhouette_score(X, labels)\n",
        "\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_k = k\n",
        "\n",
        "    # Final clustering with the optimal K\n",
        "    kmeans = KMeans(n_clusters=best_k, random_state=42, n_init='auto')\n",
        "    final_labels = kmeans.fit_predict(X)\n",
        "\n",
        "    print(f\"   --> Optimal K determined: {best_k} (Silhouette: {best_score:.4f})\")\n",
        "    return best_k, final_labels\n",
        "\n",
        "# The word 'deposit' from your corpus likely had n_instances = 3.\n",
        "# The old code tried K=3, which failed.\n",
        "# The new code will cap K at min(max_k, 3-1) = min(5, 2) = 2. It will only test K=2."
      ],
      "metadata": {
        "id": "rhgL0_J2hHbM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Process & Save the index & embeddings"
      ],
      "metadata": {
        "id": "xHE5rzl8gcoC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # --- Run the Pre-calculation ---\n",
        "start_time = time.time()\n",
        "\n",
        "# IMPORTANT: h5f must be opened before the function call\n",
        "# h5f = h5py.File('corpus_embeddings.h5', 'w')\n",
        "process_corpus_general_batched(clean_corpus, model, tokenizer, BATCH_SIZE, device, h5f)\n",
        "h5f.close() # Close after the loop finishes\n",
        "\n",
        "# --- CRITICAL: SAVE THE INDEX HERE ---\n",
        "import pandas as pd\n",
        "index_df = pd.DataFrame(index_data)\n",
        "INDEX_FILE = 'corpus_index.pkl'\n",
        "index_df.to_pickle(INDEX_FILE)\n",
        "\n",
        "print(f\"Processing complete. Index saved to {INDEX_FILE}\")\n",
        "print(f\"Time taken for Stage 1 (BERT Inference): {time.time() - start_time:.2f} seconds.\")\n",
        "print(f\"Index created for {len(index_df)} instances of ALL content words.\")"
      ],
      "metadata": {
        "id": "kHBNBu4WgbHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test & Analysis"
      ],
      "metadata": {
        "id": "RAFp0NekimXu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# Assuming find_optimal_k_and_cluster is available\n",
        "\n",
        "# --- File Paths (Must match where you saved them) ---\n",
        "HDF5_EMBEDDINGS_FILE = 'corpus_embeddings.h5'\n",
        "INDEX_FILE = 'corpus_index.pkl'\n",
        "\n",
        "# --- 0. Load the Index and Ensure Files Exist ---\n",
        "try:\n",
        "    # Load the index DataFrame from the saved pickle file\n",
        "    index_df = pd.read_pickle(INDEX_FILE)\n",
        "    print(f\"‚úÖ Loaded index with {len(index_df)} word occurrences.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"üõë Error: Index file '{INDEX_FILE}' not found. Did Stage 1 complete successfully?\")\n",
        "    exit()\n",
        "\n",
        "# Filter out words with too few instances to cluster (e.g., less than 2)\n",
        "word_counts = index_df.groupby('target_word').size()\n",
        "plausible_words = word_counts[word_counts >= 2].index.tolist()\n",
        "print(f\"Found {len(plausible_words)} words with 2 or more instances for clustering.\")\n",
        "\n",
        "\n",
        "# --- 1. Define Words to Analyze ---\n",
        "# Use a specific list, or sample from the plausible_words list\n",
        "WORDS_TO_ANALYZE = [\"man\", \"life\", \"light\", \"spirit\", \"son\", \"body\", \"bank\"]\n",
        "\n",
        "print(\"\\n--- Starting Stage 2: Efficient Sense Induction from Disk ---\")\n",
        "\n",
        "for word in WORDS_TO_ANALYZE:\n",
        "    run_start = time.time()\n",
        "\n",
        "    # A. Retrieve vectors quickly\n",
        "    # The function handles reading the correct embedding arrays from the HDF5 file\n",
        "    X_list = get_target_vectors_from_store(word, index_df)\n",
        "\n",
        "    if not X_list:\n",
        "        print(f\"  Skipping '{word}': No instances found or retrieval failed.\")\n",
        "        continue\n",
        "\n",
        "    # Convert the list of vectors to a numpy array for K-means\n",
        "    X = np.array(X_list)\n",
        "\n",
        "    # Extract the original sentences for interpretation\n",
        "    sentences = index_df[index_df['target_word'] == word.lower()]['sentence'].tolist()\n",
        "\n",
        "    # B. Find optimal K and cluster\n",
        "    # Note: max_k=5 is generally a good starting point for WSI\n",
        "    optimal_k, labels = find_optimal_k_and_cluster(X, max_k=5)\n",
        "\n",
        "    # C. Display Results\n",
        "    sense_clusters = {i: [] for i in range(optimal_k)}\n",
        "    for sentence, label in zip(sentences, labels):\n",
        "        sense_clusters[label].append(sentence)\n",
        "\n",
        "    run_end = time.time()\n",
        "    print(f\"\\n## üéØ Induced Senses for '{word}' (Run Time: {run_end - run_start:.4f}s) ##\")\n",
        "\n",
        "    for i, sentences_in_sense in sense_clusters.items():\n",
        "        print(f\"--- Sense Cluster {i+1} ({len(sentences_in_sense)} instances) ---\")\n",
        "\n",
        "        # Display up to 3 example sentences for brevity\n",
        "        for j, sentence in enumerate(sentences_in_sense[:3]):\n",
        "            print(f\"  - {sentence}\")\n",
        "        if len(sentences_in_sense) > 3:\n",
        "             print(\"  - ... (more instances)\")\n",
        "\n",
        "    print(\"-\" * 20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2ouPn9cIujp",
        "outputId": "b717c4fe-2088-44a4-cb01-6f955b7f3db0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Loaded index with 67209 word occurrences.\n",
            "Found 5257 words with 2 or more instances for clustering.\n",
            "\n",
            "--- Starting Stage 2: Efficient Sense Induction from Disk ---\n",
            "   --> Testing K in range [2, 3, 4, 5]...\n",
            "   --> Optimal K determined: 2 (Silhouette: 0.0919)\n",
            "\n",
            "## üéØ Induced Senses for 'man' (Run Time: 19.4505s) ##\n",
            "--- Sense Cluster 1 (293 instances) ---\n",
            "  - CHAPTER IX‚ÄîTHE MAN WITH THE BELL\n",
            "  - CHAPTER I‚ÄîMARIUS, WHILE SEEKING A GIRL IN A BONNET, ENCOUNTERS A MAN\n",
            "  - CHAPTER VII‚ÄîTHE MAN RECRUITED IN THE RUE DES BILLETTES\n",
            "  - ... (more instances)\n",
            "--- Sense Cluster 2 (430 instances) ---\n",
            "  - BOOK FIRST‚ÄîA JUST MAN\n",
            "  - CHAPTER X‚ÄîTHE MAN AROUSED\n",
            "  - MAN WHO MAY BE A RICH MAN\n",
            "  - ... (more instances)\n",
            "--------------------\n",
            "   --> Testing K in range [2, 3, 4, 5]...\n",
            "   --> Optimal K determined: 5 (Silhouette: 0.1284)\n",
            "\n",
            "## üéØ Induced Senses for 'life' (Run Time: 1.5005s) ##\n",
            "--- Sense Cluster 1 (15 instances) ---\n",
            "  - life, or add anything superfluous to his bare necessities.\n",
            "  - life!‚Äù\n",
            "  - life. An example, in short, etc. Besides, he was an atheist, like all\n",
            "  - ... (more instances)\n",
            "--- Sense Cluster 2 (48 instances) ---\n",
            "  - CHAPTER X‚ÄîRETURN OF THE SON WHO WAS PRODIGAL OF HIS LIFE\n",
            "  - midst of these distractions, these affections which absorbed his life,\n",
            "  - to the early portion of M. Myriel‚Äôs life? No one knew. Very few\n",
            "  - ... (more instances)\n",
            "--- Sense Cluster 3 (17 instances) ---\n",
            "  - the whole of the first portion of his life had been devoted to the\n",
            "  - never been pretty; her whole life, which had been nothing but a\n",
            "  - cloth, serges, and woollen galloons. Never in his whole life had M.\n",
            "  - ... (more instances)\n",
            "--- Sense Cluster 4 (50 instances) ---\n",
            "  - CHAPTER VI‚ÄîTHE AGONY OF DEATH AFTER THE AGONY OF LIFE\n",
            "  - of things, and which we call life. He gazed incessantly beyond this\n",
            "  - believe, enter into life: the Father is there.‚Äù When he descended from\n",
            "  - ... (more instances)\n",
            "--- Sense Cluster 5 (7 instances) ---\n",
            "  - sentenced for life. They are taken from the galleys and confronted with\n",
            "  - longer a matter of a few days in prison; it is the galleys for life.\n",
            "  - premeditation, and she was condemned for life.‚Äù\n",
            "  - ... (more instances)\n",
            "--------------------\n",
            "   --> Testing K in range [2, 3, 4, 5]...\n",
            "   --> Optimal K determined: 4 (Silhouette: 0.0685)\n",
            "\n",
            "## üéØ Induced Senses for 'light' (Run Time: 1.5658s) ##\n",
            "--- Sense Cluster 1 (54 instances) ---\n",
            "  - CHAPTER VIII‚ÄîTHE RAY OF LIGHT IN THE HOVEL\n",
            "  - provide for sex; a little matter enclosing a light; large eyes forever\n",
            "  - Bishop made him see light.\n",
            "  - ... (more instances)\n",
            "--- Sense Cluster 2 (41 instances) ---\n",
            "  - Just then a light flashed up at the end of the streets; a pine branch\n",
            "  - glittered in the bright light of midday. We were all ranged in lines on\n",
            "  - ‚ÄúThis lamp gives a very bad light,‚Äù said the Bishop.\n",
            "  - ... (more instances)\n",
            "--- Sense Cluster 3 (13 instances) ---\n",
            "  - light of your well-beloved dead in the depths of heaven.‚Äù He knew that\n",
            "  - Light; the Books of Kings call you Lord; Exodus calls you Providence;\n",
            "  - light of the expiring day the stranger perceived, in one of the gardens\n",
            "  - ... (more instances)\n",
            "--- Sense Cluster 4 (14 instances) ---\n",
            "  - CHAPTER X‚ÄîTHE BISHOP IN THE PRESENCE OF AN UNKNOWN LIGHT\n",
            "  - CHAPTER I‚ÄîFULL LIGHT\n",
            "  - CHAPTER II‚ÄîTHE STREET URCHIN AN ENEMY OF LIGHT\n",
            "  - ... (more instances)\n",
            "--------------------\n",
            "   --> Testing K in range [2, 3, 4, 5]...\n",
            "   --> Optimal K determined: 5 (Silhouette: 0.1008)\n",
            "\n",
            "## üéØ Induced Senses for 'spirit' (Run Time: 0.3480s) ##\n",
            "--- Sense Cluster 1 (5 instances) ---\n",
            "  - this verse in Genesis, _In the beginning, the spirit of God floated\n",
            "  - and the spirit of perdition, and I give it to God.‚Äù\n",
            "  - soul. I take it away from the spirit of perversity; I give it to the\n",
            "  - ... (more instances)\n",
            "--- Sense Cluster 2 (5 instances) ---\n",
            "  - traversed this grand and gentle spirit occupied with eternal things.\n",
            "  - himself, probably, what was passing in his spirit; he felt something\n",
            "  - formed the inner horizon of his spirit? Was he conscious of all that\n",
            "  - ... (more instances)\n",
            "--- Sense Cluster 3 (1 instances) ---\n",
            "  - Congregation of the Holy Spirit . . . . . . . . . .      150   ‚Äù\n",
            "--- Sense Cluster 4 (3 instances) ---\n",
            "  - pulled things to himself; the spirit of combat succeeded to the spirit\n",
            "  - pulled things to himself; the spirit of combat succeeded to the spirit\n",
            "  - proper to insist upon here, this war, which wounded the military spirit\n",
            "--- Sense Cluster 5 (7 instances) ---\n",
            "  - Thus Monseigneur Bienvenu also had his hour of party spirit, his hour\n",
            "  - intelligence and spirit which render one sensible to the mysterious\n",
            "  - over bushes, and presided over this merry-making with the spirit of a\n",
            "  - ... (more instances)\n",
            "--------------------\n",
            "   --> Testing K in range [2, 3, 4, 5]...\n",
            "   --> Optimal K determined: 2 (Silhouette: 0.1824)\n",
            "\n",
            "## üéØ Induced Senses for 'son' (Run Time: 0.1707s) ##\n",
            "--- Sense Cluster 1 (3 instances) ---\n",
            "  - son of a councillor of the Parliament of Aix; hence he belonged to the\n",
            "  - Gramont, son of the Duke Louis de Gramont, peer of France, colonel of\n",
            "  - an attorney‚Äôs by penning quibbles. He is the son of a former precentor\n",
            "--- Sense Cluster 2 (7 instances) ---\n",
            "  - CHAPTER X‚ÄîRETURN OF THE SON WHO WAS PRODIGAL OF HIS LIFE\n",
            "  - has his son away on service in the army, and his daughters at service\n",
            "  - French idea, which was making the tour of the world; beside the son of\n",
            "  - ... (more instances)\n",
            "--------------------\n",
            "   --> Testing K in range [2, 3, 4, 5]...\n",
            "   --> Optimal K determined: 4 (Silhouette: 0.1315)\n",
            "\n",
            "## üéØ Induced Senses for 'body' (Run Time: 0.3163s) ##\n",
            "--- Sense Cluster 1 (5 instances) ---\n",
            "  - person seemed made of a shadow; there was hardly sufficient body to\n",
            "  - silence of that taciturn legislative body, emboldened by catastrophe,\n",
            "  - had been a member of the legislative body of the Empire, and shared the\n",
            "  - ... (more instances)\n",
            "--- Sense Cluster 2 (5 instances) ---\n",
            "  - hands to afford a chance to nourish his soul as well as his body, and\n",
            "  - The human body has something of this tremor when the instant arrives in\n",
            "  - sometimes a crown-piece, a stone, a skeleton, a bleeding body,\n",
            "  - ... (more instances)\n",
            "--- Sense Cluster 3 (3 instances) ---\n",
            "  - body almost upright, his voice vibrating, was one of those\n",
            "  - body agitated by an imperceptible quiver and an unprecedented\n",
            "  - body? Did the veteran make himself disastrously felt in the leader? In\n",
            "--- Sense Cluster 4 (10 instances) ---\n",
            "  - ‚ÄúInspector Javert will apprehend the body of the Sieur Madeleine, mayor\n",
            "  - Sister Perp√©tue and Sister Simplice, who were watching beside the body\n",
            "  - the body still lives at Mont-Saint-Jean. His name is Dehaze. He was\n",
            "  - ... (more instances)\n",
            "--------------------\n",
            "   --> Testing K in range [2]...\n",
            "   --> Optimal K determined: 2 (Silhouette: 0.3364)\n",
            "\n",
            "## üéØ Induced Senses for 'bank' (Run Time: 0.0471s) ##\n",
            "--- Sense Cluster 1 (2 instances) ---\n",
            "  - its way, like him, to the right bank. This was of use to him. He could\n",
            "  - on their way to the right bank.\n",
            "--- Sense Cluster 2 (1 instances) ---\n",
            "  - evening, and the government bank certainly is not open at that hour.‚Äù\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hfym3lsiv8rg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract key words from clusters"
      ],
      "metadata": {
        "id": "JurG0LuKkTsj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get the closest words to the centroid of each word"
      ],
      "metadata": {
        "id": "4rDLK5Gvk-Xz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.spatial.distance import cosine\n",
        "import numpy as np\n",
        "\n",
        "def generate_sense_prototypes(sense_clusters, X_vectors, labels, model_vocab, top_n=5):\n",
        "    \"\"\"\n",
        "    Calculates the centroid for each induced sense cluster and finds the closest\n",
        "    global vocabulary words to that centroid using cosine similarity.\n",
        "\n",
        "    Args:\n",
        "        sense_clusters (dict): The dictionary {label: [sentences]}\n",
        "        X_vectors (np.array): All contextual embeddings for the target word.\n",
        "        labels (np.array): Cluster assignments for X_vectors.\n",
        "        model_vocab (dict): Dictionary mapping global words to their fixed vectors.\n",
        "        top_n (int): Number of prototype words to return per sense.\n",
        "    \"\"\"\n",
        "    interpretation = {}\n",
        "\n",
        "    # We assume vector dimensions match (768 for DistilBERT)\n",
        "    n_clusters = len(sense_clusters)\n",
        "\n",
        "    for sense_id in range(n_clusters):\n",
        "        # 1. Get vectors belonging to the current cluster\n",
        "        cluster_vectors = X_vectors[labels == sense_id]\n",
        "\n",
        "        if len(cluster_vectors) == 0:\n",
        "            interpretation[sense_id] = [\"No instances in cluster\"]\n",
        "            continue\n",
        "\n",
        "        # 2. Calculate the Centroid (Mean vector of the cluster)\n",
        "        centroid = np.mean(cluster_vectors, axis=0)\n",
        "\n",
        "        # 3. Find the closest words in the global vocabulary\n",
        "        similarity_scores = {}\n",
        "\n",
        "        # Iterate over the global vocabulary\n",
        "        for word, word_vector in model_vocab.items():\n",
        "            # Cosine similarity is 1 - cosine_distance. Higher value is better.\n",
        "            similarity = 1 - cosine(centroid, word_vector)\n",
        "            similarity_scores[word] = similarity\n",
        "\n",
        "        # 4. Sort and select top N words\n",
        "        sorted_prototypes = sorted(similarity_scores.items(), key=lambda item: item[1], reverse=True)\n",
        "\n",
        "        # Select the top N words (excluding the target word itself, if present)\n",
        "        top_prototypes = [word for word, score in sorted_prototypes if word != TARGET_WORD_LOWER][:top_n]\n",
        "\n",
        "        interpretation[sense_id] = top_prototypes\n",
        "\n",
        "    return interpretation"
      ],
      "metadata": {
        "id": "G5lUgxrGQsk9"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- IMPORTANT: Replace DUMMY_VOCAB with your actual loaded GloVe/Word2Vec model ---\n",
        "# Ensure your GLOBAL_VOCAB_VECTORS contains *768-dimensional* vectors\n",
        "# for comparison with DistilBERT embeddings.\n",
        "\n",
        "# Placeholder: Create a small dummy vocabulary for demonstration\n",
        "GLOBAL_VECTOR_DIM = 768\n",
        "DUMMY_VOCAB = {}\n",
        "# Add a few common English and example WSI words\n",
        "words_to_include = ['man', 'bishop', 'chapter', 'life', 'light', 'love']\n",
        "np.random.seed(42) # For reproducible dummy vectors\n",
        "\n",
        "for word in words_to_include:\n",
        "    # Create a random 768-dim vector\n",
        "    vec = np.random.rand(GLOBAL_VECTOR_DIM)\n",
        "    # Normalize the vector for accurate cosine similarity\n",
        "    DUMMY_VOCAB[word] = vec / np.linalg.norm(vec)\n",
        "\n",
        "GLOBAL_VOCAB_VECTORS = DUMMY_VOCAB\n",
        "\n",
        "print(f\"‚úÖ Global Vocabulary initialized with {len(GLOBAL_VOCAB_VECTORS)} words (768-dim).\")"
      ],
      "metadata": {
        "id": "lIdcuexuQwqt",
        "outputId": "43c384f7-187e-40bd-ad37-cfbf8eeb5569",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Global Vocabulary initialized with 6 words (768-dim).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Modified Stage 2: Sense Interpretation Integrated ---\n",
        "\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# Assuming find_optimal_k_and_cluster and get_target_vectors_from_store are defined\n",
        "\n",
        "print(\"\\n--- Starting Stage 2: Centroid Proximity Analysis ---\")\n",
        "\n",
        "for word in WORDS_TO_ANALYZE:\n",
        "    run_start = time.time()\n",
        "    TARGET_WORD_LOWER = word.lower() # Set global lowercase target for prototype exclusion\n",
        "\n",
        "    # A. Retrieve vectors quickly\n",
        "    X_list = get_target_vectors_from_store(word, index_df)\n",
        "\n",
        "    if not X_list:\n",
        "        print(f\"  Skipping '{word}': No instances found or retrieval failed.\")\n",
        "        continue\n",
        "\n",
        "    X = np.array(X_list)\n",
        "    sentences = index_df[index_df['target_word'] == TARGET_WORD_LOWER]['sentence'].tolist()\n",
        "\n",
        "    # B. Find optimal K and cluster\n",
        "    optimal_k, labels = find_optimal_k_and_cluster(X, max_k=5)\n",
        "\n",
        "    # C. Group Results\n",
        "    sense_clusters = {i: [] for i in range(optimal_k)}\n",
        "    for sentence, label in zip(sentences, labels):\n",
        "        sense_clusters[label].append(sentence)\n",
        "\n",
        "    # --- D. Generate Dictionary-like Meanings using Centroid Proximity ---\n",
        "    sense_prototypes = generate_sense_prototypes(\n",
        "        sense_clusters,\n",
        "        X,                  # The array of contextual vectors\n",
        "        labels,             # The cluster labels\n",
        "        GLOBAL_VOCAB_VECTORS, # The global fixed word embeddings\n",
        "        top_n=5\n",
        "    )\n",
        "\n",
        "    # --- E. Display Final Results with Prototypes ---\n",
        "    run_end = time.time()\n",
        "    print(f\"\\n## üéØ Induced Senses for '{word}' (K={optimal_k}) (Run Time: {run_end - run_start:.4f}s) ##\")\n",
        "\n",
        "    for i in range(optimal_k):\n",
        "        sentences_in_sense = sense_clusters.get(i, [])\n",
        "        prototypes = sense_prototypes.get(i, [\"(No prototypes found)\"])\n",
        "\n",
        "        print(f\"\\n--- Sense Cluster {i+1} ({len(sentences_in_sense)} instances) ---\")\n",
        "        print(f\"  Sense Prototype (Closest Global Words): **{', '.join(prototypes)}**\")\n",
        "\n",
        "        if sentences_in_sense:\n",
        "            print(\"  Example Sentence:\")\n",
        "            print(f\"  - {sentences_in_sense[0]}\")\n",
        "\n",
        "        print(\"-\" * 20)"
      ],
      "metadata": {
        "id": "oSzAuH6fQ1yR",
        "outputId": "52702445-6cc8-455d-d92b-eeb817ec8bf0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting Stage 2: Centroid Proximity Analysis ---\n",
            "   --> Testing K in range [2, 3, 4, 5]...\n",
            "   --> Optimal K determined: 2 (Silhouette: 0.0919)\n",
            "\n",
            "## üéØ Induced Senses for 'man' (K=2) (Run Time: 7.5968s) ##\n",
            "\n",
            "--- Sense Cluster 1 (293 instances) ---\n",
            "  Sense Prototype (Closest Global Words): **bishop, life, love, light, chapter**\n",
            "  Example Sentence:\n",
            "  - CHAPTER IX‚ÄîTHE MAN WITH THE BELL\n",
            "--------------------\n",
            "\n",
            "--- Sense Cluster 2 (430 instances) ---\n",
            "  Sense Prototype (Closest Global Words): **life, light, bishop, chapter, love**\n",
            "  Example Sentence:\n",
            "  - BOOK FIRST‚ÄîA JUST MAN\n",
            "--------------------\n",
            "   --> Testing K in range [2, 3, 4, 5]...\n",
            "   --> Optimal K determined: 5 (Silhouette: 0.1284)\n",
            "\n",
            "## üéØ Induced Senses for 'life' (K=5) (Run Time: 1.4910s) ##\n",
            "\n",
            "--- Sense Cluster 1 (15 instances) ---\n",
            "  Sense Prototype (Closest Global Words): **love, bishop, light, chapter, man**\n",
            "  Example Sentence:\n",
            "  - life, or add anything superfluous to his bare necessities.\n",
            "--------------------\n",
            "\n",
            "--- Sense Cluster 2 (48 instances) ---\n",
            "  Sense Prototype (Closest Global Words): **light, bishop, man, chapter, love**\n",
            "  Example Sentence:\n",
            "  - CHAPTER X‚ÄîRETURN OF THE SON WHO WAS PRODIGAL OF HIS LIFE\n",
            "--------------------\n",
            "\n",
            "--- Sense Cluster 3 (17 instances) ---\n",
            "  Sense Prototype (Closest Global Words): **light, bishop, man, love, chapter**\n",
            "  Example Sentence:\n",
            "  - the whole of the first portion of his life had been devoted to the\n",
            "--------------------\n",
            "\n",
            "--- Sense Cluster 4 (50 instances) ---\n",
            "  Sense Prototype (Closest Global Words): **light, bishop, chapter, love, man**\n",
            "  Example Sentence:\n",
            "  - CHAPTER VI‚ÄîTHE AGONY OF DEATH AFTER THE AGONY OF LIFE\n",
            "--------------------\n",
            "\n",
            "--- Sense Cluster 5 (7 instances) ---\n",
            "  Sense Prototype (Closest Global Words): **love, chapter, bishop, light, man**\n",
            "  Example Sentence:\n",
            "  - sentenced for life. They are taken from the galleys and confronted with\n",
            "--------------------\n",
            "   --> Testing K in range [2, 3, 4, 5]...\n",
            "   --> Optimal K determined: 4 (Silhouette: 0.0685)\n",
            "\n",
            "## üéØ Induced Senses for 'light' (K=4) (Run Time: 1.2225s) ##\n",
            "\n",
            "--- Sense Cluster 1 (54 instances) ---\n",
            "  Sense Prototype (Closest Global Words): **bishop, chapter, man, love, life**\n",
            "  Example Sentence:\n",
            "  - CHAPTER VIII‚ÄîTHE RAY OF LIGHT IN THE HOVEL\n",
            "--------------------\n",
            "\n",
            "--- Sense Cluster 2 (41 instances) ---\n",
            "  Sense Prototype (Closest Global Words): **bishop, chapter, love, man, life**\n",
            "  Example Sentence:\n",
            "  - Just then a light flashed up at the end of the streets; a pine branch\n",
            "--------------------\n",
            "\n",
            "--- Sense Cluster 3 (13 instances) ---\n",
            "  Sense Prototype (Closest Global Words): **bishop, chapter, love, life, man**\n",
            "  Example Sentence:\n",
            "  - light of your well-beloved dead in the depths of heaven.‚Äù He knew that\n",
            "--------------------\n",
            "\n",
            "--- Sense Cluster 4 (14 instances) ---\n",
            "  Sense Prototype (Closest Global Words): **bishop, chapter, love, life, man**\n",
            "  Example Sentence:\n",
            "  - CHAPTER X‚ÄîTHE BISHOP IN THE PRESENCE OF AN UNKNOWN LIGHT\n",
            "--------------------\n",
            "   --> Testing K in range [2, 3, 4, 5]...\n",
            "   --> Optimal K determined: 5 (Silhouette: 0.1008)\n",
            "\n",
            "## üéØ Induced Senses for 'spirit' (K=5) (Run Time: 0.2686s) ##\n",
            "\n",
            "--- Sense Cluster 1 (5 instances) ---\n",
            "  Sense Prototype (Closest Global Words): **light, bishop, chapter, life, man**\n",
            "  Example Sentence:\n",
            "  - this verse in Genesis, _In the beginning, the spirit of God floated\n",
            "--------------------\n",
            "\n",
            "--- Sense Cluster 2 (5 instances) ---\n",
            "  Sense Prototype (Closest Global Words): **light, bishop, life, chapter, man**\n",
            "  Example Sentence:\n",
            "  - traversed this grand and gentle spirit occupied with eternal things.\n",
            "--------------------\n",
            "\n",
            "--- Sense Cluster 3 (1 instances) ---\n",
            "  Sense Prototype (Closest Global Words): **life, chapter, bishop, light, man**\n",
            "  Example Sentence:\n",
            "  - Congregation of the Holy Spirit . . . . . . . . . .      150   ‚Äù\n",
            "--------------------\n",
            "\n",
            "--- Sense Cluster 4 (3 instances) ---\n",
            "  Sense Prototype (Closest Global Words): **light, bishop, life, chapter, love**\n",
            "  Example Sentence:\n",
            "  - pulled things to himself; the spirit of combat succeeded to the spirit\n",
            "--------------------\n",
            "\n",
            "--- Sense Cluster 5 (7 instances) ---\n",
            "  Sense Prototype (Closest Global Words): **bishop, light, life, chapter, man**\n",
            "  Example Sentence:\n",
            "  - Thus Monseigneur Bienvenu also had his hour of party spirit, his hour\n",
            "--------------------\n",
            "   --> Testing K in range [2, 3, 4, 5]...\n",
            "   --> Optimal K determined: 2 (Silhouette: 0.1824)\n",
            "\n",
            "## üéØ Induced Senses for 'son' (K=2) (Run Time: 0.1228s) ##\n",
            "\n",
            "--- Sense Cluster 1 (3 instances) ---\n",
            "  Sense Prototype (Closest Global Words): **chapter, life, light, bishop, love**\n",
            "  Example Sentence:\n",
            "  - son of a councillor of the Parliament of Aix; hence he belonged to the\n",
            "--------------------\n",
            "\n",
            "--- Sense Cluster 2 (7 instances) ---\n",
            "  Sense Prototype (Closest Global Words): **chapter, light, life, bishop, love**\n",
            "  Example Sentence:\n",
            "  - CHAPTER X‚ÄîRETURN OF THE SON WHO WAS PRODIGAL OF HIS LIFE\n",
            "--------------------\n",
            "   --> Testing K in range [2, 3, 4, 5]...\n",
            "   --> Optimal K determined: 4 (Silhouette: 0.1315)\n",
            "\n",
            "## üéØ Induced Senses for 'body' (K=4) (Run Time: 0.2496s) ##\n",
            "\n",
            "--- Sense Cluster 1 (5 instances) ---\n",
            "  Sense Prototype (Closest Global Words): **bishop, life, light, chapter, man**\n",
            "  Example Sentence:\n",
            "  - person seemed made of a shadow; there was hardly sufficient body to\n",
            "--------------------\n",
            "\n",
            "--- Sense Cluster 2 (5 instances) ---\n",
            "  Sense Prototype (Closest Global Words): **life, light, chapter, bishop, man**\n",
            "  Example Sentence:\n",
            "  - hands to afford a chance to nourish his soul as well as his body, and\n",
            "--------------------\n",
            "\n",
            "--- Sense Cluster 3 (3 instances) ---\n",
            "  Sense Prototype (Closest Global Words): **bishop, life, love, chapter, light**\n",
            "  Example Sentence:\n",
            "  - body almost upright, his voice vibrating, was one of those\n",
            "--------------------\n",
            "\n",
            "--- Sense Cluster 4 (10 instances) ---\n",
            "  Sense Prototype (Closest Global Words): **bishop, love, life, light, man**\n",
            "  Example Sentence:\n",
            "  - ‚ÄúInspector Javert will apprehend the body of the Sieur Madeleine, mayor\n",
            "--------------------\n",
            "   --> Testing K in range [2]...\n",
            "   --> Optimal K determined: 2 (Silhouette: 0.3364)\n",
            "\n",
            "## üéØ Induced Senses for 'bank' (K=2) (Run Time: 0.0442s) ##\n",
            "\n",
            "--- Sense Cluster 1 (2 instances) ---\n",
            "  Sense Prototype (Closest Global Words): **bishop, love, chapter, light, life**\n",
            "  Example Sentence:\n",
            "  - its way, like him, to the right bank. This was of use to him. He could\n",
            "--------------------\n",
            "\n",
            "--- Sense Cluster 2 (1 instances) ---\n",
            "  Sense Prototype (Closest Global Words): **bishop, light, chapter, love, man**\n",
            "  Example Sentence:\n",
            "  - evening, and the government bank certainly is not open at that hour.‚Äù\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63c4c506"
      },
      "source": [
        "# Frequency Analysis\n",
        "Filter the 'target_word' column in `index_df` to remove stop words, count the frequency of the remaining words, and then display the top N most frequent non-stop words along with their counts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfab3540"
      },
      "source": [
        "## Filter and Count Non-Stop Words\n",
        "\n",
        "### Subtask:\n",
        "Filter the 'target_word' column in `index_df` to remove stop words, then count the frequency of the remaining words.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ee645f5"
      },
      "source": [
        "**Reasoning**:\n",
        "Filter the 'target_word' column in `index_df` to remove stop words and then count the frequency of the remaining words.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f860a416",
        "outputId": "a0672534-2864-46aa-a146-59ad27f0f247"
      },
      "source": [
        "non_stop_words = index_df[~index_df['target_word'].isin(STOP_WORDS)]['target_word']\n",
        "word_frequencies = non_stop_words.value_counts()\n",
        "\n",
        "print(\"Frequency of non-stop words:\")\n",
        "print(word_frequencies.head())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frequency of non-stop words:\n",
            "target_word\n",
            "man        723\n",
            "said       636\n",
            "chapter    480\n",
            "little     382\n",
            "good       304\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48976e3a"
      },
      "source": [
        "**Reasoning**:\n",
        "To complete the subtask, the next step is to explicitly display the top N (e.g., 10) most frequent non-stop words along with their counts, as requested.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5f3cf93",
        "outputId": "e0a70e96-31b9-4018-8944-7edd81b27896"
      },
      "source": [
        "N = 50 # Define N for top N words\n",
        "print(f\"\\nTop {N} most frequent non-stop words:\\n{word_frequencies.head(N)}\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 50 most frequent non-stop words:\n",
            "target_word\n",
            "man          723\n",
            "said         636\n",
            "chapter      480\n",
            "little       382\n",
            "good         304\n",
            "bishop       289\n",
            "jean         289\n",
            "time         278\n",
            "old          274\n",
            "made         261\n",
            "child        236\n",
            "madeleine    219\n",
            "nothing      215\n",
            "moment       206\n",
            "day          201\n",
            "woman        196\n",
            "door         193\n",
            "first        192\n",
            "men          188\n",
            "eyes         187\n",
            "still        185\n",
            "see          184\n",
            "say          181\n",
            "well         178\n",
            "hand         167\n",
            "great        165\n",
            "one          163\n",
            "even         162\n",
            "monsieur     161\n",
            "seemed       160\n",
            "sort         160\n",
            "thought      156\n",
            "took         155\n",
            "years        154\n",
            "people       154\n",
            "head         154\n",
            "night        151\n",
            "madame       150\n",
            "long         146\n",
            "let          143\n",
            "seen         140\n",
            "name         140\n",
            "know         140\n",
            "longer       139\n",
            "god          138\n",
            "life         137\n",
            "place        136\n",
            "come         134\n",
            "way          133\n",
            "poor         133\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R-M227323jIb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}