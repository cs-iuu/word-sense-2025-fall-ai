{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cs-iuu/word-sense-2025-fall-ai/blob/main/notebooks/15.1.wsi_en_cht.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Preprocessing: Extract Common Nouns"
      ],
      "metadata": {
        "id": "OwnCnpqk0M7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy jieba pandas --break-system-packages\n",
        "!python -m spacy download en_core_web_sm\n",
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "ZDh6BuKL3z-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch transformers numpy pandas --break-system-packages"
      ],
      "metadata": {
        "id": "1uzzTQ5ogE7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn numpy pandas --break-system-packages"
      ],
      "metadata": {
        "id": "SATWTZIEgLXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk scipy matplotlib seaborn pandas numpy --break-system-packages"
      ],
      "metadata": {
        "id": "p8FfnACQgObB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Step 2: Preprocessing\n",
        "======================\n",
        "Tokenizes, POS-tags, and lemmatizes both corpora.\n",
        "Extracts common nouns only (no proper nouns, pronouns, or stopwords).\n",
        "Applies a minimum frequency threshold to filter rare words.\n",
        "\n",
        "Outputs:\n",
        "  - data/english_nouns.csv   : lemma, verse_id, token, context (full verse)\n",
        "  - data/chinese_nouns.csv   : lemma, verse_id, token, context\n",
        "  - data/english_noun_freq.csv\n",
        "  - data/chinese_noun_freq.csv\n",
        "\n",
        "Usage:\n",
        "  pip install spacy jieba pandas --break-system-packages\n",
        "  python -m spacy download en_core_web_sm\n",
        "  python 02_preprocessing.py\n",
        "\n",
        "Design decisions (paper §3.2):\n",
        "  - English: spaCy en_core_web_sm for tokenization, POS, lemmatization\n",
        "  - Chinese: jieba for word segmentation + custom POS (jieba.posseg)\n",
        "  - POS filters: English NOUN tag; Chinese POS prefix 'n' (common noun)\n",
        "  - Proper noun exclusion: English PROPN tag excluded; Chinese 'nr','ns','nt','nz' excluded\n",
        "  - Minimum frequency: MIN_FREQ = 30 (ensures sufficient WSI context)\n",
        "  - Stopwords: NLTK English stopwords; custom Chinese stopword list\n",
        "\"\"\"\n",
        "\n",
        "import re\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "\n",
        "# ─── Configuration ────────────────────────────────────────────────────────────\n",
        "\n",
        "DATA_DIR = Path(\"/content\") / \"bible_data\"\n",
        "MIN_FREQ = 30          # Minimum occurrences per lemma for WSI\n",
        "MAX_CONTEXT_LEN = 512  # Characters — prevents overlong inputs to transformers\n",
        "\n",
        "# Chinese POS tags for common nouns (jieba.posseg notation)\n",
        "ZH_NOUN_PREFIXES = {\"n\"}           # Common noun prefix\n",
        "ZH_EXCLUDE_TAGS  = {\"nr\", \"ns\", \"nt\", \"nz\", \"nw\"}  # Proper nouns to exclude\n",
        "\n",
        "# ── Theological proper noun exclusion lists ───────────────────────────────────\n",
        "# These terms are proper nouns in English (God, Lord, Christ etc.) — excluded\n",
        "# by spaCy's PROPN tag — but are tagged as common nouns n by jieba in Chinese\n",
        "# due to the absence of capitalisation. They must be excluded explicitly from\n",
        "# the Chinese data to ensure cross-lingual comparability.\n",
        "#\n",
        "# English side: spaCy correctly tags God/Lord/Christ as PROPN (excluded).\n",
        "# Exception: \"Spirit\" (Holy Spirit) is sometimes tagged NOUN by spaCy, so it\n",
        "# is added to EN_THEOLOGICAL_EXCLUDE as a lemma-level backstop.\n",
        "#\n",
        "# Borderline cases kept in both languages:\n",
        "#   先知/prophet  — generic occupational noun, polysemous, common in both\n",
        "#   天使/angel    — generic supernatural being, common noun in both\n",
        "#   魔鬼/devil    — common noun in EN; jieba tags n in ZH\n",
        "\n",
        "ZH_THEOLOGICAL_EXCLUDE = {\n",
        "    # Core deity names / titles\n",
        "    \"神\",     # God (most frequent — 1244 occurrences)\n",
        "    \"主\",     # Lord\n",
        "    \"上帝\",   # God (formal)\n",
        "    \"耶和華\", # Yahweh / LORD\n",
        "    \"基督\",   # Christ\n",
        "    \"耶穌\",   # Jesus (also usually tagged nr, but belt-and-suspenders)\n",
        "    \"聖靈\",   # Holy Spirit\n",
        "    \"聖神\",   # Holy Spirit (alternate form in some CUV editions)\n",
        "    \"彌賽亞\", # Messiah\n",
        "    # Adversarial proper nouns\n",
        "    \"撒但\",   # Satan\n",
        "    \"別西卜\", # Beelzebub\n",
        "}\n",
        "\n",
        "EN_THEOLOGICAL_EXCLUDE = {\n",
        "    # Lemma-level backstop for cases where spaCy tags as NOUN not PROPN\n",
        "    \"spirit\",    # \"Holy Spirit\" — spaCy inconsistently tags as NOUN\n",
        "    \"ghost\",     # \"Holy Ghost\" (KJV form; rare in NIV but present)\n",
        "}\n",
        "\n",
        "# Path to custom jieba dictionary for biblical proper names\n",
        "JIEBA_DICT_PATH = Path(\"/content\") / \"bible_data\" / \"jieba_biblical_dict.txt\"\n",
        "\n",
        "# ── Post-segmentation POS correction ─────────────────────────────────────────\n",
        "# jieba's POS tagger runs independently of the segmentation dictionary and\n",
        "# can assign incorrect tags even for dictionary entries. These words are\n",
        "# forced to tag n after segmentation regardless of what the POS tagger assigned.\n",
        "#\n",
        "# 地: jieba assigns uv (虛詞/copular particle) in classical subject-predicate\n",
        "#     constructions like 地是空虛混沌 (Gen.1.2) because it parses 地 as a\n",
        "#     topic marker rather than a subject noun. This is a known jieba limitation\n",
        "#     with literary Chinese. Since English \"earth\" (freq=739) is always tagged\n",
        "#     NOUN by spaCy, forcing 地 to n is required for cross-lingual comparability.\n",
        "ZH_FORCE_NOUN_TAG = {\n",
        "    \"地\",   # earth/ground/land — incorrectly tagged uv in copular constructions\n",
        "}\n",
        "\n",
        "# ─── English Preprocessing ────────────────────────────────────────────────────\n",
        "\n",
        "\n",
        "def preprocess_english(verse_csv: Path) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Process English verses with spaCy.\n",
        "    Returns long-format DataFrame: one row per noun occurrence.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        import spacy\n",
        "    except ImportError:\n",
        "        raise ImportError(\"Run: pip install spacy --break-system-packages && python -m spacy download en_core_web_sm\")\n",
        "\n",
        "    print(\"  [EN] Loading spaCy model…\")\n",
        "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"textcat\"])\n",
        "\n",
        "    df = pd.read_csv(verse_csv)\n",
        "    print(f\"  [EN] Processing {len(df):,} verses…\")\n",
        "\n",
        "    import time\n",
        "    records   = []\n",
        "    texts     = df[\"text\"].tolist()\n",
        "    verse_ids = df[\"verse_id\"].tolist()\n",
        "    total     = len(texts)\n",
        "    t0        = time.time()\n",
        "\n",
        "    for i, (doc, vid) in enumerate(zip(nlp.pipe(texts, batch_size=512), verse_ids), 1):\n",
        "        context = doc.text[:MAX_CONTEXT_LEN]\n",
        "        for token in doc:\n",
        "            # Keep only common nouns; exclude proper nouns and pronouns\n",
        "            if (\n",
        "                token.pos_ == \"NOUN\"\n",
        "                and not token.is_stop\n",
        "                and not token.is_punct\n",
        "                and len(token.lemma_) > 1\n",
        "                and token.lemma_.isalpha()\n",
        "                and token.lemma_.lower() not in EN_THEOLOGICAL_EXCLUDE\n",
        "            ):\n",
        "                records.append({\n",
        "                    \"verse_id\": vid,\n",
        "                    \"token\":    token.text,\n",
        "                    \"lemma\":    token.lemma_.lower(),\n",
        "                    \"context\":  context,\n",
        "                })\n",
        "        if i % 1000 == 0 or i == total:\n",
        "            elapsed = time.time() - t0\n",
        "            rate    = i / elapsed if elapsed > 0 else 0\n",
        "            eta_min = (total - i) / rate / 60 if rate > 0 else 0\n",
        "            print(f\"    … {i:,}/{total:,} verses  \"\n",
        "                  f\"({rate:.0f} v/s)  \"\n",
        "                  f\"ETA {eta_min:.1f} min  \"\n",
        "                  f\"nouns so far: {len(records):,}\",\n",
        "                  end=\"\\r\")\n",
        "\n",
        "    elapsed_total = time.time() - t0\n",
        "    print(f\"\\n  [EN] Done in {elapsed_total/60:.1f} min. \"\n",
        "          f\"Extracted {len(records):,} noun occurrences.\")\n",
        "    return pd.DataFrame(records)\n",
        "\n",
        "\n",
        "# ─── Chinese Preprocessing ────────────────────────────────────────────────────\n",
        "\n",
        "\n",
        "def preprocess_chinese(verse_csv: Path) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Process Chinese verses with jieba (word segmentation + POS tagging).\n",
        "    Returns long-format DataFrame: one row per noun occurrence.\n",
        "    \"\"\"\n",
        "    import time\n",
        "    try:\n",
        "        import jieba\n",
        "        import jieba.posseg as pseg\n",
        "    except ImportError:\n",
        "        raise ImportError(\"Run: pip install jieba --break-system-packages\")\n",
        "\n",
        "    # Silence jieba logging FIRST, before any other jieba calls\n",
        "    jieba.setLogLevel(\"ERROR\")\n",
        "\n",
        "    # Load custom dictionary\n",
        "    if JIEBA_DICT_PATH.exists():\n",
        "        jieba.load_userdict(str(JIEBA_DICT_PATH))\n",
        "        print(f\"  [ZH] Loaded custom dictionary: {JIEBA_DICT_PATH.name}\", flush=True)\n",
        "    else:\n",
        "        print(f\"  [ZH] WARNING: custom dictionary not found at {JIEBA_DICT_PATH}\", flush=True)\n",
        "\n",
        "    # Force jieba to build its internal trie NOW with a visible message.\n",
        "    # Without this explicit call, the first pseg.cut() silently blocks\n",
        "    # for 30-60 seconds before any progress lines appear.\n",
        "    print(\"  [ZH] Building jieba model (one-time, ~10-30s)...\", flush=True)\n",
        "    jieba.initialize()\n",
        "    print(\"  [ZH] Model ready.\", flush=True)\n",
        "\n",
        "    zh_stopwords = _load_chinese_stopwords()\n",
        "\n",
        "    df    = pd.read_csv(verse_csv)\n",
        "    total = len(df)\n",
        "    print(f\"  [ZH] Processing {total:,} verses...\", flush=True)\n",
        "    t0 = time.time()\n",
        "\n",
        "    records = []\n",
        "    for i, row in enumerate(df.itertuples(index=False), 1):\n",
        "        verse_id = row.verse_id\n",
        "        text     = str(row.text)\n",
        "        context  = text[:MAX_CONTEXT_LEN]\n",
        "\n",
        "        for word, flag in pseg.cut(text):\n",
        "            flag_str = str(flag)\n",
        "            # Force known mis-tagged tokens to correct POS\n",
        "            if word in ZH_FORCE_NOUN_TAG:\n",
        "                flag_str = \"n\"\n",
        "            if (\n",
        "                flag_str[:1] in ZH_NOUN_PREFIXES\n",
        "                and flag_str not in ZH_EXCLUDE_TAGS\n",
        "                and word not in zh_stopwords\n",
        "                and word not in ZH_THEOLOGICAL_EXCLUDE\n",
        "                and len(word) >= 1\n",
        "                and not word.isdigit()\n",
        "            ):\n",
        "                records.append({\n",
        "                    \"verse_id\": verse_id,\n",
        "                    \"token\":    word,\n",
        "                    \"lemma\":    word,\n",
        "                    \"context\":  context,\n",
        "                })\n",
        "\n",
        "        # Progress every 100 verses — print on new lines so nothing is missed\n",
        "        if i % 100 == 0 or i == total:\n",
        "            elapsed = time.time() - t0\n",
        "            rate    = i / elapsed if elapsed > 0 else 0\n",
        "            eta_min = (total - i) / rate / 60 if rate > 0 else 0\n",
        "            print(\n",
        "                f\"    ... {i:,}/{total:,} verses\"\n",
        "                f\"  ({rate:.0f} v/s)\"\n",
        "                f\"  ETA {eta_min:.1f} min\"\n",
        "                f\"  nouns: {len(records):,}\",\n",
        "                flush=True\n",
        "            )\n",
        "\n",
        "    elapsed_total = time.time() - t0\n",
        "    print(f\"  [ZH] Done in {elapsed_total/60:.1f} min. \"\n",
        "          f\"Extracted {len(records):,} noun occurrences.\")\n",
        "    return pd.DataFrame(records)\n",
        "\n",
        "def _load_chinese_stopwords() -> set:\n",
        "    \"\"\"\n",
        "    Chinese function word stoplist for CUV Traditional (CHT) text.\n",
        "\n",
        "    Design decisions:\n",
        "    ─────────────────────────────────────────────────────────────\n",
        "    1. CHT variants included alongside CHS equivalents for all\n",
        "       characters that differ between scripts (說/说, 會/会, etc.)\n",
        "\n",
        "    2. 人 is NOT a stopword. It is a genuine common noun meaning\n",
        "       \"person / people / man / humanity\" and is highly polysemous\n",
        "       in biblical text. Jieba tags it as n (common noun) in most\n",
        "       contexts, so it passes the POS filter correctly. Removing it\n",
        "       would discard one of the most semantically rich words in the\n",
        "       corpus.\n",
        "\n",
        "    3. Pronouns (他/她/祂/你/我 etc.) are NOT listed here. They are\n",
        "       tagged by jieba as r (pronoun), which is already excluded by\n",
        "       the POS filter (we keep only n* tags). Listing them would be\n",
        "       redundant. The various gendered and honorific variants\n",
        "       (他/她/它/祂) all carry the r tag and are excluded uniformly.\n",
        "\n",
        "    4. This list covers only high-frequency grammatical function\n",
        "       words that jieba may occasionally mis-tag as nouns.\n",
        "       It is intentionally conservative.\n",
        "    ─────────────────────────────────────────────────────────────\n",
        "    \"\"\"\n",
        "    return {\n",
        "        # Structural particles (occasionally mis-tagged as n by jieba)\n",
        "        # NOTE: 地 is intentionally NOT listed here.\n",
        "        # In CUV literary style, 地 is overwhelmingly used as a noun\n",
        "        # (earth/land/ground) matching English \"earth\" (freq=739).\n",
        "        # The adverbial particle use of 地 is rare in classical biblical text.\n",
        "        # Removing it would create an asymmetry with English where \"earth\"\n",
        "        # is correctly retained as a high-frequency common noun.\n",
        "        \"的\", \"得\",\n",
        "        # Aspect markers — CHT: 著, CHS: 着\n",
        "        \"了\", \"著\", \"着\",\n",
        "        # Conjunctions / connectives\n",
        "        \"和\", \"與\", \"与\", \"及\", \"或\", \"但\", \"而\", \"且\",\n",
        "        # Adverbs sometimes mis-tagged\n",
        "        \"也\", \"都\", \"就\", \"才\", \"又\", \"還\", \"还\", \"已\",\n",
        "        \"很\", \"更\", \"最\", \"太\", \"非常\",\n",
        "        # Negation\n",
        "        \"不\", \"沒有\", \"没有\", \"未\", \"無\", \"无\",\n",
        "        # Existential / copular\n",
        "        \"是\", \"有\", \"在\",\n",
        "        # Determiners / quantifiers\n",
        "        \"一\", \"這\", \"这\", \"那\", \"各\", \"每\", \"某\", \"其\",\n",
        "        # Directional / locative words with no sense variation\n",
        "        \"上\", \"下\", \"中\", \"內\", \"内\", \"外\", \"前\", \"後\", \"后\",\n",
        "        \"裡\", \"里\", \"間\", \"间\",\n",
        "        # Common verbs jieba occasionally tags as nouns in CUV\n",
        "        \"說\", \"说\", \"看\", \"去\", \"來\", \"来\", \"到\", \"給\", \"给\",\n",
        "        \"要\", \"會\", \"会\",\n",
        "    }\n",
        "\n",
        "\n",
        "# ─── Frequency Filtering ──────────────────────────────────────────────────────\n",
        "\n",
        "\n",
        "def apply_frequency_filter(df: pd.DataFrame, min_freq: int = MIN_FREQ) -> tuple:\n",
        "    \"\"\"\n",
        "    Keep only lemmas appearing at least `min_freq` times.\n",
        "    Returns (filtered_df, freq_df).\n",
        "    \"\"\"\n",
        "    freq = df.groupby(\"lemma\").size().reset_index(name=\"count\")\n",
        "    freq = freq.sort_values(\"count\", ascending=False)\n",
        "    valid_lemmas = set(freq[freq[\"count\"] >= min_freq][\"lemma\"])\n",
        "    filtered = df[df[\"lemma\"].isin(valid_lemmas)].copy()\n",
        "    return filtered, freq"
      ],
      "metadata": {
        "id": "iApWnLav0Ltb"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"Step 2: Preprocessing\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ── English ──────────────────────────────────────────────────\n",
        "en_raw = preprocess_english(DATA_DIR / \"english_verses.csv\")\n",
        "en_filtered, en_freq = apply_frequency_filter(en_raw)\n",
        "en_filtered.to_csv(DATA_DIR / \"english_nouns.csv\", index=False, encoding=\"utf-8\")\n",
        "en_freq.to_csv(DATA_DIR / \"english_noun_freq.csv\", index=False, encoding=\"utf-8\")\n",
        "print(f\"  [EN] {en_filtered['lemma'].nunique():,} lemmas ≥ {MIN_FREQ} occurrences retained.\")\n",
        "\n",
        "# ── Chinese ───────────────────────────────────────────────────\n",
        "zh_raw = preprocess_chinese(DATA_DIR / \"chinese_verses.csv\")\n",
        "zh_filtered, zh_freq = apply_frequency_filter(zh_raw)\n",
        "zh_filtered.to_csv(DATA_DIR / \"chinese_nouns.csv\", index=False, encoding=\"utf-8\")\n",
        "zh_freq.to_csv(DATA_DIR / \"chinese_noun_freq.csv\", index=False, encoding=\"utf-8\")\n",
        "print(f\"  [ZH] {zh_filtered['lemma'].nunique():,} lemmas ≥ {MIN_FREQ} occurrences retained.\")\n",
        "\n",
        "# ── Summary ───────────────────────────────────────────────────\n",
        "print(\"\\n── Preprocessing Summary ──\")\n",
        "print(f\"  EN noun tokens (filtered) : {len(en_filtered):,}\")\n",
        "print(f\"  EN unique lemmas          : {en_filtered['lemma'].nunique():,}\")\n",
        "print(f\"  ZH noun tokens (filtered) : {len(zh_filtered):,}\")\n",
        "print(f\"  ZH unique lemmas          : {zh_filtered['lemma'].nunique():,}\")\n",
        "\n",
        "print(\"\\n  Top 10 English nouns:\")\n",
        "print(en_freq.head(10).to_string(index=False))\n",
        "print(\"\\n  Top 10 Chinese nouns:\")\n",
        "print(zh_freq.head(10).to_string(index=False))\n",
        "\n",
        "print(\"\\n✓ Step 2 complete.\\n\")\n"
      ],
      "metadata": {
        "id": "ldNNKu9q0emn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Extract Context Embeddings"
      ],
      "metadata": {
        "id": "1CZa8ZJVWMeN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from pathlib import Path\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from typing import List, Tuple\n",
        "\n",
        "# ─── Configuration ────────────────────────────────────────────────────────────\n",
        "\n",
        "DATA_DIR    = Path(\"/content\") / \"bible_data\"\n",
        "MODEL_NAME  = \"xlm-roberta-base\"   # Multilingual; same model for EN and ZH\n",
        "BATCH_SIZE  = 32                   # Reduce to 8-16 if OOM on CPU\n",
        "LAYERS      = [-1, -2, -3, -4]     # Last 4 layers averaged (standard WSI practice)\n",
        "DEVICE      = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "MAX_SEQ_LEN = 128                  # Max subword tokens per sentence\n",
        "\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# ─── Model Loading ────────────────────────────────────────────────────────────\n",
        "\n",
        "def load_model():\n",
        "    print(f\"  [model] Loading {MODEL_NAME}…\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "    model     = AutoModel.from_pretrained(MODEL_NAME, output_hidden_states=True)\n",
        "    model.to(DEVICE)\n",
        "    model.eval()\n",
        "    return tokenizer, model\n",
        "\n",
        "\n",
        "# ─── Embedding Extraction ─────────────────────────────────────────────────────\n",
        "\n",
        "def get_target_embedding(\n",
        "    tokenizer,\n",
        "    model,\n",
        "    sentences:  List[str],\n",
        "    target_words: List[str],\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    For each (sentence, target_word) pair, extract the contextual embedding\n",
        "    of the target by:\n",
        "      1. Tokenizing the sentence\n",
        "      2. Finding subword token positions for the target word\n",
        "      3. Averaging hidden states across the last 4 layers at those positions\n",
        "      4. Mean-pooling across subwords for multi-token targets\n",
        "\n",
        "    Returns: np.ndarray of shape (N, hidden_dim)\n",
        "    \"\"\"\n",
        "    all_embeddings = []\n",
        "\n",
        "    for i in range(0, len(sentences), BATCH_SIZE):\n",
        "        batch_sents  = sentences[i : i + BATCH_SIZE]\n",
        "        batch_targets = target_words[i : i + BATCH_SIZE]\n",
        "\n",
        "        encoded = tokenizer(\n",
        "            batch_sents,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=MAX_SEQ_LEN,\n",
        "            return_offsets_mapping=True,\n",
        "        )\n",
        "        offset_mappings = encoded.pop(\"offset_mapping\")  # not passed to model\n",
        "\n",
        "        encoded = {k: v.to(DEVICE) for k, v in encoded.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**encoded)\n",
        "\n",
        "        # Stack selected hidden layers: shape (n_layers, batch, seq_len, hidden)\n",
        "        hidden_states = torch.stack(\n",
        "            [outputs.hidden_states[l] for l in LAYERS], dim=0\n",
        "        )\n",
        "        # Mean over selected layers: (batch, seq_len, hidden)\n",
        "        layer_mean = hidden_states.mean(dim=0).cpu().numpy()\n",
        "\n",
        "        input_ids = encoded[\"input_ids\"].cpu().numpy()\n",
        "\n",
        "        for j, (target, offsets_j) in enumerate(zip(batch_targets, offset_mappings)):\n",
        "            # Re-encode the target word alone to find its subword tokens\n",
        "            target_enc = tokenizer.encode(\n",
        "                target, add_special_tokens=False\n",
        "            )\n",
        "            # Find target subword positions in the sentence encoding\n",
        "            target_positions = _find_subword_positions(\n",
        "                input_ids[j].tolist(), target_enc\n",
        "            )\n",
        "            if target_positions:\n",
        "                token_emb = layer_mean[j][target_positions].mean(axis=0)\n",
        "            else:\n",
        "                # Fallback: mean-pool entire sequence (excluding [CLS]/[SEP])\n",
        "                seq_len = (input_ids[j] != tokenizer.pad_token_id).sum()\n",
        "                token_emb = layer_mean[j][1 : seq_len - 1].mean(axis=0)\n",
        "\n",
        "            all_embeddings.append(token_emb)\n",
        "\n",
        "        if (i // BATCH_SIZE) % 10 == 0:\n",
        "            print(f\"    … batch {i//BATCH_SIZE} / {len(sentences)//BATCH_SIZE}\", end=\"\\r\")\n",
        "\n",
        "    embeddings = np.array(all_embeddings, dtype=np.float32)\n",
        "    # L2 normalize for cosine-based clustering\n",
        "    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
        "    norms = np.where(norms == 0, 1, norms)\n",
        "    return embeddings / norms\n",
        "\n",
        "\n",
        "def _find_subword_positions(\n",
        "    sentence_ids: List[int], target_ids: List[int]\n",
        ") -> List[int]:\n",
        "    \"\"\"Find the start position of `target_ids` as a subsequence in `sentence_ids`.\"\"\"\n",
        "    n, m = len(sentence_ids), len(target_ids)\n",
        "    for start in range(n - m + 1):\n",
        "        if sentence_ids[start : start + m] == target_ids:\n",
        "            return list(range(start, start + m))\n",
        "    return []\n",
        "\n",
        "\n",
        "# ─── Per-language Pipeline ────────────────────────────────────────────────────\n",
        "\n",
        "def extract_embeddings_for_language(\n",
        "    lang: str,\n",
        "    noun_csv: Path,\n",
        "    tokenizer,\n",
        "    model,\n",
        ") -> None:\n",
        "    \"\"\"Load nouns, extract embeddings, and save as .npz.\"\"\"\n",
        "    out_path = DATA_DIR / f\"{lang}_embeddings.npz\"\n",
        "    if out_path.exists():\n",
        "        print(f\"  [{lang.upper()}] Embeddings already exist — skipping.\")\n",
        "        return\n",
        "\n",
        "    df = pd.read_csv(noun_csv)\n",
        "    print(f\"  [{lang.upper()}] Extracting embeddings for {len(df):,} noun occurrences…\")\n",
        "\n",
        "    embeddings = get_target_embedding(\n",
        "        tokenizer,\n",
        "        model,\n",
        "        sentences    = df[\"context\"].tolist(),\n",
        "        target_words = df[\"token\"].tolist(),\n",
        "    )\n",
        "    print()  # newline after progress indicator\n",
        "\n",
        "    np.savez_compressed(\n",
        "        out_path,\n",
        "        embeddings = embeddings,\n",
        "        lemmas     = df[\"lemma\"].to_numpy(dtype=str),\n",
        "        verse_ids  = df[\"verse_id\"].to_numpy(dtype=str),\n",
        "        tokens     = df[\"token\"].to_numpy(dtype=str),\n",
        "    )\n",
        "    print(f\"  [{lang.upper()}] Saved {embeddings.shape} embeddings → {out_path.name}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJF9kXsnWQDa",
        "outputId": "94b0e400-99db-435e-fc1d-a661566825d2"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"Step 3: Contextual Embedding Extraction (XLM-R)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "tokenizer, model = load_model()\n",
        "\n",
        "extract_embeddings_for_language(\n",
        "    \"english\",\n",
        "    DATA_DIR / \"english_nouns.csv\",\n",
        "    tokenizer, model,\n",
        ")\n",
        "extract_embeddings_for_language(\n",
        "    \"chinese\",\n",
        "    DATA_DIR / \"chinese_nouns.csv\",\n",
        "    tokenizer, model,\n",
        ")\n",
        "\n",
        "print(\"\\n✓ Step 3 complete.\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498,
          "referenced_widgets": [
            "e143729367c84672aae5ff048b37a0a3",
            "2e7aae7649434795a1bb5e95823cfb98",
            "5373f3c1de7d4fdab7d05495ea41e464",
            "6b4a8234621049a09ec48c9a4fa81fd1",
            "263d6262eebb411ca0f67d3ab6c81c56",
            "ce9fba2200b34cd5a0a5122ee28c0217",
            "7f577f916a6042518ef90d20d5320974",
            "9322f3934eb9403d94d114283f0e0ddc",
            "0ed16e36522f4e0e99e3163482e56433",
            "0b0e33e4f87746b48aae9377229ee37d",
            "e3b9cc5cf7014f16af32b43c4f440f2a"
          ]
        },
        "id": "ZwpfEQinX-ri",
        "outputId": "151dac8a-d785-45c9-bffa-1b2b1242e314"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Step 3: Contextual Embedding Extraction (XLM-R)\n",
            "============================================================\n",
            "  [model] Loading xlm-roberta-base…\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e143729367c84672aae5ff048b37a0a3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "XLMRobertaModel LOAD REPORT from: xlm-roberta-base\n",
            "Key                       | Status     |  | \n",
            "--------------------------+------------+--+-\n",
            "lm_head.dense.bias        | UNEXPECTED |  | \n",
            "lm_head.bias              | UNEXPECTED |  | \n",
            "lm_head.dense.weight      | UNEXPECTED |  | \n",
            "lm_head.layer_norm.bias   | UNEXPECTED |  | \n",
            "lm_head.layer_norm.weight | UNEXPECTED |  | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [ENGLISH] Extracting embeddings for 98,195 noun occurrences…\n",
            "\n",
            "  [ENGLISH] Saved (98195, 768) embeddings → english_embeddings.npz\n",
            "  [CHINESE] Extracting embeddings for 72,998 noun occurrences…\n",
            "\n",
            "  [CHINESE] Saved (72998, 768) embeddings → chinese_embeddings.npz\n",
            "\n",
            "✓ Step 3 complete.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: WSI Clustering"
      ],
      "metadata": {
        "id": "meHGlTr8aErA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Step 4: Word Sense Induction (WSI) via Agglomerative Clustering\n",
        "================================================================\n",
        "For each lemma in each language, clusters its contextual embeddings\n",
        "to induce word senses. The number of clusters k is determined\n",
        "automatically using the Silhouette Score (range 2–8) or set to 1\n",
        "if the word shows insufficient sense variation.\n",
        "\n",
        "Two clustering algorithms are run for robustness comparison:\n",
        "  (A) Agglomerative Hierarchical Clustering (Ward linkage)    — primary\n",
        "  (B) K-Means with k++ initialization                         — secondary\n",
        "\n",
        "Outputs:\n",
        "  - data/english_wsi_results.csv  : lemma, k_ward, k_kmeans, silhouette_ward, ...\n",
        "  - data/chinese_wsi_results.csv  : same\n",
        "  - data/english_sense_labels.csv : lemma, verse_id, cluster_ward, cluster_kmeans\n",
        "  - data/chinese_sense_labels.csv : same\n",
        "\n",
        "Usage:\n",
        "  pip install scikit-learn numpy pandas --break-system-packages\n",
        "  python 04_wsi_clustering.py\n",
        "\n",
        "Design decisions (paper §3.3):\n",
        "  - Ward linkage is preferred for lexical WSI (Ustalov et al. 2019)\n",
        "  - k range: 1–8 senses; beyond 8 is linguistically implausible for\n",
        "    the narrow domain of biblical text\n",
        "  - Silhouette threshold: if best_silhouette < SILHOUETTE_THRESHOLD,\n",
        "    k is set to 1 (monosemous)\n",
        "  - UMAP dimensionality reduction to 50D before clustering improves\n",
        "    silhouette stability (McInnes et al. 2018)\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.preprocessing import normalize\n",
        "from typing import Tuple, Dict\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "# ─── Configuration ────────────────────────────────────────────────────────────\n",
        "\n",
        "DATA_DIR    = Path(\"/content\") / \"bible_data\"\n",
        "# DATA_DIR             = Path(__file__).parent.parent / \"data\"\n",
        "K_RANGE              = range(2, 9)           # Test k = 2, 3, …, 8\n",
        "SILHOUETTE_THRESHOLD = 0.30                  # Below this → monosemous (k=1)\n",
        "# NOTE: 0.05 caused 0% monosemy — XLM-R embeddings always score > 0.05\n",
        "# even for genuinely monosemous words. 0.30 is the literature standard\n",
        "# for meaningful cluster structure (Ustalov et al. 2019; Neelakantan et al. 2014).\n",
        "# Run sensitivity check at SILHOUETTE_THRESHOLD = 0.20 and report both.\n",
        "MIN_INSTANCES        = 60                    # Min occurrences to attempt clustering\n",
        "# Raised from 5 to 60 to ensure UMAP dimensionality reduction works reliably.\n",
        "# UMAP requires n_samples > n_components (60 > 50). Words with 30-59 occurrences\n",
        "# are excluded from clustering as they lack sufficient contextual diversity for\n",
        "# robust sense induction (cf. Ustalov et al. 2019 who use threshold of 50).\n",
        "USE_UMAP             = True                  # Reduce to 50D before clustering\n",
        "UMAP_N_COMPONENTS    = 50\n",
        "RANDOM_STATE         = 42\n",
        "\n",
        "# ─── Optional UMAP reduction ─────────────────────────────────────────────────\n",
        "\n",
        "def reduce_embeddings(embeddings: np.ndarray) -> np.ndarray:\n",
        "#     \"\"\"\n",
        "#     Optionally reduce embedding dimensionality with UMAP before clustering.\n",
        "#     UMAP (McInnes et al. 2018) improves cluster separation for high-dim data.\n",
        "#     Falls back to PCA if umap-learn is not installed or if UMAP fails.\n",
        "#     \"\"\"\n",
        "#     if not USE_UMAP or embeddings.shape[0] <= MIN_INSTANCES: # Added <= MIN_INSTANCES for robustness\n",
        "#         return embeddings\n",
        "\n",
        "#     try:\n",
        "#         import umap\n",
        "#         # Ensure n_components is always less than the number of samples\n",
        "#         n_components_umap = min(UMAP_N_COMPONENTS, embeddings.shape[0] - 1)\n",
        "#         if n_components_umap <= 0: # Handle cases where n_samples is 1\n",
        "#             return embeddings\n",
        "\n",
        "#         reducer = umap.UMAP(\n",
        "#             n_components=n_components_umap,\n",
        "#             metric=\"cosine\",\n",
        "#             random_state=RANDOM_STATE,\n",
        "#             n_jobs=1,\n",
        "#         )\n",
        "#         return reducer.fit_transform(embeddings)\n",
        "#     except (ImportError, TypeError) as e: # Catch both ImportError and TypeError\n",
        "#         if isinstance(e, ImportError):\n",
        "#             print(\"    [warn] umap-learn not found — using PCA fallback. \"\n",
        "#                   \"Install: pip install umap-learn --break-system-packages\")\n",
        "#         else:\n",
        "#             print(f\"    [warn] UMAP failed ({e}) — using PCA fallback for this lemma. \"\n",
        "#                   f\"Embeddings shape: {embeddings.shape}\")\n",
        "#         from sklearn.decomposition import PCA\n",
        "#         # Ensure n_components for PCA is also less than number of samples and features\n",
        "#         n_components_pca = min(UMAP_N_COMPONENTS, embeddings.shape[0] - 1, embeddings.shape[1])\n",
        "#         if n_components_pca <= 0: # Handle cases where n_samples is 1\n",
        "#             return embeddings\n",
        "#         return PCA(n_components=n_components_pca, random_state=RANDOM_STATE).fit_transform(embeddings)\n",
        "    \"\"\"\n",
        "    Reduce embedding dimensionality with UMAP before clustering.\n",
        "    UMAP (McInnes et al. 2018) improves cluster separation for high-dim data.\n",
        "    Falls back to PCA if umap-learn is not installed.\n",
        "\n",
        "    With MIN_INSTANCES=60, we always have n_samples ≥ 60 > 50 = n_components,\n",
        "    so UMAP will never fail with the k >= N error.\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    Reduce embedding dimensionality with UMAP before clustering.\n",
        "    UMAP (McInnes et al. 2018) improves cluster separation for high-dim data.\n",
        "    Falls back to PCA if umap-learn is not installed.\n",
        "\n",
        "    With MIN_INSTANCES=60, we always have n_samples ≥ 60 > 50 = n_components,\n",
        "    so UMAP will never fail with the k >= N error.\n",
        "    \"\"\"\n",
        "    if not USE_UMAP or embeddings.shape[0] < UMAP_N_COMPONENTS:\n",
        "        return embeddings\n",
        "    try:\n",
        "        import umap\n",
        "        reducer = umap.UMAP(\n",
        "            n_components=UMAP_N_COMPONENTS,\n",
        "            metric=\"cosine\",\n",
        "            random_state=RANDOM_STATE,\n",
        "            n_jobs=1,\n",
        "        )\n",
        "        return reducer.fit_transform(embeddings)\n",
        "    except ImportError:\n",
        "        print(\"    [warn] umap-learn not found — using PCA fallback. \"\n",
        "              \"Install: pip install umap-learn --break-system-packages\")\n",
        "        from sklearn.decomposition import PCA\n",
        "        n = min(UMAP_N_COMPONENTS, embeddings.shape[0] - 1, embeddings.shape[1])\n",
        "        return PCA(n_components=n, random_state=RANDOM_STATE).fit_transform(embeddings)\n",
        "\n",
        "\n",
        "# ─── Core WSI for a single lemma ──────────────────────────────────────────────\n",
        "\n",
        "def induce_senses_for_lemma(\n",
        "    embeddings: np.ndarray,\n",
        ") -> Tuple[int, int, float, np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Given embeddings for all occurrences of a lemma, find the optimal k\n",
        "    using silhouette score for both Ward and KMeans clustering.\n",
        "\n",
        "    Returns:\n",
        "        k_ward, k_kmeans, best_silhouette_ward,\n",
        "        labels_ward (np.ndarray), labels_kmeans (np.ndarray)\n",
        "    \"\"\"\n",
        "    n = len(embeddings)\n",
        "\n",
        "    # Insufficient data → monosemous\n",
        "    if n < MIN_INSTANCES:\n",
        "        ones = np.zeros(n, dtype=int)\n",
        "        return 1, 1, 0.0, ones, ones\n",
        "\n",
        "    reduced = reduce_embeddings(embeddings)\n",
        "\n",
        "    best_k_ward, best_sil_ward, best_labels_ward   = 1, -1.0, np.zeros(n, dtype=int)\n",
        "    best_k_km,   best_sil_km,   best_labels_km     = 1, -1.0, np.zeros(n, dtype=int)\n",
        "\n",
        "    for k in K_RANGE:\n",
        "        if k >= n:\n",
        "            break  # Can't have more clusters than data points\n",
        "\n",
        "        # Ward agglomerative\n",
        "        try:\n",
        "            ward = AgglomerativeClustering(n_clusters=k, linkage=\"ward\")\n",
        "            labels_w = ward.fit_predict(reduced)\n",
        "            if len(np.unique(labels_w)) > 1:\n",
        "                sil_w = silhouette_score(reduced, labels_w, metric=\"euclidean\",\n",
        "                                         sample_size=min(1000, n))\n",
        "                if sil_w > best_sil_ward:\n",
        "                    best_sil_ward, best_k_ward, best_labels_ward = sil_w, k, labels_w\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        # K-Means\n",
        "        try:\n",
        "            km = KMeans(n_clusters=k, random_state=RANDOM_STATE,\n",
        "                        n_init=10, max_iter=300)\n",
        "            labels_k = km.fit_predict(reduced)\n",
        "            if len(np.unique(labels_k)) > 1:\n",
        "                sil_k = silhouette_score(reduced, labels_k, metric=\"euclidean\",\n",
        "                                          sample_size=min(1000, n))\n",
        "                if sil_k > best_sil_km:\n",
        "                    best_sil_km, best_k_km, best_labels_km = sil_k, k, labels_k\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # Apply monosemy threshold\n",
        "    if best_sil_ward < SILHOUETTE_THRESHOLD:\n",
        "        best_k_ward    = 1\n",
        "        best_labels_ward = np.zeros(n, dtype=int)\n",
        "\n",
        "    if best_sil_km < SILHOUETTE_THRESHOLD:\n",
        "        best_k_km    = 1\n",
        "        best_labels_km = np.zeros(n, dtype=int)\n",
        "\n",
        "    return (best_k_ward, best_k_km, max(best_sil_ward, 0.0),\n",
        "            best_labels_ward, best_labels_km)\n",
        "\n",
        "\n",
        "# ─── Run WSI for entire language ──────────────────────────────────────────────\n",
        "\n",
        "def run_wsi_for_language(lang: str):\n",
        "    \"\"\"\n",
        "    Load embeddings and run WSI for all lemmas in a language.\n",
        "    Saves two files:\n",
        "      1. {lang}_wsi_results.csv    - summary stats per lemma\n",
        "      2. {lang}_sense_labels.csv   - cluster assignments per occurrence\n",
        "    \"\"\"\n",
        "    embeddings_file = DATA_DIR / f\"{lang}_embeddings.npz\"\n",
        "    nouns_file      = DATA_DIR / f\"{lang}_nouns.csv\"\n",
        "\n",
        "    if not embeddings_file.exists():\n",
        "        print(f\"  [{lang.upper()}] ERROR: {embeddings_file.name} not found. Run Step 3 first.\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\n  [{lang.upper()}] Loading embeddings…\")\n",
        "    data = np.load(embeddings_file, allow_pickle=True)\n",
        "    lemmas_array = data[\"lemmas\"]\n",
        "    embeddings   = data[\"embeddings\"]\n",
        "    verse_ids    = data[\"verse_ids\"]\n",
        "    unique_lemmas = np.unique(lemmas_array)\n",
        "\n",
        "    print(f\"  [{lang.upper()}] Running WSI for {len(unique_lemmas):,} lemmas…\")\n",
        "\n",
        "    results = []\n",
        "    all_labels = []  # Collect per-instance labels for sense_labels.csv\n",
        "\n",
        "    for i, lemma in enumerate(unique_lemmas):\n",
        "        mask = (lemmas_array == lemma)\n",
        "        lemma_embeds = embeddings[mask]\n",
        "        lemma_vids   = verse_ids[mask]\n",
        "\n",
        "        k_ward, k_km, sil_ward, labels_ward, labels_km = induce_senses_for_lemma(lemma_embeds)\n",
        "\n",
        "        # Agreement: what % of instances assigned to same cluster by both methods?\n",
        "        if len(labels_ward) > 0:\n",
        "            agree = (labels_ward == labels_km).mean() * 100\n",
        "        else:\n",
        "            agree = 100.0\n",
        "\n",
        "        results.append({\n",
        "            \"lemma\":            lemma,\n",
        "            \"n_instances\":      len(lemma_embeds),\n",
        "            \"k_ward\":           k_ward,\n",
        "            \"k_kmeans\":         k_km,\n",
        "            \"silhouette_ward\":  round(sil_ward, 4),\n",
        "            \"agreement_pct\":    round(agree, 2),\n",
        "        })\n",
        "\n",
        "        # Collect per-instance cluster assignments\n",
        "        for vid, cluster_w, cluster_k in zip(lemma_vids, labels_ward, labels_km):\n",
        "            all_labels.append({\n",
        "                \"lemma\":          lemma,\n",
        "                \"verse_id\":       vid,\n",
        "                \"cluster_ward\":   int(cluster_w),\n",
        "                \"cluster_kmeans\": int(cluster_k),\n",
        "            })\n",
        "\n",
        "        if (i + 1) % 50 == 0 or (i + 1) == len(unique_lemmas):\n",
        "            print(f\"    … {i + 1:,}/{len(unique_lemmas):,} lemmas\", end=\"\\r\")\n",
        "\n",
        "    # Save summary results\n",
        "    df = pd.DataFrame(results)\n",
        "    out_path = DATA_DIR / f\"{lang}_wsi_results.csv\"\n",
        "    df.to_csv(out_path, index=False)\n",
        "    print(f\"\\n  [{lang.upper()}] Saved: {out_path.name}\")\n",
        "\n",
        "    # Save per-instance cluster labels\n",
        "    labels_df = pd.DataFrame(all_labels)\n",
        "    labels_path = DATA_DIR / f\"{lang}_sense_labels.csv\"\n",
        "    labels_df.to_csv(labels_path, index=False)\n",
        "    print(f\"  [{lang.upper()}] Saved: {labels_path.name}\")\n",
        "\n",
        "    print(f\"  [{lang.upper()}] Mean k (Ward): {df['k_ward'].mean():.2f}  \"\n",
        "          f\"Median: {df['k_ward'].median():.0f}  \"\n",
        "          f\"Monosemous (k=1): {(df['k_ward']==1).sum()} / {len(df)}\")"
      ],
      "metadata": {
        "id": "wCI4OOXjYSNs"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"Step 4: Word Sense Induction (WSI)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "run_wsi_for_language(\"english\")\n",
        "run_wsi_for_language(\"chinese\")\n",
        "\n",
        "# Quick comparison preview\n",
        "en = pd.read_csv(DATA_DIR / \"english_wsi_results.csv\")\n",
        "zh = pd.read_csv(DATA_DIR / \"chinese_wsi_results.csv\")\n",
        "\n",
        "print(\"\\n── Quick Comparison Preview ──\")\n",
        "print(f\"  EN | mean senses/lemma (Ward) : {en['k_ward'].mean():.3f}\")\n",
        "print(f\"  ZH | mean senses/lemma (Ward) : {zh['k_ward'].mean():.3f}\")\n",
        "print(f\"  EN | % polysemous lemmas       : {(en['k_ward'] > 1).mean()*100:.1f}%\")\n",
        "print(f\"  ZH | % polysemous lemmas       : {(zh['k_ward'] > 1).mean()*100:.1f}%\")\n",
        "\n",
        "print(\"\\n✓ Step 4 complete.\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUNKMUx1aRI5",
        "outputId": "8d9a3ef1-f3d8-421f-929a-54a3b4208d05"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Step 4: Word Sense Induction (WSI)\n",
            "============================================================\n",
            "\n",
            "  [ENGLISH] Loading embeddings…\n",
            "  [ENGLISH] Running WSI for 636 lemmas…\n",
            "    … 636/636 lemmas\n",
            "  [ENGLISH] Saved: english_wsi_results.csv\n",
            "  [ENGLISH] Saved: english_sense_labels.csv\n",
            "  [ENGLISH] Mean k (Ward): 2.13  Median: 2  Monosemous (k=1): 252 / 636\n",
            "\n",
            "  [CHINESE] Loading embeddings…\n",
            "  [CHINESE] Running WSI for 572 lemmas…\n",
            "    … 572/572 lemmas\n",
            "  [CHINESE] Saved: chinese_wsi_results.csv\n",
            "  [CHINESE] Saved: chinese_sense_labels.csv\n",
            "  [CHINESE] Mean k (Ward): 2.11  Median: 2  Monosemous (k=1): 274 / 572\n",
            "\n",
            "── Quick Comparison Preview ──\n",
            "  EN | mean senses/lemma (Ward) : 2.127\n",
            "  ZH | mean senses/lemma (Ward) : 2.110\n",
            "  EN | % polysemous lemmas       : 60.4%\n",
            "  ZH | % polysemous lemmas       : 52.1%\n",
            "\n",
            "✓ Step 4 complete.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Validation and Statistical Analysis"
      ],
      "metadata": {
        "id": "lX7M7-Toc3Dl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Step 5: Validation and Statistical Analysis\n",
        "============================================\n",
        "Validates WSI-induced sense counts against gold-standard lexical resources:\n",
        "  - English: Princeton WordNet (via NLTK)\n",
        "  - Chinese:  Chinese WordNet (via Taiwanese CWN or HowNet)\n",
        "\n",
        "Also performs the core statistical comparison between languages:\n",
        "  - Mann-Whitney U test (non-parametric, appropriate for skewed count data)\n",
        "  - Cohen's d effect size\n",
        "  - Spearman correlation with WordNet sense counts (validation)\n",
        "  - Distribution plots (saved as PNG for inclusion in paper)\n",
        "\n",
        "Outputs:\n",
        "  - output/validation_correlation_en.csv\n",
        "  - output/validation_correlation_zh.csv   (if CWN available)\n",
        "  - output/statistical_comparison.csv\n",
        "  - output/figures/sense_distribution_en.png\n",
        "  - output/figures/sense_distribution_zh.png\n",
        "  - output/figures/comparison_boxplot.png\n",
        "  - output/figures/wordnet_correlation_en.png\n",
        "\n",
        "Usage:\n",
        "  pip install nltk scipy matplotlib seaborn pandas numpy --break-system-packages\n",
        "  python -c \"import nltk; nltk.download('wordnet')\"\n",
        "  python 05_validation_statistics.py\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "from scipy import stats\n",
        "from typing import Optional, Tuple\n",
        "\n",
        "# ─── Configuration ────────────────────────────────────────────────────────────\n",
        "\n",
        "DATA_DIR    = Path(\"/content\") / \"bible_data\"\n",
        "OUTPUT_DIR = Path(\"/content\") / \"output\"\n",
        "# DATA_DIR   = Path(__file__).parent.parent / \"data\"\n",
        "# OUTPUT_DIR = Path(__file__).parent.parent / \"output\"\n",
        "FIG_DIR    = OUTPUT_DIR / \"figures\"\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "FIG_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "plt.rcParams.update({\n",
        "    \"font.family\":  \"DejaVu Sans\",\n",
        "    \"font.size\":    11,\n",
        "    \"axes.titlesize\": 13,\n",
        "    \"axes.labelsize\": 12,\n",
        "})\n",
        "\n",
        "\n",
        "# ─── WordNet Validation (English) ────────────────────────────────────────────\n",
        "\n",
        "def get_wordnet_sense_counts(lemmas: list) -> dict:\n",
        "    \"\"\"\n",
        "    Look up the number of synsets for each lemma in Princeton WordNet.\n",
        "    Noun synsets only (pos='n').\n",
        "    \"\"\"\n",
        "    try:\n",
        "        from nltk.corpus import wordnet as wn\n",
        "    except ImportError:\n",
        "        raise ImportError(\"Run: pip install nltk --break-system-packages && \"\n",
        "                          \"python -c \\\"import nltk; nltk.download('wordnet')\\\"\")\n",
        "\n",
        "    counts = {}\n",
        "    for lemma in lemmas:\n",
        "        synsets = wn.synsets(lemma.lower(), pos=wn.NOUN)\n",
        "        counts[lemma] = len(synsets)\n",
        "    return counts\n",
        "\n",
        "\n",
        "def validate_english(en_results: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Correlate WSI-induced k_ward with WordNet sense counts for English nouns.\n",
        "    Returns a merged DataFrame with both counts.\n",
        "    \"\"\"\n",
        "    print(\"  [validate] Looking up Princeton WordNet sense counts…\")\n",
        "    lemmas = en_results[\"lemma\"].tolist()\n",
        "    wn_counts = get_wordnet_sense_counts(lemmas)\n",
        "\n",
        "    en_results = en_results.copy()\n",
        "    en_results[\"wn_senses\"] = en_results[\"lemma\"].map(wn_counts).fillna(0).astype(int)\n",
        "\n",
        "    # Keep only lemmas with at least 1 WordNet entry\n",
        "    valid = en_results[en_results[\"wn_senses\"] > 0].copy()\n",
        "\n",
        "    rho, p = stats.spearmanr(valid[\"k_ward\"], valid[\"wn_senses\"])\n",
        "    print(f\"  [validate] EN Spearman ρ(k_ward, WN_senses) = {rho:.3f}  p = {p:.4f}  \"\n",
        "          f\"(n={len(valid)})\")\n",
        "\n",
        "    return valid, rho, p\n",
        "\n",
        "\n",
        "# ─── Statistical Comparison ──────────────────────────────────────────────────\n",
        "\n",
        "def mann_whitney_comparison(\n",
        "    en_k: np.ndarray,\n",
        "    zh_k: np.ndarray,\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    Mann-Whitney U test comparing mean sense counts between EN and ZH.\n",
        "    Also computes Cohen's d and the common language effect size (CLES).\n",
        "    \"\"\"\n",
        "    u_stat, p_val = stats.mannwhitneyu(en_k, zh_k, alternative=\"two-sided\")\n",
        "    n1, n2        = len(en_k), len(zh_k)\n",
        "    cles          = u_stat / (n1 * n2)  # Common Language Effect Size\n",
        "\n",
        "    # Cohen's d (for reference alongside CLES)\n",
        "    pooled_std = np.sqrt(\n",
        "        ((n1 - 1) * en_k.std(ddof=1) ** 2 + (n2 - 1) * zh_k.std(ddof=1) ** 2)\n",
        "        / (n1 + n2 - 2)\n",
        "    )\n",
        "    cohens_d = (en_k.mean() - zh_k.mean()) / (pooled_std + 1e-9)\n",
        "\n",
        "    return {\n",
        "        \"en_mean_k\":   round(en_k.mean(), 4),\n",
        "        \"zh_mean_k\":   round(zh_k.mean(), 4),\n",
        "        \"en_median_k\": round(float(np.median(en_k)), 4),\n",
        "        \"zh_median_k\": round(float(np.median(zh_k)), 4),\n",
        "        \"en_std_k\":    round(en_k.std(ddof=1), 4),\n",
        "        \"zh_std_k\":    round(zh_k.std(ddof=1), 4),\n",
        "        \"U_statistic\": round(u_stat, 2),\n",
        "        \"p_value\":     round(p_val, 6),\n",
        "        \"CLES\":        round(cles, 4),\n",
        "        \"cohens_d\":    round(cohens_d, 4),\n",
        "        \"n_en\":        int(n1),\n",
        "        \"n_zh\":        int(n2),\n",
        "    }\n",
        "\n",
        "\n",
        "# ─── Figures ─────────────────────────────────────────────────────────────────\n",
        "\n",
        "def plot_sense_distribution(df: pd.DataFrame, lang: str, col: str = \"k_ward\") -> None:\n",
        "    fig, ax = plt.subplots(figsize=(8, 4))\n",
        "    counts = df[col].value_counts().sort_index()\n",
        "    ax.bar(counts.index, counts.values, color=\"#4C72B0\", edgecolor=\"white\", linewidth=0.5)\n",
        "    ax.set_xlabel(\"Number of Induced Senses (k)\")\n",
        "    ax.set_ylabel(\"Number of Lemmas\")\n",
        "    ax.set_title(f\"{lang.capitalize()} — Distribution of Induced Senses per Noun Lemma\")\n",
        "    ax.set_xticks(range(1, df[col].max() + 1))\n",
        "    fig.tight_layout()\n",
        "    path = FIG_DIR / f\"sense_distribution_{lang}.png\"\n",
        "    fig.savefig(path, dpi=150)\n",
        "    plt.close(fig)\n",
        "    print(f\"  [fig] Saved: {path.name}\")\n",
        "\n",
        "\n",
        "def plot_comparison_boxplot(en_df: pd.DataFrame, zh_df: pd.DataFrame) -> None:\n",
        "    combined = pd.concat([\n",
        "        en_df[[\"k_ward\"]].assign(Language=\"English\"),\n",
        "        zh_df[[\"k_ward\"]].assign(Language=\"Chinese\"),\n",
        "    ])\n",
        "    fig, ax = plt.subplots(figsize=(7, 5))\n",
        "    sns.violinplot(data=combined, x=\"Language\", y=\"k_ward\",\n",
        "                   palette=[\"#4C72B0\", \"#DD8452\"], inner=\"box\",\n",
        "                   cut=0,   # ← add this to fix sns artifact of extending beyond 8\n",
        "                   ax=ax)\n",
        "    ax.set_ylabel(\"Induced Senses per Lemma (k, Ward)\")\n",
        "    ax.set_title(\"Distribution of Polysemy Degree: English vs. Chinese Common Nouns\\n\"\n",
        "                 \"(Bible Parallel Corpus, WSI via XLM-R + Agglomerative Clustering)\")\n",
        "    fig.tight_layout()\n",
        "    path = FIG_DIR / \"comparison_violinplot.png\"\n",
        "    fig.savefig(path, dpi=150)\n",
        "    plt.close(fig)\n",
        "    print(f\"  [fig] Saved: {path.name}\")\n",
        "\n",
        "\n",
        "def plot_wordnet_correlation(valid_df: pd.DataFrame, rho: float, p: float) -> None:\n",
        "    fig, ax = plt.subplots(figsize=(6, 5))\n",
        "    ax.scatter(valid_df[\"wn_senses\"], valid_df[\"k_ward\"],\n",
        "               alpha=0.4, s=20, color=\"#4C72B0\")\n",
        "    ax.set_xlabel(\"WordNet Noun Synset Count\")\n",
        "    ax.set_ylabel(\"WSI-Induced k (Ward)\")\n",
        "    ax.set_title(f\"Validation: WSI k vs. WordNet Senses (English Nouns)\\n\"\n",
        "                 f\"Spearman ρ = {rho:.3f}, p = {p:.4f}\")\n",
        "    # Trend line\n",
        "    m, b = np.polyfit(valid_df[\"wn_senses\"], valid_df[\"k_ward\"], 1)\n",
        "    x_line = np.linspace(valid_df[\"wn_senses\"].min(), valid_df[\"wn_senses\"].max(), 100)\n",
        "    ax.plot(x_line, m * x_line + b, color=\"red\", linewidth=1.5, linestyle=\"--\")\n",
        "    fig.tight_layout()\n",
        "    path = FIG_DIR / \"wordnet_correlation_en.png\"\n",
        "    fig.savefig(path, dpi=150)\n",
        "    plt.close(fig)\n",
        "    print(f\"  [fig] Saved: {path.name}\")\n",
        "\n",
        "\n",
        "def plot_silhouette_distribution(en_df: pd.DataFrame, zh_df: pd.DataFrame,\n",
        "                                  threshold: float = 0.30) -> None:\n",
        "    \"\"\"\n",
        "    Plot silhouette score distributions for both languages.\n",
        "    Shows the threshold τ that separates monosemous (k=1) from polysemous words.\n",
        "\n",
        "    Why this figure matters:\n",
        "      - Validates that τ=0.30 is correctly positioned in the distribution\n",
        "      - Shows monosemous vs polysemous words have different silhouette profiles\n",
        "      - Confirms the threshold is not arbitrary\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 4.5), sharey=False)\n",
        "    fig.suptitle(\"Silhouette Score Distributions with Monosemy Threshold τ = 0.30\",\n",
        "                 fontsize=13, fontweight=\"bold\")\n",
        "\n",
        "    for ax, df, lang, color in zip(\n",
        "        axes,\n",
        "        [en_df, zh_df],\n",
        "        [\"English\", \"Chinese\"],\n",
        "        [\"#4C72B0\", \"#DD8452\"]\n",
        "    ):\n",
        "        mono = df[df[\"k_ward\"] == 1][\"silhouette_ward\"]\n",
        "        poly = df[df[\"k_ward\"] >  1][\"silhouette_ward\"]\n",
        "\n",
        "        # Monosemous words: silhouette = 0 (they were never clustered)\n",
        "        # Only plot polysemous words' silhouette scores — monosemous are always 0\n",
        "        ax.hist(poly, bins=20, color=color, alpha=0.75,\n",
        "                edgecolor=\"white\", linewidth=0.5,\n",
        "                label=f\"Polysemous (k≥2, n={len(poly)})\")\n",
        "        ax.hist(mono, bins=20, color=\"lightgray\", alpha=0.75,\n",
        "                edgecolor=\"white\", linewidth=0.5,\n",
        "                label=f\"Monosemous (k=1, n={len(mono)})\")\n",
        "\n",
        "        # Draw threshold line\n",
        "        ax.axvline(threshold, color=\"red\", linewidth=2, linestyle=\"--\",\n",
        "                   label=f\"Threshold τ={threshold}\")\n",
        "\n",
        "        # Annotate monosemy rate\n",
        "        mono_rate = len(mono) / len(df) * 100\n",
        "        ax.text(threshold + 0.01, ax.get_ylim()[1] * 0.9 if ax.get_ylim()[1] > 0 else 10,\n",
        "                f\"{mono_rate:.1f}%\\nmonosemous\",\n",
        "                color=\"red\", fontsize=9, va=\"top\")\n",
        "\n",
        "        ax.set_xlabel(\"Silhouette Score\", fontsize=11)\n",
        "        ax.set_ylabel(\"Number of Lemmas\", fontsize=11)\n",
        "        ax.set_title(f\"{lang} (n={len(df)})\", fontsize=12)\n",
        "        ax.set_xlim(-0.05, 0.75)\n",
        "        ax.legend(fontsize=9, loc=\"upper right\")\n",
        "        ax.grid(True, alpha=0.3, axis=\"y\")\n",
        "\n",
        "    fig.tight_layout()\n",
        "    path = FIG_DIR / \"silhouette_distribution.png\"\n",
        "    fig.savefig(path, dpi=150, bbox_inches=\"tight\")\n",
        "    plt.close(fig)\n",
        "    print(f\"  [fig] Saved: {path.name}\")"
      ],
      "metadata": {
        "id": "A2wqaZynajnY"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"Step 5: Validation and Statistical Analysis\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "en = pd.read_csv(DATA_DIR / \"english_wsi_results.csv\")\n",
        "zh = pd.read_csv(DATA_DIR / \"chinese_wsi_results.csv\")\n",
        "\n",
        "# ── Validation (English vs. WordNet) ──────────────────────────\n",
        "valid_en, rho, p = validate_english(en)\n",
        "valid_en.to_csv(OUTPUT_DIR / \"validation_correlation_en.csv\", index=False)\n",
        "\n",
        "# ── Statistical Comparison ────────────────────────────────────\n",
        "stats_result = mann_whitney_comparison(\n",
        "    en[\"k_ward\"].values,\n",
        "    zh[\"k_ward\"].values,\n",
        ")\n",
        "stats_df = pd.DataFrame([stats_result])\n",
        "stats_df.to_csv(OUTPUT_DIR / \"statistical_comparison.csv\", index=False)\n",
        "\n",
        "print(\"\\n── Statistical Comparison Results ──\")\n",
        "for k, v in stats_result.items():\n",
        "    print(f\"  {k:22s}: {v}\")\n",
        "\n",
        "# ── Silhouette score diagnostic ───────────────────────────────\n",
        "# Shows whether the threshold is calibrated correctly.\n",
        "# If monosemy rate is 0%, raise SILHOUETTE_THRESHOLD in 04_wsi_clustering.py\n",
        "# Target: 20-50% monosemous words. Rerun Step 4 after changing threshold.\n",
        "print(\"\\n── Silhouette Diagnostic (tune threshold if monosemy=0%) ──\")\n",
        "for label, df_lang in [(\"EN\", en), (\"ZH\", zh)]:\n",
        "    sil = df_lang[\"silhouette_ward\"]\n",
        "    mono_rate = (df_lang[\"k_ward\"] == 1).mean() * 100\n",
        "    print(f\"  {label} silhouette: \"\n",
        "            f\"min={sil.min():.3f}  \"\n",
        "            f\"p25={sil.quantile(0.25):.3f}  \"\n",
        "            f\"median={sil.median():.3f}  \"\n",
        "            f\"p75={sil.quantile(0.75):.3f}  \"\n",
        "            f\"max={sil.max():.3f}\")\n",
        "    print(f\"  {label} monosemy rate (k=1): {mono_rate:.1f}%\")\n",
        "\n",
        "# ── Figures ───────────────────────────────────────────────────\n",
        "plot_sense_distribution(en, \"english\")\n",
        "plot_sense_distribution(zh, \"chinese\")\n",
        "plot_comparison_boxplot(en, zh)\n",
        "plot_wordnet_correlation(valid_en, rho, p)\n",
        "plot_silhouette_distribution(en, zh)\n",
        "\n",
        "# ── Interpretation ────────────────────────────────────────────\n",
        "print(\"\\n── Interpretation ──\")\n",
        "if stats_result[\"p_value\"] < 0.05:\n",
        "    direction = \"English\" if stats_result[\"en_mean_k\"] > stats_result[\"zh_mean_k\"] else \"Chinese\"\n",
        "    print(f\"  Significant difference found (p={stats_result['p_value']:.4f}).\")\n",
        "    print(f\"  {direction} nouns show higher mean polysemy degree.\")\n",
        "else:\n",
        "    print(f\"  No significant difference found (p={stats_result['p_value']:.4f}).\")\n",
        "\n",
        "d = abs(stats_result[\"cohens_d\"])\n",
        "magnitude = \"small\" if d < 0.2 else (\"medium\" if d < 0.5 else \"large\")\n",
        "print(f\"  Effect size: Cohen's d = {stats_result['cohens_d']:.3f} ({magnitude})\")\n",
        "print(f\"  Spearman ρ (EN WSI vs. WordNet): {rho:.3f} (p={p:.4f})\")\n",
        "\n",
        "print(\"\\n✓ Step 5 complete.\\n\")\n"
      ],
      "metadata": {
        "id": "M1bzCPW5dCC_",
        "outputId": "04646667-7afd-4d45-addc-ecddd3ebd3b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Step 5: Validation and Statistical Analysis\n",
            "============================================================\n",
            "  [validate] Looking up Princeton WordNet sense counts…\n",
            "  [validate] EN Spearman ρ(k_ward, WN_senses) = 0.117  p = 0.0034  (n=630)\n",
            "\n",
            "── Statistical Comparison Results ──\n",
            "  en_mean_k             : 2.1274\n",
            "  zh_mean_k             : 2.1101\n",
            "  en_median_k           : 2.0\n",
            "  zh_median_k           : 2.0\n",
            "  en_std_k              : 1.5837\n",
            "  zh_std_k              : 1.7527\n",
            "  U_statistic           : 194171.0\n",
            "  p_value               : 0.029396\n",
            "  CLES                  : 0.5337\n",
            "  cohens_d              : 0.0103\n",
            "  n_en                  : 636\n",
            "  n_zh                  : 572\n",
            "\n",
            "── Silhouette Diagnostic (tune threshold if monosemy=0%) ──\n",
            "  EN silhouette: min=0.000  p25=0.000  median=0.471  p75=0.852  max=0.960\n",
            "  EN monosemy rate (k=1): 39.6%\n",
            "  ZH silhouette: min=0.000  p25=0.000  median=0.336  p75=0.468  max=0.963\n",
            "  ZH monosemy rate (k=1): 47.9%\n",
            "  [fig] Saved: sense_distribution_english.png\n",
            "  [fig] Saved: sense_distribution_chinese.png\n",
            "  [fig] Saved: comparison_violinplot.png\n",
            "  [fig] Saved: wordnet_correlation_en.png\n",
            "  [fig] Saved: silhouette_distribution.png\n",
            "\n",
            "── Interpretation ──\n",
            "  Significant difference found (p=0.0294).\n",
            "  English nouns show higher mean polysemy degree.\n",
            "  Effect size: Cohen's d = 0.010 (small)\n",
            "  Spearman ρ (EN WSI vs. WordNet): 0.117 (p=0.0034)\n",
            "\n",
            "✓ Step 5 complete.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6: Qualitative Analysis"
      ],
      "metadata": {
        "id": "O3m1UJnseHjn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Step 6: Qualitative Analysis — Sense Cluster Inspection\n",
        "=========================================================\n",
        "For each polysemous lemma, retrieves representative example verses\n",
        "for each induced sense cluster. This supports the qualitative\n",
        "analysis section of the paper, demonstrating that clusters correspond\n",
        "to meaningful, interpretable senses.\n",
        "\n",
        "Also generates a LaTeX-ready table of top polysemous words\n",
        "for both languages (for paper Table 3).\n",
        "\n",
        "Outputs:\n",
        "  - output/qualitative_en_top_polysemous.txt   (sense examples)\n",
        "  - output/qualitative_zh_top_polysemous.txt\n",
        "  - output/table_top_polysemous_latex.tex       (LaTeX table)\n",
        "  - output/polysemy_profile_comparison.csv      (wide comparison table)\n",
        "\n",
        "Usage:\n",
        "  python 06_qualitative_analysis.py\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "\n",
        "# ─── Configuration ────────────────────────────────────────────────────────────\n",
        "\n",
        "DATA_DIR    = Path(\"/content\") / \"bible_data\"\n",
        "OUTPUT_DIR = Path(\"/content\") / \"output\"\n",
        "# DATA_DIR   = Path(__file__).parent.parent / \"data\"\n",
        "# OUTPUT_DIR = Path(__file__).parent.parent / \"output\"\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "TOP_N_WORDS       = 20   # Top N most polysemous lemmas per language\n",
        "EXAMPLES_PER_SENSE = 2   # Number of example verses per cluster\n",
        "\n",
        "\n",
        "# ─── Sense Example Retrieval ─────────────────────────────────────────────────\n",
        "\n",
        "def get_sense_examples(\n",
        "    lang: str,\n",
        "    top_n: int = TOP_N_WORDS,\n",
        "    examples_per_sense: int = EXAMPLES_PER_SENSE,\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    For the top_n most polysemous lemmas, retrieve example contexts\n",
        "    for each induced sense cluster.\n",
        "\n",
        "    Returns a formatted string ready for a paper's qualitative appendix.\n",
        "    \"\"\"\n",
        "    results_df = pd.read_csv(DATA_DIR / f\"{lang}_wsi_results.csv\")\n",
        "    labels_df  = pd.read_csv(DATA_DIR / f\"{lang}_sense_labels.csv\")\n",
        "    nouns_df   = pd.read_csv(DATA_DIR / f\"{lang}_nouns.csv\")\n",
        "\n",
        "    # Top polysemous by k_ward\n",
        "    poly = results_df[results_df[\"k_ward\"] > 1].nlargest(top_n, \"k_ward\")\n",
        "\n",
        "    # Merge labels with original contexts\n",
        "    merged = labels_df.merge(\n",
        "        nouns_df[[\"lemma\", \"verse_id\", \"context\"]].drop_duplicates(),\n",
        "        on=[\"lemma\", \"verse_id\"],\n",
        "        how=\"left\",\n",
        "    )\n",
        "\n",
        "    lines = []\n",
        "    lines.append(f\"{'='*60}\")\n",
        "    lines.append(f\"QUALITATIVE SENSE ANALYSIS — {lang.upper()}\")\n",
        "    lines.append(f\"Top {top_n} Most Polysemous Nouns (WSI, Ward Clustering)\")\n",
        "    lines.append(f\"{'='*60}\\n\")\n",
        "\n",
        "    for _, row in poly.iterrows():\n",
        "        lemma = row[\"lemma\"]\n",
        "        k     = int(row[\"k_ward\"])\n",
        "        n_occ = int(row[\"n_occurrences\"])\n",
        "        sil   = row.get(\"silhouette_ward\", \"N/A\")\n",
        "\n",
        "        lines.append(f\"Lemma: '{lemma}'  |  k={k}  |  n={n_occ}  |  silhouette={sil}\")\n",
        "        lines.append(\"-\" * 50)\n",
        "\n",
        "        lemma_data = merged[merged[\"lemma\"] == lemma]\n",
        "\n",
        "        for cluster_id in range(k):\n",
        "            cluster_rows = lemma_data[lemma_data[\"cluster_ward\"] == cluster_id]\n",
        "            lines.append(f\"  Sense {cluster_id + 1} ({len(cluster_rows)} occurrences):\")\n",
        "\n",
        "            # Sample diverse examples\n",
        "            sample = cluster_rows.dropna(subset=[\"context\"]).head(examples_per_sense)\n",
        "            for _, ex in sample.iterrows():\n",
        "                ctx = str(ex[\"context\"])[:200].replace(\"\\n\", \" \")\n",
        "                lines.append(f\"    • {ex['verse_id']}: {ctx}\")\n",
        "\n",
        "        lines.append(\"\")\n",
        "\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "\n",
        "# ─── LaTeX Table Generation ──────────────────────────────────────────────────\n",
        "\n",
        "def generate_latex_table(top_n: int = 15) -> str:\n",
        "    \"\"\"\n",
        "    Generate a LaTeX longtable comparing top polysemous nouns in both languages.\n",
        "    Format:\n",
        "      Rank | English Lemma | EN k | Chinese Lemma | ZH k\n",
        "    \"\"\"\n",
        "    en = pd.read_csv(DATA_DIR / \"english_wsi_results.csv\")\n",
        "    zh = pd.read_csv(DATA_DIR / \"chinese_wsi_results.csv\")\n",
        "\n",
        "    en_top = en.nlargest(top_n, \"k_ward\")[[\"lemma\", \"k_ward\", \"n_occurrences\"]].reset_index(drop=True)\n",
        "    zh_top = zh.nlargest(top_n, \"k_ward\")[[\"lemma\", \"k_ward\", \"n_occurrences\"]].reset_index(drop=True)\n",
        "\n",
        "    lines = [\n",
        "        r\"\\begin{table}[h]\",\n",
        "        r\"\\centering\",\n",
        "        r\"\\caption{Top Polysemous Common Nouns by Induced Sense Count (k): English vs. Chinese}\",\n",
        "        r\"\\label{tab:top_polysemous}\",\n",
        "        r\"\\begin{tabular}{clccclcc}\",\n",
        "        r\"\\toprule\",\n",
        "        r\"Rank & English Lemma & EN $k$ & EN $n$ & & Chinese Lemma & ZH $k$ & ZH $n$ \\\\\",\n",
        "        r\"\\midrule\",\n",
        "    ]\n",
        "\n",
        "    for i in range(top_n):\n",
        "        en_row = en_top.iloc[i] if i < len(en_top) else None\n",
        "        zh_row = zh_top.iloc[i] if i < len(zh_top) else None\n",
        "\n",
        "        en_lemma = en_row[\"lemma\"]                  if en_row is not None else \"\"\n",
        "        en_k     = int(en_row[\"k_ward\"])            if en_row is not None else \"\"\n",
        "        en_n     = int(en_row[\"n_occurrences\"])     if en_row is not None else \"\"\n",
        "        zh_lemma = zh_row[\"lemma\"]                  if zh_row is not None else \"\"\n",
        "        zh_k     = int(zh_row[\"k_ward\"])            if zh_row is not None else \"\"\n",
        "        zh_n     = int(zh_row[\"n_occurrences\"])     if zh_row is not None else \"\"\n",
        "\n",
        "        lines.append(\n",
        "            f\"{i+1} & {en_lemma} & {en_k} & {en_n} & & {zh_lemma} & {zh_k} & {zh_n} \\\\\\\\\"\n",
        "        )\n",
        "\n",
        "    lines += [\n",
        "        r\"\\bottomrule\",\n",
        "        r\"\\end{tabular}\",\n",
        "        r\"\\end{table}\",\n",
        "    ]\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "\n",
        "# ─── Wide Comparison Profile ─────────────────────────────────────────────────\n",
        "\n",
        "def generate_comparison_profile() -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Create a summary comparison table for paper Table 2.\n",
        "    \"\"\"\n",
        "    en = pd.read_csv(DATA_DIR / \"english_wsi_results.csv\")\n",
        "    zh = pd.read_csv(DATA_DIR / \"chinese_wsi_results.csv\")\n",
        "\n",
        "    def profile(df: pd.DataFrame, lang: str) -> dict:\n",
        "        return {\n",
        "            \"Language\":            lang,\n",
        "            \"Total lemmas\":        len(df),\n",
        "            \"Mean k (Ward)\":       round(df[\"k_ward\"].mean(), 3),\n",
        "            \"Median k (Ward)\":     round(df[\"k_ward\"].median(), 3),\n",
        "            \"Std k (Ward)\":        round(df[\"k_ward\"].std(ddof=1), 3),\n",
        "            \"% Monosemous (k=1)\":  round((df[\"k_ward\"] == 1).mean() * 100, 1),\n",
        "            \"% Polysemous (k>1)\":  round((df[\"k_ward\"] > 1).mean() * 100, 1),\n",
        "            \"Max k\":               int(df[\"k_ward\"].max()),\n",
        "            \"Mean k (KMeans)\":     round(df[\"k_kmeans\"].mean(), 3),\n",
        "            \"Agreement (Ward=KM)\": round((df[\"k_ward\"] == df[\"k_kmeans\"]).mean() * 100, 1),\n",
        "        }\n",
        "\n",
        "    rows = [profile(en, \"English\"), profile(zh, \"Chinese\")]\n",
        "    return pd.DataFrame(rows)"
      ],
      "metadata": {
        "id": "d6Rm4f_vdq7A"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "\n",
        "# ─── Configuration ────────────────────────────────────────────────────────────\n",
        "\n",
        "# DATA_DIR and OUTPUT_DIR are defined in previous cells and are globally accessible.\n",
        "# For example, they are set in cell d6Rm4f_vdq7A.\n",
        "# DATA_DIR    = Path(\"/content\") / \"bible_data\"\n",
        "# OUTPUT_DIR = Path(\"/content\") / \"output\"\n",
        "# OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "TOP_N_WORDS       = 20   # Top N most polysemous lemmas per language\n",
        "EXAMPLES_PER_SENSE = 2   # Number of example verses per cluster\n",
        "\n",
        "# ─── Sense Example Retrieval (Corrected) ─────────────────────────────────────────────────\n",
        "\n",
        "def get_sense_examples(\n",
        "    lang: str,\n",
        "    top_n: int = TOP_N_WORDS,\n",
        "    examples_per_sense: int = EXAMPLES_PER_SENSE,\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    For the top_n most polysemous lemmas, retrieve example contexts\n",
        "    for each induced sense cluster.\n",
        "\n",
        "    Returns a formatted string ready for a paper's qualitative appendix.\n",
        "    \"\"\"\n",
        "    results_df = pd.read_csv(DATA_DIR / f\"{lang}_wsi_results.csv\")\n",
        "    labels_df  = pd.read_csv(DATA_DIR / f\"{lang}_sense_labels.csv\")\n",
        "    nouns_df   = pd.read_csv(DATA_DIR / f\"{lang}_nouns.csv\")\n",
        "\n",
        "    # Top polysemous by k_ward\n",
        "    poly = results_df[results_df[\"k_ward\"] > 1].nlargest(top_n, \"k_ward\")\n",
        "\n",
        "    # Merge labels with original contexts\n",
        "    merged = labels_df.merge(\n",
        "        nouns_df[[\"lemma\", \"verse_id\", \"context\"]].drop_duplicates(),\n",
        "        on=[\"lemma\", \"verse_id\"],\n",
        "        how=\"left\",\n",
        "    )\n",
        "\n",
        "    lines = []\n",
        "    lines.append(f\"{'='*60}\")\n",
        "    lines.append(f\"QUALITATIVE SENSE ANALYSIS — {lang.upper()}\")\n",
        "    lines.append(f\"Top {top_n} Most Polysemous Nouns (WSI, Ward Clustering)\")\n",
        "    lines.append(f\"{'='*60}\\n\")\n",
        "\n",
        "    for _, row in poly.iterrows():\n",
        "        lemma = row[\"lemma\"]\n",
        "        k     = int(row[\"k_ward\"])\n",
        "        # Corrected column name from 'n_occurrences' to 'n_instances'\n",
        "        n_occ = int(row[\"n_instances\"])\n",
        "        sil   = row.get(\"silhouette_ward\", \"N/A\")\n",
        "\n",
        "        lines.append(f\"Lemma: '{lemma}'  |  k={k}  |  n={n_occ}  |  silhouette={sil}\")\n",
        "        lines.append(\"-\" * 50)\n",
        "\n",
        "        lemma_data = merged[merged[\"lemma\"] == lemma]\n",
        "\n",
        "        for cluster_id in range(k):\n",
        "            cluster_rows = lemma_data[lemma_data[\"cluster_ward\"] == cluster_id]\n",
        "            lines.append(f\"  Sense {cluster_id + 1} ({len(cluster_rows)} occurrences):\")\n",
        "\n",
        "            # Sample diverse examples\n",
        "            sample = cluster_rows.dropna(subset=[\"context\"]).head(examples_per_sense)\n",
        "            for _, ex in sample.iterrows():\n",
        "                ctx = str(ex[\"context\"])[:200].replace(\"\\n\", \" \")\n",
        "                lines.append(f\"    • {ex['verse_id']}: {ctx}\")\n",
        "\n",
        "        lines.append(\"\")\n",
        "\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "\n",
        "# ─── LaTeX Table Generation (Corrected) ──────────────────────────────────────────────────\n",
        "\n",
        "def generate_latex_table(top_n: int = 15) -> str:\n",
        "    \"\"\"\n",
        "    Generate a LaTeX longtable comparing top polysemous nouns in both languages.\n",
        "    Format:\n",
        "      Rank | English Lemma | EN k | Chinese Lemma | ZH k\n",
        "    \"\"\"\n",
        "    en = pd.read_csv(DATA_DIR / \"english_wsi_results.csv\")\n",
        "    zh = pd.read_csv(DATA_DIR / \"chinese_wsi_results.csv\")\n",
        "\n",
        "    # Corrected column name from 'n_occurrences' to 'n_instances'\n",
        "    en_top = en.nlargest(top_n, \"k_ward\")[[\"lemma\", \"k_ward\", \"n_instances\"]].reset_index(drop=True)\n",
        "    zh_top = zh.nlargest(top_n, \"k_ward\")[[\"lemma\", \"k_ward\", \"n_instances\"]].reset_index(drop=True)\n",
        "\n",
        "    lines = [\n",
        "        r\"\\begin{table}[h]\",\n",
        "        r\"\\centering\",\n",
        "        r\"\\caption{Top Polysemous Common Nouns by Induced Sense Count (k): English vs. Chinese}\",\n",
        "        r\"\\label{tab:top_polysemous}\",\n",
        "        r\"\\begin{tabular}{clccclcc}\",\n",
        "        r\"\\toprule\",\n",
        "        r\"Rank & English Lemma & EN $k$ & EN $n$ & & Chinese Lemma & ZH $k$ & ZH $n$ \\\\\",\n",
        "        r\"\\midrule\",\n",
        "    ]\n",
        "\n",
        "    for i in range(top_n):\n",
        "        en_row = en_top.iloc[i] if i < len(en_top) else None\n",
        "        zh_row = zh_top.iloc[i] if i < len(zh_top) else None\n",
        "\n",
        "        en_lemma = en_row[\"lemma\"]                  if en_row is not None else \"\"\n",
        "        en_k     = int(en_row[\"k_ward\"])            if en_row is not None else \"\"\n",
        "        # Corrected column name from 'n_occurrences' to 'n_instances'\n",
        "        en_n     = int(en_row[\"n_instances\"])     if en_row is not None else \"\"\n",
        "        zh_lemma = zh_row[\"lemma\"]                  if zh_row is not None else \"\"\n",
        "        zh_k     = int(zh_row[\"k_ward\"])            if zh_row is not None else \"\"\n",
        "        # Corrected column name from 'n_occurrences' to 'n_instances'\n",
        "        zh_n     = int(zh_row[\"n_instances\"])     if zh_row is not None else \"\"\n",
        "\n",
        "        lines.append(\n",
        "            f\"{i+1} & {en_lemma} & {en_k} & {en_n} & & {zh_lemma} & {zh_k} & {zh_n} \\\\\"\n",
        "        )\n",
        "\n",
        "    lines += [\n",
        "        r\"\\bottomrule\",\n",
        "        r\"\\end{tabular}\",\n",
        "        r\"\\end{table}\",\n",
        "    ]\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "# ─── Wide Comparison Profile (Copied for context) ─────────────────────────────────────────────────\n",
        "\n",
        "def generate_comparison_profile() -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Create a summary comparison table for paper Table 2.\n",
        "    \"\"\"\n",
        "    en = pd.read_csv(DATA_DIR / \"english_wsi_results.csv\")\n",
        "    zh = pd.read_csv(DATA_DIR / \"chinese_wsi_results.csv\")\n",
        "\n",
        "    def profile(df: pd.DataFrame, lang: str) -> dict:\n",
        "        return {\n",
        "            \"Language\":            lang,\n",
        "            \"Total lemmas\":        len(df),\n",
        "            \"Mean k (Ward)\":       round(df[\"k_ward\"].mean(), 3),\n",
        "            \"Median k (Ward)\":     round(df[\"k_ward\"].median(), 3),\n",
        "            \"Std k (Ward)\":        round(df[\"k_ward\"].std(ddof=1), 3),\n",
        "            \"% Monosemous (k=1)\":  round((df[\"k_ward\"] == 1).mean() * 100, 1),\n",
        "            \"% Polysemous (k>1)\":  round((df[\"k_ward\"] > 1).mean() * 100, 1),\n",
        "            \"Max k\":               int(df[\"k_ward\"].max()),\n",
        "            \"Mean k (KMeans)\":     round(df[\"k_kmeans\"].mean(), 3),\n",
        "            \"Agreement (Ward=KM)\": round((df[\"k_ward\"] == df[\"k_kmeans\"]).mean() * 100, 1),\n",
        "        }\n",
        "\n",
        "    rows = [profile(en, \"English\"), profile(zh, \"Chinese\")]\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Step 6: Qualitative Analysis\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ── Sense examples ────────────────────────────────────────────\n",
        "for lang in [\"english\", \"chinese\"]:\n",
        "    text = get_sense_examples(lang)\n",
        "    out  = OUTPUT_DIR / f\"qualitative_{lang}_top_polysemous.txt\"\n",
        "    out.write_text(text, encoding=\"utf-8\")\n",
        "    print(f\"  [saved] {out.name}\")\n",
        "\n",
        "# ── LaTeX table ───────────────────────────────────────────────\n",
        "latex = generate_latex_table(top_n=15)\n",
        "tex_path = OUTPUT_DIR / \"table_top_polysemous_latex.tex\"\n",
        "tex_path.write_text(latex, encoding=\"utf-8\")\n",
        "print(f\"  [saved] {tex_path.name}\")\n",
        "\n",
        "# ── Comparison profile ────────────────────────────────────────\n",
        "profile = generate_comparison_profile()\n",
        "csv_path = OUTPUT_DIR / \"polysemy_profile_comparison.csv\"\n",
        "profile.to_csv(csv_path, index=False)\n",
        "print(f\"  [saved] {csv_path.name}\")\n",
        "print()\n",
        "print(profile.to_string(index=False))\n",
        "\n",
        "print(\"\\n✓ Step 6 complete.\\n\")\n"
      ],
      "metadata": {
        "id": "K9Gd4DVWeOYw",
        "outputId": "82693b2d-54d5-440d-f5f5-258db46912fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Step 6: Qualitative Analysis\n",
            "============================================================\n",
            "  [saved] qualitative_english_top_polysemous.txt\n",
            "  [saved] qualitative_chinese_top_polysemous.txt\n",
            "  [saved] table_top_polysemous_latex.tex\n",
            "  [saved] polysemy_profile_comparison.csv\n",
            "\n",
            "Language  Total lemmas  Mean k (Ward)  Median k (Ward)  Std k (Ward)  % Monosemous (k=1)  % Polysemous (k>1)  Max k  Mean k (KMeans)  Agreement (Ward=KM)\n",
            " English           636          2.127              2.0         1.584                39.6                60.4      8            2.101                 92.3\n",
            " Chinese           572          2.110              2.0         1.753                47.9                52.1      8            2.114                 92.8\n",
            "\n",
            "✓ Step 6 complete.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r output_ot-nt.zip /content/output/"
      ],
      "metadata": {
        "id": "wnUe-3lgedhv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r data_niv_cuv_ot-nt.zip /content/bible_data/"
      ],
      "metadata": {
        "id": "Pj8aPu8Cfp1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf output\n",
        "!rm -rf bible_data"
      ],
      "metadata": {
        "id": "Xco5z30hrNhT"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ac45311"
      },
      "source": [
        "# Visualize Word Sense Clusters\n",
        "To visualize the word sense clusters for 'bread' and '餅', I will perform the following steps:\n",
        "\n",
        "1.  **Install UMAP**: Ensure the `umap-learn` library is installed for dimensionality reduction.\n",
        "2.  **Load Data**: Load the English and Chinese word embeddings (`english_embeddings.npz`, `chinese_embeddings.npz`) and sense labels (`english_sense_labels.csv`, `chinese_sense_labels.csv`).\n",
        "3.  **Filter by Lemma**: Select data specifically for the lemma 'bread' in English and '餅' (bǐng, meaning 'cake' or 'flatbread') in Chinese.\n",
        "4.  **Dimensionality Reduction**: Apply UMAP to reduce the high-dimensional embeddings of these lemmas to two dimensions (2D) for easier plotting.\n",
        "5.  **Plot Sense Clusters**: Create scatter plots for both 'bread' and '餅', where each point represents an occurrence of the word, and its color indicates the induced sense cluster. Add a legend to distinguish between senses.\n",
        "6.  **Review Plots**: Examine the generated plots to visually assess the separation and distinctness of the induced sense clusters for both lemmas.\n",
        "\n",
        "This visualization will help in qualitatively understanding how well the clustering algorithm separated the different meanings of these words based on their contextual embeddings.\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import umap\n",
        "from pathlib import Path\n",
        "\n",
        "# Configuration\n",
        "DATA_DIR = Path(\"/content\") / \"bible_data\"\n",
        "OUTPUT_DIR = Path(\"/content\") / \"output\"\n",
        "FIG_DIR = OUTPUT_DIR / \"figures\"\n",
        "FIG_DIR.mkdir(exist_ok=True) # Ensure figures directory exists\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "# --- Function to load data and filter by lemma ---\n",
        "def load_lemma_data(lang: str, lemma: str):\n",
        "    \"\"\"Loads embeddings and sense labels for a specific lemma.\"\"\"\n",
        "    embeddings_file = DATA_DIR / f\"{lang}_embeddings.npz\"\n",
        "    labels_file = DATA_DIR / f\"{lang}_sense_labels.csv\"\n",
        "\n",
        "    print(f\"Loading data for {lang} lemma '{lemma}'...\")\n",
        "\n",
        "    # Load embeddings and lemmas from .npz\n",
        "    data = np.load(embeddings_file, allow_pickle=True)\n",
        "    all_embeddings = data[\"embeddings\"]\n",
        "    all_lemmas = data[\"lemmas\"]\n",
        "    all_verse_ids = data[\"verse_ids\"]\n",
        "\n",
        "    # Filter for the specific lemma\n",
        "    lemma_mask = (all_lemmas == lemma)\n",
        "    lemma_embeddings = all_embeddings[lemma_mask]\n",
        "    lemma_verse_ids = all_verse_ids[lemma_mask]\n",
        "\n",
        "    # Load sense labels\n",
        "    labels_df = pd.read_csv(labels_file)\n",
        "    # Filter labels for the specific lemma and relevant verse_ids\n",
        "    lemma_labels_df = labels_df[\n",
        "        (labels_df[\"lemma\"] == lemma) & (labels_df[\"verse_id\"].isin(lemma_verse_ids))\n",
        "    ].copy()\n",
        "\n",
        "    # Ensure the order of labels matches the order of embeddings\n",
        "    # This assumes verse_ids in embeddings are unique enough or can be joined reliably\n",
        "    # If not, a more robust merge is needed using original index or full context\n",
        "    # For now, we'll align by verse_id, assuming embeddings and labels are generated from the same source order\n",
        "    lemma_labels_df = lemma_labels_df.set_index('verse_id').loc[lemma_verse_ids].reset_index()\n",
        "\n",
        "    if len(lemma_embeddings) != len(lemma_labels_df):\n",
        "        print(f\"Warning: Mismatch in count for '{lemma}'. Embeddings: {len(lemma_embeddings)}, Labels: {len(lemma_labels_df)}\")\n",
        "        # Attempt to reconcile by matching contexts if available, or just proceed with common.\n",
        "        # For simplicity, proceeding assuming verse_ids align after filtering\n",
        "        # If this causes issues, more rigorous data alignment might be needed.\n",
        "\n",
        "    # Only keep Ward clustering labels for visualization as per plan\n",
        "    sense_labels = lemma_labels_df[\"cluster_ward\"].values\n",
        "\n",
        "    print(f\"Found {len(lemma_embeddings)} occurrences for '{lemma}'.\")\n",
        "    return lemma_embeddings, sense_labels, lemma_verse_ids\n",
        "\n",
        "# --- Function for UMAP dimensionality reduction ---\n",
        "def apply_umap(embeddings: np.ndarray, n_components: int = 2):\n",
        "    \"\"\"Applies UMAP for dimensionality reduction.\"\"\"\n",
        "    print(f\"Applying UMAP to reduce embeddings to {n_components}D...\")\n",
        "    reducer = umap.UMAP(\n",
        "        n_components=n_components,\n",
        "        random_state=RANDOM_STATE,\n",
        "        metric='cosine', # Good for high-dimensional data like embeddings\n",
        "        n_jobs=-1 # Use all available cores\n",
        "    )\n",
        "    reduced_embeddings = reducer.fit_transform(embeddings)\n",
        "    return reduced_embeddings\n",
        "\n",
        "# --- Function to plot sense clusters ---\n",
        "def plot_sense_clusters(\n",
        "    reduced_embeddings: np.ndarray,\n",
        "    sense_labels: np.ndarray,\n",
        "    lemma: str,\n",
        "    lang: str,\n",
        "):\n",
        "    \"\"\"Generates a scatter plot of 2D embeddings, colored by sense cluster.\"\"\"\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    unique_senses = sorted(np.unique(sense_labels))\n",
        "\n",
        "    # Use a color palette suitable for categorical data\n",
        "    palette = sns.color_palette(\"tab10\", n_colors=len(unique_senses))\n",
        "\n",
        "    for i, sense_id in enumerate(unique_senses):\n",
        "        mask = (sense_labels == sense_id)\n",
        "        plt.scatter(\n",
        "            reduced_embeddings[mask, 0],\n",
        "            reduced_embeddings[mask, 1],\n",
        "            label=f\"Sense {sense_id + 1}\",\n",
        "            color=palette[i],\n",
        "            alpha=0.7,\n",
        "            s=50, # size of points\n",
        "            edgecolors='w', # white edge for better visibility\n",
        "            linewidth=0.5\n",
        "        )\n",
        "\n",
        "    plt.title(f\"UMAP Visualization of {lang.capitalize()} '{lemma}' Sense Clusters\", fontsize=14)\n",
        "    plt.xlabel(\"UMAP Component 1\", fontsize=12)\n",
        "    plt.ylabel(\"UMAP Component 2\", fontsize=12)\n",
        "    plt.legend(title=\"Sense Clusters\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    plt.grid(True, linestyle='--', alpha=0.6)\n",
        "    plt.tight_layout() # Adjust layout to prevent labels from overlapping\n",
        "    plot_filename = FIG_DIR / f\"{lang}_{lemma}_sense_clusters_umap.png\"\n",
        "    plt.savefig(plot_filename, dpi=300)\n",
        "    plt.show()\n",
        "    print(f\"Saved plot to {plot_filename}\")\n",
        "\n",
        "# --- Main execution ---\n",
        "print(\"--- Starting Sense Cluster Visualization ---\")\n",
        "\n",
        "# 1. English 'bread'\n",
        "en_lemma = \"bread\"\n",
        "en_embeddings, en_labels, _ = load_lemma_data(\"english\", en_lemma)\n",
        "if len(en_embeddings) > 0:\n",
        "    en_reduced_embeddings = apply_umap(en_embeddings)\n",
        "    plot_sense_clusters(en_reduced_embeddings, en_labels, en_lemma, \"english\")\n",
        "else:\n",
        "    print(f\"No embeddings found for English lemma '{en_lemma}'. Skipping visualization.\")\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "# 2. Chinese '餅'\n",
        "zh_lemma = \"餅\"\n",
        "zh_embeddings, zh_labels, _ = load_lemma_data(\"chinese\", zh_lemma)\n",
        "if len(zh_embeddings) > 0:\n",
        "    zh_reduced_embeddings = apply_umap(zh_embeddings)\n",
        "    plot_sense_clusters(zh_reduced_embeddings, zh_labels, zh_lemma, \"chinese\")\n",
        "else:\n",
        "    print(f\"No embeddings found for Chinese lemma '{zh_lemma}'. Skipping visualization.\")\n",
        "\n",
        "print(\"\\n--- Sense Cluster Visualization Complete ---\")\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import umap\n",
        "from pathlib import Path\n",
        "import matplotlib.font_manager as fm\n",
        "import os\n",
        "\n",
        "# Configuration\n",
        "DATA_DIR = Path(\"/content\") / \"bible_data\"\n",
        "OUTPUT_DIR = Path(\"/content\") / \"output\"\n",
        "FIG_DIR = OUTPUT_DIR / \"figures\"\n",
        "FIG_DIR.mkdir(exist_ok=True) # Ensure figures directory exists\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "# --- Function to load data and filter by lemma ---\n",
        "def load_lemma_data(lang: str, lemma: str):\n",
        "    \"\"\"\n",
        "    Loads embeddings and sense labels for a specific lemma, ensuring proper alignment.\n",
        "    This version slices directly from the full embeddings and labels arrays using global indices.\n",
        "    \"\"\"\n",
        "    embeddings_file = DATA_DIR / f\"{lang}_embeddings.npz\"\n",
        "    labels_file = DATA_DIR / f\"{lang}_sense_labels.csv\"\n",
        "\n",
        "    print(f\"Loading data for {lang} lemma '{lemma}'...\")\n",
        "\n",
        "    if not embeddings_file.exists():\n",
        "        raise FileNotFoundError(f\"Embeddings file not found: {embeddings_file}\")\n",
        "    if not labels_file.exists():\n",
        "        raise FileNotFoundError(f\"Labels file not found: {labels_file}\")\n",
        "\n",
        "    # Load all data from the .npz file\n",
        "    npz_data = np.load(embeddings_file, allow_pickle=True)\n",
        "    all_embeddings_npz = npz_data[\"embeddings\"]\n",
        "    all_lemmas_npz = npz_data[\"lemmas\"]\n",
        "\n",
        "    # Load all sense labels from the .csv file\n",
        "    labels_df_full = pd.read_csv(labels_file)\n",
        "    all_cluster_ward_labels_full = labels_df_full['cluster_ward'].values\n",
        "\n",
        "    # Identify the global indices of occurrences for the target lemma\n",
        "    lemma_indices = np.where(all_lemmas_npz == lemma)[0]\n",
        "\n",
        "    if len(lemma_indices) == 0:\n",
        "        print(f\"No occurrences found for lemma '{lemma}' in {lang} data. Skipping.\")\n",
        "        return np.array([]), np.array([]), np.array([])\n",
        "\n",
        "    # Slice the embeddings and labels using these global indices\n",
        "    lemma_embeddings = all_embeddings_npz[lemma_indices]\n",
        "    lemma_sense_labels = all_cluster_ward_labels_full[lemma_indices]\n",
        "\n",
        "    # Final sanity check on lengths\n",
        "    if len(lemma_embeddings) != len(lemma_sense_labels):\n",
        "        raise ValueError(\n",
        "            f\"Final alignment failed for lemma '{lemma}'. \"\n",
        "            f\"Embeddings count: {len(lemma_embeddings)}, Labels count: {len(lemma_sense_labels)}. \"\n",
        "            \"This indicates a deep inconsistency in data generation which should be investigated in Step 2/3/4.\"\n",
        "        )\n",
        "\n",
        "    print(f\"Found {len(lemma_embeddings)} occurrences for '{lemma}', with aligned labels.\")\n",
        "    # Return None for verse_ids as they are not needed for plotting and the new method doesn't explicitly track them here.\n",
        "    return lemma_embeddings, lemma_sense_labels, None\n",
        "\n",
        "# --- Function for UMAP dimensionality reduction ---\n",
        "def apply_umap(embeddings: np.ndarray, n_components: int = 2):\n",
        "    \"\"\"Applies UMAP for dimensionality reduction.\"\"\"\n",
        "    print(f\"Applying UMAP to reduce embeddings to {n_components}D...\")\n",
        "    reducer = umap.UMAP(\n",
        "        n_components=n_components,\n",
        "        random_state=RANDOM_STATE,\n",
        "        metric='cosine', # Good for high-dimensional data like embeddings\n",
        "        n_jobs=-1 # Use all available cores\n",
        "    )\n",
        "    reduced_embeddings = reducer.fit_transform(embeddings)\n",
        "    return reduced_embeddings\n",
        "\n",
        "# --- Function to plot sense clusters ---\n",
        "def plot_sense_clusters(\n",
        "    reduced_embeddings: np.ndarray,\n",
        "    sense_labels: np.ndarray,\n",
        "    lemma: str,\n",
        "    lang: str,\n",
        "):\n",
        "    \"\"\"Generates a scatter plot of 2D embeddings, colored by sense cluster.\"\"\"\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    unique_senses = sorted(np.unique(sense_labels))\n",
        "\n",
        "    # Use a color palette suitable for categorical data\n",
        "    palette = sns.color_palette(\"tab10\", n_colors=len(unique_senses))\n",
        "\n",
        "    for i, sense_id in enumerate(unique_senses):\n",
        "        mask = (sense_labels == sense_id)\n",
        "        plt.scatter(\n",
        "            reduced_embeddings[mask, 0],\n",
        "            reduced_embeddings[mask, 1],\n",
        "            label=f\"Sense {sense_id + 1}\",\n",
        "            color=palette[i],\n",
        "            alpha=0.7,\n",
        "            s=50, # size of points\n",
        "            edgecolors='w', # white edge for better visibility\n",
        "            linewidth=0.5\n",
        "        )\n",
        "\n",
        "    plt.title(f\"UMAP Visualization of {lang.capitalize()} '{lemma}' Sense Clusters\", fontsize=14)\n",
        "    plt.xlabel(\"UMAP Component 1\", fontsize=12)\n",
        "    plt.ylabel(\"UMAP Component 2\", fontsize=12)\n",
        "    plt.legend(title=\"Sense Clusters\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    plt.grid(True, linestyle='--', alpha=0.6)\n",
        "    plt.tight_layout() # Adjust layout to prevent labels from overlapping\n",
        "    plot_filename = FIG_DIR / f\"{lang}_{lemma}_sense_clusters_umap.png\"\n",
        "    plt.savefig(plot_filename, dpi=300)\n",
        "    plt.show()\n",
        "    print(f\"Saved plot to {plot_filename}\")\n",
        "\n",
        "# --- Main execution ---\n",
        "print(\"--- Starting Sense Cluster Visualization ---\")\n",
        "\n",
        "# --- Font Configuration for Chinese Characters ---\n",
        "# Install a Chinese font (e.g., Noto Sans CJK)\n",
        "!apt-get update -qq > /dev/null\n",
        "!apt-get install -qq fonts-noto-cjk > /dev/null\n",
        "\n",
        "# Clear and rebuild matplotlib font cache\n",
        "# This is crucial for newly installed fonts to be recognized\n",
        "fm._load_fontmanager(try_read_cache=False)\n",
        "fm._fmcache = {} # Clear the font cache manually\n",
        "print(\"Matplotlib font cache cleared.\")\n",
        "\n",
        "# Find the path to a Noto Sans CJK font\n",
        "font_files = fm.findSystemFonts(fontpaths=None, fontext='ttf')\n",
        "chosen_font_path = None\n",
        "for fpath in font_files:\n",
        "    if 'NotoSansCJK' in fpath:\n",
        "        chosen_font_path = fpath\n",
        "        break\n",
        "\n",
        "if chosen_font_path:\n",
        "    print(f\"Found Noto Sans CJK font at: {chosen_font_path}\")\n",
        "    # Add this specific font to font manager\n",
        "    fm.fontManager.addfont(chosen_font_path)\n",
        "\n",
        "    # Get the actual font name from the file\n",
        "    prop = fm.FontProperties(fname=chosen_font_path)\n",
        "    actual_font_name = prop.get_name()\n",
        "    print(f\"Actual font family name: {actual_font_name}\")\n",
        "\n",
        "    # Set generic font family to 'sans-serif'\n",
        "    plt.rcParams['font.family'] = ['sans-serif']\n",
        "    # Add the Chinese font to the beginning of the 'sans-serif' font list\n",
        "    # This makes it the preferred font for sans-serif text.\n",
        "    plt.rcParams['font.sans-serif'] = [actual_font_name] + plt.rcParams['font.sans-serif']\n",
        "    plt.rcParams['axes.unicode_minus'] = False # Fix minus sign display issues\n",
        "    # Set font size for better readability\n",
        "    plt.rcParams[\"font.size\"] = 11\n",
        "    plt.rcParams[\"axes.titlesize\"] = 13\n",
        "    plt.rcParams[\"axes.labelsize\"] = 12\n",
        "    print(f\"Matplotlib configured to use '{actual_font_name}'.\")\n",
        "else:\n",
        "    print(f\"Warning: No Noto Sans CJK font found after install. Chinese characters might not display correctly.\")\n",
        "\n",
        "# 1. English 'bread'\n",
        "en_lemma = \"bread\"\n",
        "en_embeddings, en_labels, _ = load_lemma_data(\"english\", en_lemma)\n",
        "if len(en_embeddings) > 0:\n",
        "    en_reduced_embeddings = apply_umap(en_embeddings)\n",
        "    plot_sense_clusters(en_reduced_embeddings, en_labels, en_lemma, \"english\")\n",
        "else:\n",
        "    print(f\"No embeddings found for English lemma '{en_lemma}'. Skipping visualization.\")\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "# 2. Chinese '禮'\n",
        "# The user requested '禮' instead of '餅' in the previous run, so I'll keep '禮' for consistency with the traceback.\n",
        "zh_lemma = \"禮\"\n",
        "zh_embeddings, zh_labels, _ = load_lemma_data(\"chinese\", zh_lemma)\n",
        "if len(zh_embeddings) > 0:\n",
        "    zh_reduced_embeddings = apply_umap(zh_embeddings)\n",
        "    plot_sense_clusters(zh_reduced_embeddings, zh_labels, zh_lemma, \"chinese\")\n",
        "else:\n",
        "    print(f\"No embeddings found for Chinese lemma '{zh_lemma}'. Skipping visualization.\")\n",
        "\n",
        "print(\"\\n--- Sense Cluster Visualization Complete ---\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfoirPbZ4jrf",
        "outputId": "331374e4-87bb-49b6-b714-1e0a358c50d1"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Sense Cluster Visualization ---\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Matplotlib font cache cleared.\n",
            "Found Noto Sans CJK font at: /usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc\n",
            "Actual font family name: Noto Sans CJK JP\n",
            "Matplotlib configured to use 'Noto Sans CJK JP'.\n",
            "Loading data for english lemma 'bread'...\n",
            "Found 178 occurrences for 'bread', with aligned labels.\n",
            "Applying UMAP to reduce embeddings to 2D...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved plot to /content/output/figures/english_bread_sense_clusters_umap.png\n",
            "\n",
            "\n",
            "Loading data for chinese lemma '禮'...\n",
            "Found 77 occurrences for '禮', with aligned labels.\n",
            "Applying UMAP to reduce embeddings to 2D...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved plot to /content/output/figures/chinese_禮_sense_clusters_umap.png\n",
            "\n",
            "--- Sense Cluster Visualization Complete ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "950f8253",
        "outputId": "b94cdf7a-39af-48d3-98e5-a6e7fbfa61b4"
      },
      "source": [
        "print(\"--- Quantitative Metrics Comparison ---\")\n",
        "\n",
        "# Metrics for English 'bread'\n",
        "en_bread_metrics = en[en['lemma'] == 'bread']\n",
        "print(\"\\nEnglish 'bread' metrics:\")\n",
        "display(en_bread_metrics[['lemma', 'n_instances', 'k_ward', 'k_kmeans', 'silhouette_ward', 'agreement_pct']])\n",
        "\n",
        "# Metrics for Chinese '禮'\n",
        "zh_li_metrics = zh[zh['lemma'] == '禮']\n",
        "print(\"\\nChinese '禮' metrics:\")\n",
        "display(zh_li_metrics[['lemma', 'n_instances', 'k_ward', 'k_kmeans', 'silhouette_ward', 'agreement_pct']])\n",
        "\n",
        "print(\"\\n--- End of Quantitative Metrics Comparison ---\")"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Quantitative Metrics Comparison ---\n",
            "\n",
            "English 'bread' metrics:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "    lemma  n_instances  k_ward  k_kmeans  silhouette_ward  agreement_pct\n",
              "49  bread          178       2         2           0.4003          94.38"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b1404972-b0cb-4f87-9bc4-9e91dff7db1d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>lemma</th>\n",
              "      <th>n_instances</th>\n",
              "      <th>k_ward</th>\n",
              "      <th>k_kmeans</th>\n",
              "      <th>silhouette_ward</th>\n",
              "      <th>agreement_pct</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>bread</td>\n",
              "      <td>178</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0.4003</td>\n",
              "      <td>94.38</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b1404972-b0cb-4f87-9bc4-9e91dff7db1d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b1404972-b0cb-4f87-9bc4-9e91dff7db1d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b1404972-b0cb-4f87-9bc4-9e91dff7db1d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"print(\\\"\\\\n--- End of Quantitative Metrics Comparison ---\\\")\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"lemma\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"bread\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"n_instances\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 178,\n        \"max\": 178,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          178\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"k_ward\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 2,\n        \"max\": 2,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"k_kmeans\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 2,\n        \"max\": 2,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"silhouette_ward\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.400299996137619,\n        \"max\": 0.400299996137619,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.400299996137619\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"agreement_pct\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 94.38,\n        \"max\": 94.38,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          94.38\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chinese '禮' metrics:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "    lemma  n_instances  k_ward  k_kmeans  silhouette_ward  agreement_pct\n",
              "330     禮           77       2         2           0.4549            1.3"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9ee9fd3f-13d0-4d8c-8169-27a9688a23d3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>lemma</th>\n",
              "      <th>n_instances</th>\n",
              "      <th>k_ward</th>\n",
              "      <th>k_kmeans</th>\n",
              "      <th>silhouette_ward</th>\n",
              "      <th>agreement_pct</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>330</th>\n",
              "      <td>禮</td>\n",
              "      <td>77</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0.4549</td>\n",
              "      <td>1.3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9ee9fd3f-13d0-4d8c-8169-27a9688a23d3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9ee9fd3f-13d0-4d8c-8169-27a9688a23d3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9ee9fd3f-13d0-4d8c-8169-27a9688a23d3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"print(\\\"\\\\n--- End of Quantitative Metrics Comparison ---\\\")\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"lemma\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"\\u79ae\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"n_instances\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 77,\n        \"max\": 77,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          77\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"k_ward\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 2,\n        \"max\": 2,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"k_kmeans\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 2,\n        \"max\": 2,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"silhouette_ward\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.4548999965190887,\n        \"max\": 0.4548999965190887,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.4548999965190887\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"agreement_pct\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 1.3,\n        \"max\": 1.3,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1.3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- End of Quantitative Metrics Comparison ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SPzgYOhpf8ps"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
