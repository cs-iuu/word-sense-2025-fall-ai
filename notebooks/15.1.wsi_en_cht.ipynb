{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d49ff9dcc67549d0b8bd58f6102a6261": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cc35e34a34de4c7890ed5240ab98a306",
              "IPY_MODEL_67fddc51749445a7b7c435ba71963658",
              "IPY_MODEL_de19b72f4810489fac9cf5a4eb956843"
            ],
            "layout": "IPY_MODEL_4b616585cba948a7afc3bcc096a6c9f6"
          }
        },
        "cc35e34a34de4c7890ed5240ab98a306": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a4c8580f4b9142bb90ae8b1333518b93",
            "placeholder": "​",
            "style": "IPY_MODEL_7e0f573c3a7f4fc4a1fc375f7908d0e9",
            "value": "Loading weights: 100%"
          }
        },
        "67fddc51749445a7b7c435ba71963658": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a91b92710774db6aedce4488a4cefcb",
            "max": 199,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f323edfb8b1e4087b4d19a103977b8bf",
            "value": 199
          }
        },
        "de19b72f4810489fac9cf5a4eb956843": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d80c8ef594684cf889582699515f78cf",
            "placeholder": "​",
            "style": "IPY_MODEL_2e951d7208ba4b53b69486269030d878",
            "value": " 199/199 [00:00&lt;00:00, 831.66it/s, Materializing param=pooler.dense.weight]"
          }
        },
        "4b616585cba948a7afc3bcc096a6c9f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4c8580f4b9142bb90ae8b1333518b93": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e0f573c3a7f4fc4a1fc375f7908d0e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1a91b92710774db6aedce4488a4cefcb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f323edfb8b1e4087b4d19a103977b8bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d80c8ef594684cf889582699515f78cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e951d7208ba4b53b69486269030d878": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cs-iuu/word-sense-2025-fall-ai/blob/main/notebooks/15.1.wsi_en_cht.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Preprocessing: Extract Common Nouns"
      ],
      "metadata": {
        "id": "OwnCnpqk0M7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy jieba pandas --break-system-packages\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "ZDh6BuKL3z-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Step 2: Preprocessing\n",
        "======================\n",
        "Tokenizes, POS-tags, and lemmatizes both corpora.\n",
        "Extracts common nouns only (no proper nouns, pronouns, or stopwords).\n",
        "Applies a minimum frequency threshold to filter rare words.\n",
        "\n",
        "Outputs:\n",
        "  - data/english_nouns.csv   : lemma, verse_id, token, context (full verse)\n",
        "  - data/chinese_nouns.csv   : lemma, verse_id, token, context\n",
        "  - data/english_noun_freq.csv\n",
        "  - data/chinese_noun_freq.csv\n",
        "\n",
        "Usage:\n",
        "  pip install spacy jieba pandas --break-system-packages\n",
        "  python -m spacy download en_core_web_sm\n",
        "  python 02_preprocessing.py\n",
        "\n",
        "Design decisions (paper §3.2):\n",
        "  - English: spaCy en_core_web_sm for tokenization, POS, lemmatization\n",
        "  - Chinese: jieba for word segmentation + custom POS (jieba.posseg)\n",
        "  - POS filters: English NOUN tag; Chinese POS prefix 'n' (common noun)\n",
        "  - Proper noun exclusion: English PROPN tag excluded; Chinese 'nr','ns','nt','nz' excluded\n",
        "  - Minimum frequency: MIN_FREQ = 30 (ensures sufficient WSI context)\n",
        "  - Stopwords: NLTK English stopwords; custom Chinese stopword list\n",
        "\"\"\"\n",
        "\n",
        "import re\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "\n",
        "# ─── Configuration ────────────────────────────────────────────────────────────\n",
        "\n",
        "DATA_DIR = Path(\"/content\") / \"bible_data\"\n",
        "MIN_FREQ = 30          # Minimum occurrences per lemma for WSI\n",
        "MAX_CONTEXT_LEN = 512  # Characters — prevents overlong inputs to transformers\n",
        "\n",
        "# Chinese POS tags for common nouns (jieba.posseg notation)\n",
        "ZH_NOUN_PREFIXES = {\"n\"}           # Common noun prefix\n",
        "ZH_EXCLUDE_TAGS  = {\"nr\", \"ns\", \"nt\", \"nz\", \"nw\"}  # Proper nouns to exclude\n",
        "\n",
        "# ── Theological proper noun exclusion lists ───────────────────────────────────\n",
        "# These terms are proper nouns in English (God, Lord, Christ etc.) — excluded\n",
        "# by spaCy's PROPN tag — but are tagged as common nouns n by jieba in Chinese\n",
        "# due to the absence of capitalisation. They must be excluded explicitly from\n",
        "# the Chinese data to ensure cross-lingual comparability.\n",
        "#\n",
        "# English side: spaCy correctly tags God/Lord/Christ as PROPN (excluded).\n",
        "# Exception: \"Spirit\" (Holy Spirit) is sometimes tagged NOUN by spaCy, so it\n",
        "# is added to EN_THEOLOGICAL_EXCLUDE as a lemma-level backstop.\n",
        "#\n",
        "# Borderline cases kept in both languages:\n",
        "#   先知/prophet  — generic occupational noun, polysemous, common in both\n",
        "#   天使/angel    — generic supernatural being, common noun in both\n",
        "#   魔鬼/devil    — common noun in EN; jieba tags n in ZH\n",
        "\n",
        "ZH_THEOLOGICAL_EXCLUDE = {\n",
        "    # Core deity names / titles\n",
        "    \"神\",     # God (most frequent — 1244 occurrences)\n",
        "    \"主\",     # Lord\n",
        "    \"上帝\",   # God (formal)\n",
        "    \"耶和華\", # Yahweh / LORD\n",
        "    \"基督\",   # Christ\n",
        "    \"耶穌\",   # Jesus (also usually tagged nr, but belt-and-suspenders)\n",
        "    \"聖靈\",   # Holy Spirit\n",
        "    \"聖神\",   # Holy Spirit (alternate form in some CUV editions)\n",
        "    \"彌賽亞\", # Messiah\n",
        "    # Adversarial proper nouns\n",
        "    \"撒但\",   # Satan\n",
        "    \"別西卜\", # Beelzebub\n",
        "}\n",
        "\n",
        "EN_THEOLOGICAL_EXCLUDE = {\n",
        "    # Lemma-level backstop for cases where spaCy tags as NOUN not PROPN\n",
        "    \"spirit\",    # \"Holy Spirit\" — spaCy inconsistently tags as NOUN\n",
        "    \"ghost\",     # \"Holy Ghost\" (KJV form; rare in NIV but present)\n",
        "}\n",
        "\n",
        "# Path to custom jieba dictionary for biblical proper names\n",
        "JIEBA_DICT_PATH = Path(\"/content\") / \"bible_data\" / \"jieba_biblical_dict.txt\"\n",
        "\n",
        "# ── Post-segmentation POS correction ─────────────────────────────────────────\n",
        "# jieba's POS tagger runs independently of the segmentation dictionary and\n",
        "# can assign incorrect tags even for dictionary entries. These words are\n",
        "# forced to tag n after segmentation regardless of what the POS tagger assigned.\n",
        "#\n",
        "# 地: jieba assigns uv (虛詞/copular particle) in classical subject-predicate\n",
        "#     constructions like 地是空虛混沌 (Gen.1.2) because it parses 地 as a\n",
        "#     topic marker rather than a subject noun. This is a known jieba limitation\n",
        "#     with literary Chinese. Since English \"earth\" (freq=739) is always tagged\n",
        "#     NOUN by spaCy, forcing 地 to n is required for cross-lingual comparability.\n",
        "ZH_FORCE_NOUN_TAG = {\n",
        "    \"地\",   # earth/ground/land — incorrectly tagged uv in copular constructions\n",
        "}\n",
        "\n",
        "# ─── English Preprocessing ────────────────────────────────────────────────────\n",
        "\n",
        "\n",
        "def preprocess_english(verse_csv: Path) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Process English verses with spaCy.\n",
        "    Returns long-format DataFrame: one row per noun occurrence.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        import spacy\n",
        "    except ImportError:\n",
        "        raise ImportError(\"Run: pip install spacy --break-system-packages && python -m spacy download en_core_web_sm\")\n",
        "\n",
        "    print(\"  [EN] Loading spaCy model…\")\n",
        "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"textcat\"])\n",
        "\n",
        "    df = pd.read_csv(verse_csv)\n",
        "    print(f\"  [EN] Processing {len(df):,} verses…\")\n",
        "\n",
        "    import time\n",
        "    records   = []\n",
        "    texts     = df[\"text\"].tolist()\n",
        "    verse_ids = df[\"verse_id\"].tolist()\n",
        "    total     = len(texts)\n",
        "    t0        = time.time()\n",
        "\n",
        "    for i, (doc, vid) in enumerate(zip(nlp.pipe(texts, batch_size=512), verse_ids), 1):\n",
        "        context = doc.text[:MAX_CONTEXT_LEN]\n",
        "        for token in doc:\n",
        "            # Keep only common nouns; exclude proper nouns and pronouns\n",
        "            if (\n",
        "                token.pos_ == \"NOUN\"\n",
        "                and not token.is_stop\n",
        "                and not token.is_punct\n",
        "                and len(token.lemma_) > 1\n",
        "                and token.lemma_.isalpha()\n",
        "                and token.lemma_.lower() not in EN_THEOLOGICAL_EXCLUDE\n",
        "            ):\n",
        "                records.append({\n",
        "                    \"verse_id\": vid,\n",
        "                    \"token\":    token.text,\n",
        "                    \"lemma\":    token.lemma_.lower(),\n",
        "                    \"context\":  context,\n",
        "                })\n",
        "        if i % 1000 == 0 or i == total:\n",
        "            elapsed = time.time() - t0\n",
        "            rate    = i / elapsed if elapsed > 0 else 0\n",
        "            eta_min = (total - i) / rate / 60 if rate > 0 else 0\n",
        "            print(f\"    … {i:,}/{total:,} verses  \"\n",
        "                  f\"({rate:.0f} v/s)  \"\n",
        "                  f\"ETA {eta_min:.1f} min  \"\n",
        "                  f\"nouns so far: {len(records):,}\",\n",
        "                  end=\"\\r\")\n",
        "\n",
        "    elapsed_total = time.time() - t0\n",
        "    print(f\"\\n  [EN] Done in {elapsed_total/60:.1f} min. \"\n",
        "          f\"Extracted {len(records):,} noun occurrences.\")\n",
        "    return pd.DataFrame(records)\n",
        "\n",
        "\n",
        "# ─── Chinese Preprocessing ────────────────────────────────────────────────────\n",
        "\n",
        "\n",
        "def preprocess_chinese(verse_csv: Path) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Process Chinese verses with jieba (word segmentation + POS tagging).\n",
        "    Returns long-format DataFrame: one row per noun occurrence.\n",
        "    \"\"\"\n",
        "    import time\n",
        "    try:\n",
        "        import jieba\n",
        "        import jieba.posseg as pseg\n",
        "    except ImportError:\n",
        "        raise ImportError(\"Run: pip install jieba --break-system-packages\")\n",
        "\n",
        "    # Silence jieba logging FIRST, before any other jieba calls\n",
        "    jieba.setLogLevel(\"ERROR\")\n",
        "\n",
        "    # Load custom dictionary\n",
        "    if JIEBA_DICT_PATH.exists():\n",
        "        jieba.load_userdict(str(JIEBA_DICT_PATH))\n",
        "        print(f\"  [ZH] Loaded custom dictionary: {JIEBA_DICT_PATH.name}\", flush=True)\n",
        "    else:\n",
        "        print(f\"  [ZH] WARNING: custom dictionary not found at {JIEBA_DICT_PATH}\", flush=True)\n",
        "\n",
        "    # Force jieba to build its internal trie NOW with a visible message.\n",
        "    # Without this explicit call, the first pseg.cut() silently blocks\n",
        "    # for 30-60 seconds before any progress lines appear.\n",
        "    print(\"  [ZH] Building jieba model (one-time, ~10-30s)...\", flush=True)\n",
        "    jieba.initialize()\n",
        "    print(\"  [ZH] Model ready.\", flush=True)\n",
        "\n",
        "    zh_stopwords = _load_chinese_stopwords()\n",
        "\n",
        "    df    = pd.read_csv(verse_csv)\n",
        "    total = len(df)\n",
        "    print(f\"  [ZH] Processing {total:,} verses...\", flush=True)\n",
        "    t0 = time.time()\n",
        "\n",
        "    records = []\n",
        "    for i, row in enumerate(df.itertuples(index=False), 1):\n",
        "        verse_id = row.verse_id\n",
        "        text     = str(row.text)\n",
        "        context  = text[:MAX_CONTEXT_LEN]\n",
        "\n",
        "        for word, flag in pseg.cut(text):\n",
        "            flag_str = str(flag)\n",
        "            # Force known mis-tagged tokens to correct POS\n",
        "            if word in ZH_FORCE_NOUN_TAG:\n",
        "                flag_str = \"n\"\n",
        "            if (\n",
        "                flag_str[:1] in ZH_NOUN_PREFIXES\n",
        "                and flag_str not in ZH_EXCLUDE_TAGS\n",
        "                and word not in zh_stopwords\n",
        "                and word not in ZH_THEOLOGICAL_EXCLUDE\n",
        "                and len(word) >= 1\n",
        "                and not word.isdigit()\n",
        "            ):\n",
        "                records.append({\n",
        "                    \"verse_id\": verse_id,\n",
        "                    \"token\":    word,\n",
        "                    \"lemma\":    word,\n",
        "                    \"context\":  context,\n",
        "                })\n",
        "\n",
        "        # Progress every 100 verses — print on new lines so nothing is missed\n",
        "        if i % 100 == 0 or i == total:\n",
        "            elapsed = time.time() - t0\n",
        "            rate    = i / elapsed if elapsed > 0 else 0\n",
        "            eta_min = (total - i) / rate / 60 if rate > 0 else 0\n",
        "            print(\n",
        "                f\"    ... {i:,}/{total:,} verses\"\n",
        "                f\"  ({rate:.0f} v/s)\"\n",
        "                f\"  ETA {eta_min:.1f} min\"\n",
        "                f\"  nouns: {len(records):,}\",\n",
        "                flush=True\n",
        "            )\n",
        "\n",
        "    elapsed_total = time.time() - t0\n",
        "    print(f\"  [ZH] Done in {elapsed_total/60:.1f} min. \"\n",
        "          f\"Extracted {len(records):,} noun occurrences.\")\n",
        "    return pd.DataFrame(records)\n",
        "\n",
        "def _load_chinese_stopwords() -> set:\n",
        "    \"\"\"\n",
        "    Chinese function word stoplist for CUV Traditional (CHT) text.\n",
        "\n",
        "    Design decisions:\n",
        "    ─────────────────────────────────────────────────────────────\n",
        "    1. CHT variants included alongside CHS equivalents for all\n",
        "       characters that differ between scripts (說/说, 會/会, etc.)\n",
        "\n",
        "    2. 人 is NOT a stopword. It is a genuine common noun meaning\n",
        "       \"person / people / man / humanity\" and is highly polysemous\n",
        "       in biblical text. Jieba tags it as n (common noun) in most\n",
        "       contexts, so it passes the POS filter correctly. Removing it\n",
        "       would discard one of the most semantically rich words in the\n",
        "       corpus.\n",
        "\n",
        "    3. Pronouns (他/她/祂/你/我 etc.) are NOT listed here. They are\n",
        "       tagged by jieba as r (pronoun), which is already excluded by\n",
        "       the POS filter (we keep only n* tags). Listing them would be\n",
        "       redundant. The various gendered and honorific variants\n",
        "       (他/她/它/祂) all carry the r tag and are excluded uniformly.\n",
        "\n",
        "    4. This list covers only high-frequency grammatical function\n",
        "       words that jieba may occasionally mis-tag as nouns.\n",
        "       It is intentionally conservative.\n",
        "    ─────────────────────────────────────────────────────────────\n",
        "    \"\"\"\n",
        "    return {\n",
        "        # Structural particles (occasionally mis-tagged as n by jieba)\n",
        "        # NOTE: 地 is intentionally NOT listed here.\n",
        "        # In CUV literary style, 地 is overwhelmingly used as a noun\n",
        "        # (earth/land/ground) matching English \"earth\" (freq=739).\n",
        "        # The adverbial particle use of 地 is rare in classical biblical text.\n",
        "        # Removing it would create an asymmetry with English where \"earth\"\n",
        "        # is correctly retained as a high-frequency common noun.\n",
        "        \"的\", \"得\",\n",
        "        # Aspect markers — CHT: 著, CHS: 着\n",
        "        \"了\", \"著\", \"着\",\n",
        "        # Conjunctions / connectives\n",
        "        \"和\", \"與\", \"与\", \"及\", \"或\", \"但\", \"而\", \"且\",\n",
        "        # Adverbs sometimes mis-tagged\n",
        "        \"也\", \"都\", \"就\", \"才\", \"又\", \"還\", \"还\", \"已\",\n",
        "        \"很\", \"更\", \"最\", \"太\", \"非常\",\n",
        "        # Negation\n",
        "        \"不\", \"沒有\", \"没有\", \"未\", \"無\", \"无\",\n",
        "        # Existential / copular\n",
        "        \"是\", \"有\", \"在\",\n",
        "        # Determiners / quantifiers\n",
        "        \"一\", \"這\", \"这\", \"那\", \"各\", \"每\", \"某\", \"其\",\n",
        "        # Directional / locative words with no sense variation\n",
        "        \"上\", \"下\", \"中\", \"內\", \"内\", \"外\", \"前\", \"後\", \"后\",\n",
        "        \"裡\", \"里\", \"間\", \"间\",\n",
        "        # Common verbs jieba occasionally tags as nouns in CUV\n",
        "        \"說\", \"说\", \"看\", \"去\", \"來\", \"来\", \"到\", \"給\", \"给\",\n",
        "        \"要\", \"會\", \"会\",\n",
        "    }\n",
        "\n",
        "\n",
        "# ─── Frequency Filtering ──────────────────────────────────────────────────────\n",
        "\n",
        "\n",
        "def apply_frequency_filter(df: pd.DataFrame, min_freq: int = MIN_FREQ) -> tuple:\n",
        "    \"\"\"\n",
        "    Keep only lemmas appearing at least `min_freq` times.\n",
        "    Returns (filtered_df, freq_df).\n",
        "    \"\"\"\n",
        "    freq = df.groupby(\"lemma\").size().reset_index(name=\"count\")\n",
        "    freq = freq.sort_values(\"count\", ascending=False)\n",
        "    valid_lemmas = set(freq[freq[\"count\"] >= min_freq][\"lemma\"])\n",
        "    filtered = df[df[\"lemma\"].isin(valid_lemmas)].copy()\n",
        "    return filtered, freq\n",
        "\n",
        "\n",
        "# # ─── Main ─────────────────────────────────────────────────────────────────────\n",
        "\n",
        "\n",
        "# def main():\n",
        "#     print(\"=\" * 60)\n",
        "#     print(\"Step 2: Preprocessing\")\n",
        "#     print(\"=\" * 60)\n",
        "\n",
        "#     # ── English ──────────────────────────────────────────────────\n",
        "#     en_raw = preprocess_english(DATA_DIR / \"english_verses.csv\")\n",
        "#     en_filtered, en_freq = apply_frequency_filter(en_raw)\n",
        "#     en_filtered.to_csv(DATA_DIR / \"english_nouns.csv\", index=False, encoding=\"utf-8\")\n",
        "#     en_freq.to_csv(DATA_DIR / \"english_noun_freq.csv\", index=False, encoding=\"utf-8\")\n",
        "#     print(f\"  [EN] {en_filtered['lemma'].nunique():,} lemmas ≥ {MIN_FREQ} occurrences retained.\")\n",
        "\n",
        "#     # ── Chinese ───────────────────────────────────────────────────\n",
        "#     zh_raw = preprocess_chinese(DATA_DIR / \"chinese_verses.csv\")\n",
        "#     zh_filtered, zh_freq = apply_frequency_filter(zh_raw)\n",
        "#     zh_filtered.to_csv(DATA_DIR / \"chinese_nouns.csv\", index=False, encoding=\"utf-8\")\n",
        "#     zh_freq.to_csv(DATA_DIR / \"chinese_noun_freq.csv\", index=False, encoding=\"utf-8\")\n",
        "#     print(f\"  [ZH] {zh_filtered['lemma'].nunique():,} lemmas ≥ {MIN_FREQ} occurrences retained.\")\n",
        "\n",
        "#     # ── Summary ───────────────────────────────────────────────────\n",
        "#     print(\"\\n── Preprocessing Summary ──\")\n",
        "#     print(f\"  EN noun tokens (filtered) : {len(en_filtered):,}\")\n",
        "#     print(f\"  EN unique lemmas          : {en_filtered['lemma'].nunique():,}\")\n",
        "#     print(f\"  ZH noun tokens (filtered) : {len(zh_filtered):,}\")\n",
        "#     print(f\"  ZH unique lemmas          : {zh_filtered['lemma'].nunique():,}\")\n",
        "\n",
        "#     print(\"\\n  Top 10 English nouns:\")\n",
        "#     print(en_freq.head(10).to_string(index=False))\n",
        "#     print(\"\\n  Top 10 Chinese nouns:\")\n",
        "#     print(zh_freq.head(10).to_string(index=False))\n",
        "\n",
        "#     print(\"\\n✓ Step 2 complete.\\n\")\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()\n"
      ],
      "metadata": {
        "id": "iApWnLav0Ltb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"Step 2: Preprocessing\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ── English ──────────────────────────────────────────────────\n",
        "en_raw = preprocess_english(DATA_DIR / \"english_verses.csv\")\n",
        "en_filtered, en_freq = apply_frequency_filter(en_raw)\n",
        "en_filtered.to_csv(DATA_DIR / \"english_nouns.csv\", index=False, encoding=\"utf-8\")\n",
        "en_freq.to_csv(DATA_DIR / \"english_noun_freq.csv\", index=False, encoding=\"utf-8\")\n",
        "print(f\"  [EN] {en_filtered['lemma'].nunique():,} lemmas ≥ {MIN_FREQ} occurrences retained.\")\n",
        "\n",
        "# ── Chinese ───────────────────────────────────────────────────\n",
        "zh_raw = preprocess_chinese(DATA_DIR / \"chinese_verses.csv\")\n",
        "zh_filtered, zh_freq = apply_frequency_filter(zh_raw)\n",
        "zh_filtered.to_csv(DATA_DIR / \"chinese_nouns.csv\", index=False, encoding=\"utf-8\")\n",
        "zh_freq.to_csv(DATA_DIR / \"chinese_noun_freq.csv\", index=False, encoding=\"utf-8\")\n",
        "print(f\"  [ZH] {zh_filtered['lemma'].nunique():,} lemmas ≥ {MIN_FREQ} occurrences retained.\")\n",
        "\n",
        "# ── Summary ───────────────────────────────────────────────────\n",
        "print(\"\\n── Preprocessing Summary ──\")\n",
        "print(f\"  EN noun tokens (filtered) : {len(en_filtered):,}\")\n",
        "print(f\"  EN unique lemmas          : {en_filtered['lemma'].nunique():,}\")\n",
        "print(f\"  ZH noun tokens (filtered) : {len(zh_filtered):,}\")\n",
        "print(f\"  ZH unique lemmas          : {zh_filtered['lemma'].nunique():,}\")\n",
        "\n",
        "print(\"\\n  Top 10 English nouns:\")\n",
        "print(en_freq.head(10).to_string(index=False))\n",
        "print(\"\\n  Top 10 Chinese nouns:\")\n",
        "print(zh_freq.head(10).to_string(index=False))\n",
        "\n",
        "print(\"\\n✓ Step 2 complete.\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldNNKu9q0emn",
        "outputId": "bffae402-1384-4cb7-db8d-38454d604bd9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Step 2: Preprocessing\n",
            "============================================================\n",
            "  [EN] Loading spaCy model…\n",
            "  [EN] Processing 31,088 verses…\n",
            "    … 31,088/31,088 verses  (542 v/s)  ETA 0.0 min  nouns so far: 116,878\n",
            "  [EN] Done in 1.0 min. Extracted 116,878 noun occurrences.\n",
            "  [EN] 636 lemmas ≥ 30 occurrences retained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jieba/__init__.py:44: SyntaxWarning: invalid escape sequence '\\.'\n",
            "  re_han_default = re.compile(\"([\\u4E00-\\u9FD5a-zA-Z0-9+#&\\._%\\-]+)\", re.U)\n",
            "/usr/local/lib/python3.12/dist-packages/jieba/__init__.py:46: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  re_skip_default = re.compile(\"(\\r\\n|\\s)\", re.U)\n",
            "/usr/local/lib/python3.12/dist-packages/jieba/finalseg/__init__.py:78: SyntaxWarning: invalid escape sequence '\\.'\n",
            "  re_skip = re.compile(\"([a-zA-Z0-9]+(?:\\.\\d+)?%?)\")\n",
            "/usr/local/lib/python3.12/dist-packages/jieba/posseg/__init__.py:16: SyntaxWarning: invalid escape sequence '\\.'\n",
            "  re_skip_detail = re.compile(\"([\\.0-9]+|[a-zA-Z0-9]+)\")\n",
            "/usr/local/lib/python3.12/dist-packages/jieba/posseg/__init__.py:17: SyntaxWarning: invalid escape sequence '\\.'\n",
            "  re_han_internal = re.compile(\"([\\u4E00-\\u9FD5a-zA-Z0-9+#&\\._]+)\")\n",
            "/usr/local/lib/python3.12/dist-packages/jieba/posseg/__init__.py:18: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  re_skip_internal = re.compile(\"(\\r\\n|\\s)\")\n",
            "/usr/local/lib/python3.12/dist-packages/jieba/posseg/__init__.py:21: SyntaxWarning: invalid escape sequence '\\.'\n",
            "  re_num = re.compile(\"[\\.0-9]+\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [ZH] Loaded custom dictionary: jieba_biblical_dict.txt\n",
            "  [ZH] Building jieba model (one-time, ~10-30s)...\n",
            "  [ZH] Model ready.\n",
            "  [ZH] Processing 31,069 verses...\n",
            "    ... 100/31,069 verses  (42 v/s)  ETA 12.4 min  nouns: 359\n",
            "    ... 200/31,069 verses  (46 v/s)  ETA 11.1 min  nouns: 652\n",
            "    ... 300/31,069 verses  (50 v/s)  ETA 10.4 min  nouns: 934\n",
            "    ... 400/31,069 verses  (46 v/s)  ETA 11.2 min  nouns: 1,237\n",
            "    ... 500/31,069 verses  (42 v/s)  ETA 12.1 min  nouns: 1,544\n",
            "    ... 600/31,069 verses  (43 v/s)  ETA 11.9 min  nouns: 1,853\n",
            "    ... 700/31,069 verses  (43 v/s)  ETA 11.8 min  nouns: 2,207\n",
            "    ... 800/31,069 verses  (43 v/s)  ETA 11.6 min  nouns: 2,512\n",
            "    ... 900/31,069 verses  (44 v/s)  ETA 11.4 min  nouns: 2,847\n",
            "    ... 1,000/31,069 verses  (44 v/s)  ETA 11.3 min  nouns: 3,157\n",
            "    ... 1,100/31,069 verses  (45 v/s)  ETA 11.1 min  nouns: 3,487\n",
            "    ... 1,200/31,069 verses  (44 v/s)  ETA 11.2 min  nouns: 3,796\n",
            "    ... 1,300/31,069 verses  (43 v/s)  ETA 11.5 min  nouns: 4,072\n",
            "    ... 1,400/31,069 verses  (42 v/s)  ETA 11.7 min  nouns: 4,382\n",
            "    ... 1,500/31,069 verses  (43 v/s)  ETA 11.6 min  nouns: 4,753\n",
            "    ... 1,600/31,069 verses  (43 v/s)  ETA 11.5 min  nouns: 5,069\n",
            "    ... 1,700/31,069 verses  (43 v/s)  ETA 11.5 min  nouns: 5,352\n",
            "    ... 1,800/31,069 verses  (42 v/s)  ETA 11.5 min  nouns: 5,713\n",
            "    ... 1,900/31,069 verses  (42 v/s)  ETA 11.5 min  nouns: 6,060\n",
            "    ... 2,000/31,069 verses  (42 v/s)  ETA 11.6 min  nouns: 6,354\n",
            "    ... 2,100/31,069 verses  (42 v/s)  ETA 11.6 min  nouns: 6,628\n",
            "    ... 2,200/31,069 verses  (42 v/s)  ETA 11.5 min  nouns: 6,953\n",
            "    ... 2,300/31,069 verses  (42 v/s)  ETA 11.4 min  nouns: 7,428\n",
            "    ... 2,400/31,069 verses  (41 v/s)  ETA 11.6 min  nouns: 7,884\n",
            "    ... 2,500/31,069 verses  (41 v/s)  ETA 11.6 min  nouns: 8,221\n",
            "    ... 2,600/31,069 verses  (41 v/s)  ETA 11.5 min  nouns: 8,644\n",
            "    ... 2,700/31,069 verses  (41 v/s)  ETA 11.5 min  nouns: 9,170\n",
            "    ... 2,800/31,069 verses  (41 v/s)  ETA 11.5 min  nouns: 9,598\n",
            "    ... 2,900/31,069 verses  (41 v/s)  ETA 11.5 min  nouns: 10,041\n",
            "    ... 3,000/31,069 verses  (41 v/s)  ETA 11.5 min  nouns: 10,416\n",
            "    ... 3,100/31,069 verses  (40 v/s)  ETA 11.6 min  nouns: 10,820\n",
            "    ... 3,200/31,069 verses  (40 v/s)  ETA 11.6 min  nouns: 11,259\n",
            "    ... 3,300/31,069 verses  (40 v/s)  ETA 11.6 min  nouns: 11,647\n",
            "    ... 3,400/31,069 verses  (40 v/s)  ETA 11.5 min  nouns: 12,017\n",
            "    ... 3,500/31,069 verses  (40 v/s)  ETA 11.5 min  nouns: 12,321\n",
            "    ... 3,600/31,069 verses  (40 v/s)  ETA 11.6 min  nouns: 12,684\n",
            "    ... 3,700/31,069 verses  (40 v/s)  ETA 11.4 min  nouns: 12,992\n",
            "    ... 3,800/31,069 verses  (40 v/s)  ETA 11.3 min  nouns: 13,373\n",
            "    ... 3,900/31,069 verses  (40 v/s)  ETA 11.3 min  nouns: 13,805\n",
            "    ... 4,000/31,069 verses  (40 v/s)  ETA 11.3 min  nouns: 14,182\n",
            "    ... 4,100/31,069 verses  (40 v/s)  ETA 11.4 min  nouns: 14,488\n",
            "    ... 4,200/31,069 verses  (39 v/s)  ETA 11.4 min  nouns: 14,780\n",
            "    ... 4,300/31,069 verses  (39 v/s)  ETA 11.4 min  nouns: 15,141\n",
            "    ... 4,400/31,069 verses  (39 v/s)  ETA 11.4 min  nouns: 15,513\n",
            "    ... 4,500/31,069 verses  (39 v/s)  ETA 11.4 min  nouns: 15,824\n",
            "    ... 4,600/31,069 verses  (39 v/s)  ETA 11.4 min  nouns: 16,123\n",
            "    ... 4,700/31,069 verses  (39 v/s)  ETA 11.3 min  nouns: 16,518\n",
            "    ... 4,800/31,069 verses  (39 v/s)  ETA 11.3 min  nouns: 16,773\n",
            "    ... 4,900/31,069 verses  (39 v/s)  ETA 11.2 min  nouns: 17,132\n",
            "    ... 5,000/31,069 verses  (39 v/s)  ETA 11.2 min  nouns: 17,449\n",
            "    ... 5,100/31,069 verses  (39 v/s)  ETA 11.2 min  nouns: 17,738\n",
            "    ... 5,200/31,069 verses  (39 v/s)  ETA 11.2 min  nouns: 18,023\n",
            "    ... 5,300/31,069 verses  (38 v/s)  ETA 11.2 min  nouns: 18,322\n",
            "    ... 5,400/31,069 verses  (38 v/s)  ETA 11.1 min  nouns: 18,661\n",
            "    ... 5,500/31,069 verses  (38 v/s)  ETA 11.1 min  nouns: 19,063\n",
            "    ... 5,600/31,069 verses  (38 v/s)  ETA 11.1 min  nouns: 19,395\n",
            "    ... 5,700/31,069 verses  (38 v/s)  ETA 11.0 min  nouns: 19,721\n",
            "    ... 5,800/31,069 verses  (38 v/s)  ETA 11.1 min  nouns: 20,046\n",
            "    ... 5,900/31,069 verses  (38 v/s)  ETA 11.0 min  nouns: 20,380\n",
            "    ... 6,000/31,069 verses  (38 v/s)  ETA 11.0 min  nouns: 20,820\n",
            "    ... 6,100/31,069 verses  (38 v/s)  ETA 11.0 min  nouns: 21,197\n",
            "    ... 6,200/31,069 verses  (38 v/s)  ETA 11.0 min  nouns: 21,587\n",
            "    ... 6,300/31,069 verses  (38 v/s)  ETA 10.9 min  nouns: 21,914\n",
            "    ... 6,400/31,069 verses  (38 v/s)  ETA 10.8 min  nouns: 22,313\n",
            "    ... 6,500/31,069 verses  (38 v/s)  ETA 10.8 min  nouns: 22,765\n",
            "    ... 6,600/31,069 verses  (38 v/s)  ETA 10.8 min  nouns: 23,147\n",
            "    ... 6,700/31,069 verses  (38 v/s)  ETA 10.8 min  nouns: 23,549\n",
            "    ... 6,800/31,069 verses  (38 v/s)  ETA 10.8 min  nouns: 24,000\n",
            "    ... 6,900/31,069 verses  (38 v/s)  ETA 10.7 min  nouns: 24,382\n",
            "    ... 7,000/31,069 verses  (37 v/s)  ETA 10.7 min  nouns: 24,785\n",
            "    ... 7,100/31,069 verses  (37 v/s)  ETA 10.7 min  nouns: 25,206\n",
            "    ... 7,200/31,069 verses  (37 v/s)  ETA 10.7 min  nouns: 25,556\n",
            "    ... 7,300/31,069 verses  (37 v/s)  ETA 10.6 min  nouns: 25,886\n",
            "    ... 7,400/31,069 verses  (37 v/s)  ETA 10.6 min  nouns: 26,271\n",
            "    ... 7,500/31,069 verses  (37 v/s)  ETA 10.6 min  nouns: 26,570\n",
            "    ... 7,600/31,069 verses  (37 v/s)  ETA 10.6 min  nouns: 26,891\n",
            "    ... 7,700/31,069 verses  (37 v/s)  ETA 10.6 min  nouns: 27,235\n",
            "    ... 7,800/31,069 verses  (37 v/s)  ETA 10.5 min  nouns: 27,566\n",
            "    ... 7,900/31,069 verses  (37 v/s)  ETA 10.5 min  nouns: 27,895\n",
            "    ... 8,000/31,069 verses  (37 v/s)  ETA 10.4 min  nouns: 28,225\n",
            "    ... 8,100/31,069 verses  (37 v/s)  ETA 10.4 min  nouns: 28,570\n",
            "    ... 8,200/31,069 verses  (37 v/s)  ETA 10.3 min  nouns: 28,916\n",
            "    ... 8,300/31,069 verses  (37 v/s)  ETA 10.3 min  nouns: 29,285\n",
            "    ... 8,400/31,069 verses  (37 v/s)  ETA 10.3 min  nouns: 29,615\n",
            "    ... 8,500/31,069 verses  (37 v/s)  ETA 10.3 min  nouns: 29,972\n",
            "    ... 8,600/31,069 verses  (37 v/s)  ETA 10.2 min  nouns: 30,385\n",
            "    ... 8,700/31,069 verses  (37 v/s)  ETA 10.2 min  nouns: 30,674\n",
            "    ... 8,800/31,069 verses  (37 v/s)  ETA 10.1 min  nouns: 31,016\n",
            "    ... 8,900/31,069 verses  (37 v/s)  ETA 10.1 min  nouns: 31,372\n",
            "    ... 9,000/31,069 verses  (37 v/s)  ETA 10.0 min  nouns: 31,791\n",
            "    ... 9,100/31,069 verses  (37 v/s)  ETA 10.0 min  nouns: 32,140\n",
            "    ... 9,200/31,069 verses  (37 v/s)  ETA 9.9 min  nouns: 32,539\n",
            "    ... 9,300/31,069 verses  (37 v/s)  ETA 9.9 min  nouns: 32,914\n",
            "    ... 9,400/31,069 verses  (37 v/s)  ETA 9.8 min  nouns: 33,271\n",
            "    ... 9,500/31,069 verses  (37 v/s)  ETA 9.8 min  nouns: 33,619\n",
            "    ... 9,600/31,069 verses  (37 v/s)  ETA 9.7 min  nouns: 33,934\n",
            "    ... 9,700/31,069 verses  (37 v/s)  ETA 9.7 min  nouns: 34,339\n",
            "    ... 9,800/31,069 verses  (37 v/s)  ETA 9.7 min  nouns: 34,748\n",
            "    ... 9,900/31,069 verses  (37 v/s)  ETA 9.7 min  nouns: 35,172\n",
            "    ... 10,000/31,069 verses  (36 v/s)  ETA 9.6 min  nouns: 35,534\n",
            "    ... 10,100/31,069 verses  (36 v/s)  ETA 9.6 min  nouns: 35,872\n",
            "    ... 10,200/31,069 verses  (36 v/s)  ETA 9.5 min  nouns: 36,315\n",
            "    ... 10,300/31,069 verses  (37 v/s)  ETA 9.5 min  nouns: 36,653\n",
            "    ... 10,400/31,069 verses  (37 v/s)  ETA 9.4 min  nouns: 36,975\n",
            "    ... 10,500/31,069 verses  (37 v/s)  ETA 9.3 min  nouns: 37,408\n",
            "    ... 10,600/31,069 verses  (37 v/s)  ETA 9.2 min  nouns: 37,774\n",
            "    ... 10,700/31,069 verses  (37 v/s)  ETA 9.2 min  nouns: 38,182\n",
            "    ... 10,800/31,069 verses  (37 v/s)  ETA 9.1 min  nouns: 38,508\n",
            "    ... 10,900/31,069 verses  (37 v/s)  ETA 9.1 min  nouns: 38,833\n",
            "    ... 11,000/31,069 verses  (37 v/s)  ETA 9.0 min  nouns: 39,241\n",
            "    ... 11,100/31,069 verses  (37 v/s)  ETA 9.0 min  nouns: 39,618\n",
            "    ... 11,200/31,069 verses  (37 v/s)  ETA 8.9 min  nouns: 40,089\n",
            "    ... 11,300/31,069 verses  (37 v/s)  ETA 8.9 min  nouns: 40,533\n",
            "    ... 11,400/31,069 verses  (37 v/s)  ETA 8.8 min  nouns: 40,921\n",
            "    ... 11,500/31,069 verses  (37 v/s)  ETA 8.8 min  nouns: 41,250\n",
            "    ... 11,600/31,069 verses  (37 v/s)  ETA 8.8 min  nouns: 41,550\n",
            "    ... 11,700/31,069 verses  (37 v/s)  ETA 8.7 min  nouns: 42,016\n",
            "    ... 11,800/31,069 verses  (37 v/s)  ETA 8.7 min  nouns: 42,435\n",
            "    ... 11,900/31,069 verses  (37 v/s)  ETA 8.7 min  nouns: 42,828\n",
            "    ... 12,000/31,069 verses  (37 v/s)  ETA 8.6 min  nouns: 43,272\n",
            "    ... 12,100/31,069 verses  (37 v/s)  ETA 8.6 min  nouns: 43,615\n",
            "    ... 12,200/31,069 verses  (37 v/s)  ETA 8.5 min  nouns: 44,053\n",
            "    ... 12,300/31,069 verses  (37 v/s)  ETA 8.5 min  nouns: 44,448\n",
            "    ... 12,400/31,069 verses  (37 v/s)  ETA 8.4 min  nouns: 44,899\n",
            "    ... 12,500/31,069 verses  (37 v/s)  ETA 8.4 min  nouns: 45,241\n",
            "    ... 12,600/31,069 verses  (37 v/s)  ETA 8.3 min  nouns: 45,648\n",
            "    ... 12,700/31,069 verses  (37 v/s)  ETA 8.3 min  nouns: 46,031\n",
            "    ... 12,800/31,069 verses  (37 v/s)  ETA 8.2 min  nouns: 46,404\n",
            "    ... 12,900/31,069 verses  (37 v/s)  ETA 8.2 min  nouns: 46,814\n",
            "    ... 13,000/31,069 verses  (37 v/s)  ETA 8.1 min  nouns: 47,047\n",
            "    ... 13,100/31,069 verses  (37 v/s)  ETA 8.1 min  nouns: 47,250\n",
            "    ... 13,200/31,069 verses  (37 v/s)  ETA 8.0 min  nouns: 47,467\n",
            "    ... 13,300/31,069 verses  (37 v/s)  ETA 7.9 min  nouns: 47,682\n",
            "    ... 13,400/31,069 verses  (37 v/s)  ETA 7.9 min  nouns: 47,896\n",
            "    ... 13,500/31,069 verses  (38 v/s)  ETA 7.8 min  nouns: 48,141\n",
            "    ... 13,600/31,069 verses  (38 v/s)  ETA 7.7 min  nouns: 48,374\n",
            "    ... 13,700/31,069 verses  (38 v/s)  ETA 7.7 min  nouns: 48,587\n",
            "    ... 13,800/31,069 verses  (38 v/s)  ETA 7.6 min  nouns: 48,804\n",
            "    ... 13,900/31,069 verses  (38 v/s)  ETA 7.5 min  nouns: 49,046\n",
            "    ... 14,000/31,069 verses  (38 v/s)  ETA 7.5 min  nouns: 49,300\n",
            "    ... 14,100/31,069 verses  (38 v/s)  ETA 7.4 min  nouns: 49,536\n",
            "    ... 14,200/31,069 verses  (38 v/s)  ETA 7.4 min  nouns: 49,759\n",
            "    ... 14,300/31,069 verses  (38 v/s)  ETA 7.3 min  nouns: 49,981\n",
            "    ... 14,400/31,069 verses  (38 v/s)  ETA 7.3 min  nouns: 50,190\n",
            "    ... 14,500/31,069 verses  (38 v/s)  ETA 7.2 min  nouns: 50,428\n",
            "    ... 14,600/31,069 verses  (38 v/s)  ETA 7.2 min  nouns: 50,671\n",
            "    ... 14,700/31,069 verses  (38 v/s)  ETA 7.1 min  nouns: 50,889\n",
            "    ... 14,800/31,069 verses  (38 v/s)  ETA 7.1 min  nouns: 51,124\n",
            "    ... 14,900/31,069 verses  (38 v/s)  ETA 7.0 min  nouns: 51,367\n",
            "    ... 15,000/31,069 verses  (39 v/s)  ETA 6.9 min  nouns: 51,605\n",
            "    ... 15,100/31,069 verses  (39 v/s)  ETA 6.9 min  nouns: 51,835\n",
            "    ... 15,200/31,069 verses  (39 v/s)  ETA 6.8 min  nouns: 52,069\n",
            "    ... 15,300/31,069 verses  (39 v/s)  ETA 6.8 min  nouns: 52,276\n",
            "    ... 15,400/31,069 verses  (39 v/s)  ETA 6.7 min  nouns: 52,477\n",
            "    ... 15,500/31,069 verses  (39 v/s)  ETA 6.7 min  nouns: 52,703\n",
            "    ... 15,600/31,069 verses  (39 v/s)  ETA 6.6 min  nouns: 52,944\n",
            "    ... 15,700/31,069 verses  (39 v/s)  ETA 6.6 min  nouns: 53,155\n",
            "    ... 15,800/31,069 verses  (39 v/s)  ETA 6.5 min  nouns: 53,375\n",
            "    ... 15,900/31,069 verses  (39 v/s)  ETA 6.5 min  nouns: 53,528\n",
            "    ... 16,000/31,069 verses  (39 v/s)  ETA 6.4 min  nouns: 53,734\n",
            "    ... 16,100/31,069 verses  (39 v/s)  ETA 6.3 min  nouns: 53,918\n",
            "    ... 16,200/31,069 verses  (39 v/s)  ETA 6.3 min  nouns: 54,125\n",
            "    ... 16,300/31,069 verses  (39 v/s)  ETA 6.2 min  nouns: 54,346\n",
            "    ... 16,400/31,069 verses  (40 v/s)  ETA 6.2 min  nouns: 54,577\n",
            "    ... 16,500/31,069 verses  (40 v/s)  ETA 6.1 min  nouns: 54,757\n",
            "    ... 16,600/31,069 verses  (40 v/s)  ETA 6.1 min  nouns: 54,996\n",
            "    ... 16,700/31,069 verses  (40 v/s)  ETA 6.0 min  nouns: 55,244\n",
            "    ... 16,800/31,069 verses  (40 v/s)  ETA 6.0 min  nouns: 55,543\n",
            "    ... 16,900/31,069 verses  (40 v/s)  ETA 5.9 min  nouns: 55,826\n",
            "    ... 17,000/31,069 verses  (40 v/s)  ETA 5.9 min  nouns: 56,093\n",
            "    ... 17,100/31,069 verses  (40 v/s)  ETA 5.8 min  nouns: 56,327\n",
            "    ... 17,200/31,069 verses  (40 v/s)  ETA 5.8 min  nouns: 56,639\n",
            "    ... 17,300/31,069 verses  (40 v/s)  ETA 5.7 min  nouns: 56,922\n",
            "    ... 17,400/31,069 verses  (40 v/s)  ETA 5.7 min  nouns: 57,220\n",
            "    ... 17,500/31,069 verses  (40 v/s)  ETA 5.6 min  nouns: 57,585\n",
            "    ... 17,600/31,069 verses  (40 v/s)  ETA 5.6 min  nouns: 58,015\n",
            "    ... 17,700/31,069 verses  (40 v/s)  ETA 5.6 min  nouns: 58,409\n",
            "    ... 17,800/31,069 verses  (40 v/s)  ETA 5.5 min  nouns: 58,826\n",
            "    ... 17,900/31,069 verses  (40 v/s)  ETA 5.5 min  nouns: 59,208\n",
            "    ... 18,000/31,069 verses  (40 v/s)  ETA 5.4 min  nouns: 59,622\n",
            "    ... 18,100/31,069 verses  (40 v/s)  ETA 5.4 min  nouns: 59,985\n",
            "    ... 18,200/31,069 verses  (40 v/s)  ETA 5.4 min  nouns: 60,368\n",
            "    ... 18,300/31,069 verses  (40 v/s)  ETA 5.3 min  nouns: 60,793\n",
            "    ... 18,400/31,069 verses  (40 v/s)  ETA 5.3 min  nouns: 61,141\n",
            "    ... 18,500/31,069 verses  (40 v/s)  ETA 5.3 min  nouns: 61,461\n",
            "    ... 18,600/31,069 verses  (40 v/s)  ETA 5.2 min  nouns: 61,783\n",
            "    ... 18,700/31,069 verses  (40 v/s)  ETA 5.2 min  nouns: 62,102\n",
            "    ... 18,800/31,069 verses  (40 v/s)  ETA 5.2 min  nouns: 62,496\n",
            "    ... 18,900/31,069 verses  (39 v/s)  ETA 5.1 min  nouns: 62,872\n",
            "    ... 19,000/31,069 verses  (39 v/s)  ETA 5.1 min  nouns: 63,212\n",
            "    ... 19,100/31,069 verses  (39 v/s)  ETA 5.1 min  nouns: 63,593\n",
            "    ... 19,200/31,069 verses  (39 v/s)  ETA 5.0 min  nouns: 63,971\n",
            "    ... 19,300/31,069 verses  (39 v/s)  ETA 5.0 min  nouns: 64,324\n",
            "    ... 19,400/31,069 verses  (39 v/s)  ETA 5.0 min  nouns: 64,680\n",
            "    ... 19,500/31,069 verses  (39 v/s)  ETA 5.0 min  nouns: 65,084\n",
            "    ... 19,600/31,069 verses  (39 v/s)  ETA 4.9 min  nouns: 65,432\n",
            "    ... 19,700/31,069 verses  (39 v/s)  ETA 4.9 min  nouns: 65,751\n",
            "    ... 19,800/31,069 verses  (39 v/s)  ETA 4.8 min  nouns: 66,126\n",
            "    ... 19,900/31,069 verses  (39 v/s)  ETA 4.8 min  nouns: 66,537\n",
            "    ... 20,000/31,069 verses  (39 v/s)  ETA 4.8 min  nouns: 66,946\n",
            "    ... 20,100/31,069 verses  (39 v/s)  ETA 4.7 min  nouns: 67,272\n",
            "    ... 20,200/31,069 verses  (38 v/s)  ETA 4.7 min  nouns: 67,661\n",
            "    ... 20,300/31,069 verses  (38 v/s)  ETA 4.7 min  nouns: 68,009\n",
            "    ... 20,400/31,069 verses  (38 v/s)  ETA 4.6 min  nouns: 68,310\n",
            "    ... 20,500/31,069 verses  (38 v/s)  ETA 4.6 min  nouns: 68,678\n",
            "    ... 20,600/31,069 verses  (38 v/s)  ETA 4.5 min  nouns: 69,069\n",
            "    ... 20,700/31,069 verses  (38 v/s)  ETA 4.5 min  nouns: 69,443\n",
            "    ... 20,800/31,069 verses  (38 v/s)  ETA 4.5 min  nouns: 69,827\n",
            "    ... 20,900/31,069 verses  (38 v/s)  ETA 4.4 min  nouns: 70,200\n",
            "    ... 21,000/31,069 verses  (38 v/s)  ETA 4.4 min  nouns: 70,557\n",
            "    ... 21,100/31,069 verses  (38 v/s)  ETA 4.3 min  nouns: 70,936\n",
            "    ... 21,200/31,069 verses  (38 v/s)  ETA 4.3 min  nouns: 71,324\n",
            "    ... 21,300/31,069 verses  (38 v/s)  ETA 4.3 min  nouns: 71,746\n",
            "    ... 21,400/31,069 verses  (38 v/s)  ETA 4.2 min  nouns: 72,101\n",
            "    ... 21,500/31,069 verses  (38 v/s)  ETA 4.2 min  nouns: 72,593\n",
            "    ... 21,600/31,069 verses  (38 v/s)  ETA 4.1 min  nouns: 73,088\n",
            "    ... 21,700/31,069 verses  (38 v/s)  ETA 4.1 min  nouns: 73,555\n",
            "    ... 21,800/31,069 verses  (38 v/s)  ETA 4.1 min  nouns: 73,940\n",
            "    ... 21,900/31,069 verses  (38 v/s)  ETA 4.0 min  nouns: 74,380\n",
            "    ... 22,000/31,069 verses  (38 v/s)  ETA 4.0 min  nouns: 74,798\n",
            "    ... 22,100/31,069 verses  (38 v/s)  ETA 3.9 min  nouns: 75,150\n",
            "    ... 22,200/31,069 verses  (38 v/s)  ETA 3.9 min  nouns: 75,462\n",
            "    ... 22,300/31,069 verses  (38 v/s)  ETA 3.9 min  nouns: 75,838\n",
            "    ... 22,400/31,069 verses  (38 v/s)  ETA 3.8 min  nouns: 76,204\n",
            "    ... 22,500/31,069 verses  (38 v/s)  ETA 3.8 min  nouns: 76,555\n",
            "    ... 22,600/31,069 verses  (38 v/s)  ETA 3.7 min  nouns: 76,902\n",
            "    ... 22,700/31,069 verses  (38 v/s)  ETA 3.7 min  nouns: 77,293\n",
            "    ... 22,800/31,069 verses  (38 v/s)  ETA 3.7 min  nouns: 77,689\n",
            "    ... 22,900/31,069 verses  (38 v/s)  ETA 3.6 min  nouns: 77,997\n",
            "    ... 23,000/31,069 verses  (38 v/s)  ETA 3.6 min  nouns: 78,324\n",
            "    ... 23,100/31,069 verses  (38 v/s)  ETA 3.5 min  nouns: 78,671\n",
            "    ... 23,200/31,069 verses  (38 v/s)  ETA 3.5 min  nouns: 78,979\n",
            "    ... 23,300/31,069 verses  (38 v/s)  ETA 3.4 min  nouns: 79,281\n",
            "    ... 23,400/31,069 verses  (38 v/s)  ETA 3.4 min  nouns: 79,559\n",
            "    ... 23,500/31,069 verses  (38 v/s)  ETA 3.4 min  nouns: 79,881\n",
            "    ... 23,600/31,069 verses  (38 v/s)  ETA 3.3 min  nouns: 80,163\n",
            "    ... 23,700/31,069 verses  (38 v/s)  ETA 3.3 min  nouns: 80,479\n",
            "    ... 23,800/31,069 verses  (38 v/s)  ETA 3.2 min  nouns: 80,735\n",
            "    ... 23,900/31,069 verses  (38 v/s)  ETA 3.2 min  nouns: 81,009\n",
            "    ... 24,000/31,069 verses  (38 v/s)  ETA 3.1 min  nouns: 81,319\n",
            "    ... 24,100/31,069 verses  (38 v/s)  ETA 3.1 min  nouns: 81,567\n",
            "    ... 24,200/31,069 verses  (38 v/s)  ETA 3.0 min  nouns: 81,827\n",
            "    ... 24,300/31,069 verses  (38 v/s)  ETA 3.0 min  nouns: 82,090\n",
            "    ... 24,400/31,069 verses  (38 v/s)  ETA 3.0 min  nouns: 82,335\n",
            "    ... 24,500/31,069 verses  (38 v/s)  ETA 2.9 min  nouns: 82,715\n",
            "    ... 24,600/31,069 verses  (38 v/s)  ETA 2.9 min  nouns: 82,989\n",
            "    ... 24,700/31,069 verses  (38 v/s)  ETA 2.8 min  nouns: 83,248\n",
            "    ... 24,800/31,069 verses  (38 v/s)  ETA 2.8 min  nouns: 83,503\n",
            "    ... 24,900/31,069 verses  (38 v/s)  ETA 2.7 min  nouns: 83,761\n",
            "    ... 25,000/31,069 verses  (38 v/s)  ETA 2.7 min  nouns: 84,015\n",
            "    ... 25,100/31,069 verses  (38 v/s)  ETA 2.6 min  nouns: 84,346\n",
            "    ... 25,200/31,069 verses  (38 v/s)  ETA 2.6 min  nouns: 84,664\n",
            "    ... 25,300/31,069 verses  (38 v/s)  ETA 2.6 min  nouns: 84,974\n",
            "    ... 25,400/31,069 verses  (38 v/s)  ETA 2.5 min  nouns: 85,240\n",
            "    ... 25,500/31,069 verses  (38 v/s)  ETA 2.5 min  nouns: 85,568\n",
            "    ... 25,600/31,069 verses  (38 v/s)  ETA 2.4 min  nouns: 85,835\n",
            "    ... 25,700/31,069 verses  (38 v/s)  ETA 2.4 min  nouns: 86,094\n",
            "    ... 25,800/31,069 verses  (38 v/s)  ETA 2.3 min  nouns: 86,333\n",
            "    ... 25,900/31,069 verses  (38 v/s)  ETA 2.3 min  nouns: 86,587\n",
            "    ... 26,000/31,069 verses  (38 v/s)  ETA 2.2 min  nouns: 86,818\n",
            "    ... 26,100/31,069 verses  (38 v/s)  ETA 2.2 min  nouns: 87,027\n",
            "    ... 26,200/31,069 verses  (38 v/s)  ETA 2.2 min  nouns: 87,270\n",
            "    ... 26,300/31,069 verses  (38 v/s)  ETA 2.1 min  nouns: 87,516\n",
            "    ... 26,400/31,069 verses  (38 v/s)  ETA 2.1 min  nouns: 87,723\n",
            "    ... 26,500/31,069 verses  (38 v/s)  ETA 2.0 min  nouns: 87,954\n",
            "    ... 26,600/31,069 verses  (38 v/s)  ETA 2.0 min  nouns: 88,158\n",
            "    ... 26,700/31,069 verses  (38 v/s)  ETA 1.9 min  nouns: 88,397\n",
            "    ... 26,800/31,069 verses  (38 v/s)  ETA 1.9 min  nouns: 88,633\n",
            "    ... 26,900/31,069 verses  (38 v/s)  ETA 1.8 min  nouns: 88,933\n",
            "    ... 27,000/31,069 verses  (38 v/s)  ETA 1.8 min  nouns: 89,210\n",
            "    ... 27,100/31,069 verses  (38 v/s)  ETA 1.8 min  nouns: 89,528\n",
            "    ... 27,200/31,069 verses  (38 v/s)  ETA 1.7 min  nouns: 89,788\n",
            "    ... 27,300/31,069 verses  (38 v/s)  ETA 1.7 min  nouns: 90,043\n",
            "    ... 27,400/31,069 verses  (38 v/s)  ETA 1.6 min  nouns: 90,321\n",
            "    ... 27,500/31,069 verses  (38 v/s)  ETA 1.6 min  nouns: 90,612\n",
            "    ... 27,600/31,069 verses  (38 v/s)  ETA 1.5 min  nouns: 90,898\n",
            "    ... 27,700/31,069 verses  (38 v/s)  ETA 1.5 min  nouns: 91,157\n",
            "    ... 27,800/31,069 verses  (38 v/s)  ETA 1.4 min  nouns: 91,424\n",
            "    ... 27,900/31,069 verses  (38 v/s)  ETA 1.4 min  nouns: 91,710\n",
            "    ... 28,000/31,069 verses  (38 v/s)  ETA 1.4 min  nouns: 92,026\n",
            "    ... 28,100/31,069 verses  (38 v/s)  ETA 1.3 min  nouns: 92,345\n",
            "    ... 28,200/31,069 verses  (38 v/s)  ETA 1.3 min  nouns: 92,598\n",
            "    ... 28,300/31,069 verses  (38 v/s)  ETA 1.2 min  nouns: 92,845\n",
            "    ... 28,400/31,069 verses  (38 v/s)  ETA 1.2 min  nouns: 93,107\n",
            "    ... 28,500/31,069 verses  (38 v/s)  ETA 1.1 min  nouns: 93,427\n",
            "    ... 28,600/31,069 verses  (38 v/s)  ETA 1.1 min  nouns: 93,738\n",
            "    ... 28,700/31,069 verses  (38 v/s)  ETA 1.0 min  nouns: 94,052\n",
            "    ... 28,800/31,069 verses  (38 v/s)  ETA 1.0 min  nouns: 94,268\n",
            "    ... 28,900/31,069 verses  (38 v/s)  ETA 1.0 min  nouns: 94,484\n",
            "    ... 29,000/31,069 verses  (38 v/s)  ETA 0.9 min  nouns: 94,731\n",
            "    ... 29,100/31,069 verses  (38 v/s)  ETA 0.9 min  nouns: 95,024\n",
            "    ... 29,200/31,069 verses  (38 v/s)  ETA 0.8 min  nouns: 95,299\n",
            "    ... 29,300/31,069 verses  (38 v/s)  ETA 0.8 min  nouns: 95,572\n",
            "    ... 29,400/31,069 verses  (38 v/s)  ETA 0.7 min  nouns: 95,830\n",
            "    ... 29,500/31,069 verses  (38 v/s)  ETA 0.7 min  nouns: 96,125\n",
            "    ... 29,600/31,069 verses  (38 v/s)  ETA 0.6 min  nouns: 96,383\n",
            "    ... 29,700/31,069 verses  (38 v/s)  ETA 0.6 min  nouns: 96,653\n",
            "    ... 29,800/31,069 verses  (38 v/s)  ETA 0.6 min  nouns: 96,954\n",
            "    ... 29,900/31,069 verses  (38 v/s)  ETA 0.5 min  nouns: 97,261\n",
            "    ... 30,000/31,069 verses  (38 v/s)  ETA 0.5 min  nouns: 97,515\n",
            "    ... 30,100/31,069 verses  (38 v/s)  ETA 0.4 min  nouns: 97,826\n",
            "    ... 30,200/31,069 verses  (38 v/s)  ETA 0.4 min  nouns: 98,126\n",
            "    ... 30,300/31,069 verses  (38 v/s)  ETA 0.3 min  nouns: 98,424\n",
            "    ... 30,400/31,069 verses  (38 v/s)  ETA 0.3 min  nouns: 98,712\n",
            "    ... 30,500/31,069 verses  (38 v/s)  ETA 0.3 min  nouns: 99,036\n",
            "    ... 30,600/31,069 verses  (38 v/s)  ETA 0.2 min  nouns: 99,298\n",
            "    ... 30,700/31,069 verses  (38 v/s)  ETA 0.2 min  nouns: 99,641\n",
            "    ... 30,800/31,069 verses  (38 v/s)  ETA 0.1 min  nouns: 100,025\n",
            "    ... 30,900/31,069 verses  (38 v/s)  ETA 0.1 min  nouns: 100,509\n",
            "    ... 31,000/31,069 verses  (38 v/s)  ETA 0.0 min  nouns: 100,959\n",
            "    ... 31,069/31,069 verses  (38 v/s)  ETA 0.0 min  nouns: 101,243\n",
            "  [ZH] Done in 13.8 min. Extracted 101,243 noun occurrences.\n",
            "  [ZH] 572 lemmas ≥ 30 occurrences retained.\n",
            "\n",
            "── Preprocessing Summary ──\n",
            "  EN noun tokens (filtered) : 98,195\n",
            "  EN unique lemmas          : 636\n",
            "  ZH noun tokens (filtered) : 72,998\n",
            "  ZH unique lemmas          : 572\n",
            "\n",
            "  Top 10 English nouns:\n",
            "   lemma  count\n",
            "     man   3927\n",
            "     son   2929\n",
            "    king   2533\n",
            "  people   2376\n",
            "     day   2037\n",
            "    land   1501\n",
            "    hand   1296\n",
            "  father   1289\n",
            "   house   1054\n",
            "offering   1026\n",
            "\n",
            "  Top 10 Chinese nouns:\n",
            "lemma  count\n",
            "    人   9184\n",
            "   兒子   2356\n",
            "    事   1562\n",
            "    話   1496\n",
            "    地   1373\n",
            "   時候   1339\n",
            "   祭司    855\n",
            "   百姓    827\n",
            "   弟兄    695\n",
            "    城    688\n",
            "\n",
            "✓ Step 2 complete.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Extract Context Embeddings"
      ],
      "metadata": {
        "id": "1CZa8ZJVWMeN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch transformers numpy pandas --break-system-packages"
      ],
      "metadata": {
        "id": "5F743oiHY2rW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from pathlib import Path\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from typing import List, Tuple\n",
        "\n",
        "# ─── Configuration ────────────────────────────────────────────────────────────\n",
        "\n",
        "DATA_DIR    = Path(\"/content\") / \"bible_data\"\n",
        "MODEL_NAME  = \"xlm-roberta-base\"   # Multilingual; same model for EN and ZH\n",
        "BATCH_SIZE  = 32                   # Reduce to 8-16 if OOM on CPU\n",
        "LAYERS      = [-1, -2, -3, -4]     # Last 4 layers averaged (standard WSI practice)\n",
        "DEVICE      = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "MAX_SEQ_LEN = 128                  # Max subword tokens per sentence\n",
        "\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# ─── Model Loading ────────────────────────────────────────────────────────────\n",
        "\n",
        "def load_model():\n",
        "    print(f\"  [model] Loading {MODEL_NAME}…\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "    model     = AutoModel.from_pretrained(MODEL_NAME, output_hidden_states=True)\n",
        "    model.to(DEVICE)\n",
        "    model.eval()\n",
        "    return tokenizer, model\n",
        "\n",
        "\n",
        "# ─── Embedding Extraction ─────────────────────────────────────────────────────\n",
        "\n",
        "def get_target_embedding(\n",
        "    tokenizer,\n",
        "    model,\n",
        "    sentences:  List[str],\n",
        "    target_words: List[str],\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    For each (sentence, target_word) pair, extract the contextual embedding\n",
        "    of the target by:\n",
        "      1. Tokenizing the sentence\n",
        "      2. Finding subword token positions for the target word\n",
        "      3. Averaging hidden states across the last 4 layers at those positions\n",
        "      4. Mean-pooling across subwords for multi-token targets\n",
        "\n",
        "    Returns: np.ndarray of shape (N, hidden_dim)\n",
        "    \"\"\"\n",
        "    all_embeddings = []\n",
        "\n",
        "    for i in range(0, len(sentences), BATCH_SIZE):\n",
        "        batch_sents  = sentences[i : i + BATCH_SIZE]\n",
        "        batch_targets = target_words[i : i + BATCH_SIZE]\n",
        "\n",
        "        encoded = tokenizer(\n",
        "            batch_sents,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=MAX_SEQ_LEN,\n",
        "            return_offsets_mapping=True,\n",
        "        )\n",
        "        offset_mappings = encoded.pop(\"offset_mapping\")  # not passed to model\n",
        "\n",
        "        encoded = {k: v.to(DEVICE) for k, v in encoded.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**encoded)\n",
        "\n",
        "        # Stack selected hidden layers: shape (n_layers, batch, seq_len, hidden)\n",
        "        hidden_states = torch.stack(\n",
        "            [outputs.hidden_states[l] for l in LAYERS], dim=0\n",
        "        )\n",
        "        # Mean over selected layers: (batch, seq_len, hidden)\n",
        "        layer_mean = hidden_states.mean(dim=0).cpu().numpy()\n",
        "\n",
        "        input_ids = encoded[\"input_ids\"].cpu().numpy()\n",
        "\n",
        "        for j, (target, offsets_j) in enumerate(zip(batch_targets, offset_mappings)):\n",
        "            # Re-encode the target word alone to find its subword tokens\n",
        "            target_enc = tokenizer.encode(\n",
        "                target, add_special_tokens=False\n",
        "            )\n",
        "            # Find target subword positions in the sentence encoding\n",
        "            target_positions = _find_subword_positions(\n",
        "                input_ids[j].tolist(), target_enc\n",
        "            )\n",
        "            if target_positions:\n",
        "                token_emb = layer_mean[j][target_positions].mean(axis=0)\n",
        "            else:\n",
        "                # Fallback: mean-pool entire sequence (excluding [CLS]/[SEP])\n",
        "                seq_len = (input_ids[j] != tokenizer.pad_token_id).sum()\n",
        "                token_emb = layer_mean[j][1 : seq_len - 1].mean(axis=0)\n",
        "\n",
        "            all_embeddings.append(token_emb)\n",
        "\n",
        "        if (i // BATCH_SIZE) % 10 == 0:\n",
        "            print(f\"    … batch {i//BATCH_SIZE} / {len(sentences)//BATCH_SIZE}\", end=\"\\r\")\n",
        "\n",
        "    embeddings = np.array(all_embeddings, dtype=np.float32)\n",
        "    # L2 normalize for cosine-based clustering\n",
        "    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
        "    norms = np.where(norms == 0, 1, norms)\n",
        "    return embeddings / norms\n",
        "\n",
        "\n",
        "def _find_subword_positions(\n",
        "    sentence_ids: List[int], target_ids: List[int]\n",
        ") -> List[int]:\n",
        "    \"\"\"Find the start position of `target_ids` as a subsequence in `sentence_ids`.\"\"\"\n",
        "    n, m = len(sentence_ids), len(target_ids)\n",
        "    for start in range(n - m + 1):\n",
        "        if sentence_ids[start : start + m] == target_ids:\n",
        "            return list(range(start, start + m))\n",
        "    return []\n",
        "\n",
        "\n",
        "# ─── Per-language Pipeline ────────────────────────────────────────────────────\n",
        "\n",
        "def extract_embeddings_for_language(\n",
        "    lang: str,\n",
        "    noun_csv: Path,\n",
        "    tokenizer,\n",
        "    model,\n",
        ") -> None:\n",
        "    \"\"\"Load nouns, extract embeddings, and save as .npz.\"\"\"\n",
        "    out_path = DATA_DIR / f\"{lang}_embeddings.npz\"\n",
        "    if out_path.exists():\n",
        "        print(f\"  [{lang.upper()}] Embeddings already exist — skipping.\")\n",
        "        return\n",
        "\n",
        "    df = pd.read_csv(noun_csv)\n",
        "    print(f\"  [{lang.upper()}] Extracting embeddings for {len(df):,} noun occurrences…\")\n",
        "\n",
        "    embeddings = get_target_embedding(\n",
        "        tokenizer,\n",
        "        model,\n",
        "        sentences    = df[\"context\"].tolist(),\n",
        "        target_words = df[\"token\"].tolist(),\n",
        "    )\n",
        "    print()  # newline after progress indicator\n",
        "\n",
        "    np.savez_compressed(\n",
        "        out_path,\n",
        "        embeddings = embeddings,\n",
        "        lemmas     = df[\"lemma\"].to_numpy(dtype=str),\n",
        "        verse_ids  = df[\"verse_id\"].to_numpy(dtype=str),\n",
        "        tokens     = df[\"token\"].to_numpy(dtype=str),\n",
        "    )\n",
        "    print(f\"  [{lang.upper()}] Saved {embeddings.shape} embeddings → {out_path.name}\")\n",
        "\n",
        "\n",
        "# ─── Main ─────────────────────────────────────────────────────────────────────\n",
        "\n",
        "# def main():\n",
        "#     print(\"=\" * 60)\n",
        "#     print(\"Step 3: Contextual Embedding Extraction (XLM-R)\")\n",
        "#     print(\"=\" * 60)\n",
        "\n",
        "#     tokenizer, model = load_model()\n",
        "\n",
        "#     extract_embeddings_for_language(\n",
        "#         \"english\",\n",
        "#         DATA_DIR / \"english_nouns.csv\",\n",
        "#         tokenizer, model,\n",
        "#     )\n",
        "#     extract_embeddings_for_language(\n",
        "#         \"chinese\",\n",
        "#         DATA_DIR / \"chinese_nouns.csv\",\n",
        "#         tokenizer, model,\n",
        "#     )\n",
        "\n",
        "#     print(\"\\n✓ Step 3 complete.\\n\")\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJF9kXsnWQDa",
        "outputId": "5f48b1d9-c8c1-4b88-df0f-c3060116a981"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"Step 3: Contextual Embedding Extraction (XLM-R)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "tokenizer, model = load_model()\n",
        "\n",
        "extract_embeddings_for_language(\n",
        "    \"english\",\n",
        "    DATA_DIR / \"english_nouns.csv\",\n",
        "    tokenizer, model,\n",
        ")\n",
        "extract_embeddings_for_language(\n",
        "    \"chinese\",\n",
        "    DATA_DIR / \"chinese_nouns.csv\",\n",
        "    tokenizer, model,\n",
        ")\n",
        "\n",
        "print(\"\\n✓ Step 3 complete.\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426,
          "referenced_widgets": [
            "d49ff9dcc67549d0b8bd58f6102a6261",
            "cc35e34a34de4c7890ed5240ab98a306",
            "67fddc51749445a7b7c435ba71963658",
            "de19b72f4810489fac9cf5a4eb956843",
            "4b616585cba948a7afc3bcc096a6c9f6",
            "a4c8580f4b9142bb90ae8b1333518b93",
            "7e0f573c3a7f4fc4a1fc375f7908d0e9",
            "1a91b92710774db6aedce4488a4cefcb",
            "f323edfb8b1e4087b4d19a103977b8bf",
            "d80c8ef594684cf889582699515f78cf",
            "2e951d7208ba4b53b69486269030d878"
          ]
        },
        "id": "ZwpfEQinX-ri",
        "outputId": "a406ae00-d7b3-47e1-a7b5-09c2beeddb67"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Step 3: Contextual Embedding Extraction (XLM-R)\n",
            "============================================================\n",
            "  [model] Loading xlm-roberta-base…\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d49ff9dcc67549d0b8bd58f6102a6261"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "XLMRobertaModel LOAD REPORT from: xlm-roberta-base\n",
            "Key                       | Status     |  | \n",
            "--------------------------+------------+--+-\n",
            "lm_head.dense.weight      | UNEXPECTED |  | \n",
            "lm_head.layer_norm.bias   | UNEXPECTED |  | \n",
            "lm_head.bias              | UNEXPECTED |  | \n",
            "lm_head.layer_norm.weight | UNEXPECTED |  | \n",
            "lm_head.dense.bias        | UNEXPECTED |  | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [ENGLISH] Embeddings already exist — skipping.\n",
            "  [CHINESE] Embeddings already exist — skipping.\n",
            "\n",
            "✓ Step 3 complete.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: WSI Clustering"
      ],
      "metadata": {
        "id": "meHGlTr8aErA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn numpy pandas --break-system-packages"
      ],
      "metadata": {
        "id": "8Igdh2ErazoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Step 4: Word Sense Induction (WSI) via Agglomerative Clustering\n",
        "================================================================\n",
        "For each lemma in each language, clusters its contextual embeddings\n",
        "to induce word senses. The number of clusters k is determined\n",
        "automatically using the Silhouette Score (range 2–8) or set to 1\n",
        "if the word shows insufficient sense variation.\n",
        "\n",
        "Two clustering algorithms are run for robustness comparison:\n",
        "  (A) Agglomerative Hierarchical Clustering (Ward linkage)    — primary\n",
        "  (B) K-Means with k++ initialization                         — secondary\n",
        "\n",
        "Outputs:\n",
        "  - data/english_wsi_results.csv  : lemma, k_ward, k_kmeans, silhouette_ward, ...\n",
        "  - data/chinese_wsi_results.csv  : same\n",
        "  - data/english_sense_labels.csv : lemma, verse_id, cluster_ward, cluster_kmeans\n",
        "  - data/chinese_sense_labels.csv : same\n",
        "\n",
        "Usage:\n",
        "  pip install scikit-learn numpy pandas --break-system-packages\n",
        "  python 04_wsi_clustering.py\n",
        "\n",
        "Design decisions (paper §3.3):\n",
        "  - Ward linkage is preferred for lexical WSI (Ustalov et al. 2019)\n",
        "  - k range: 1–8 senses; beyond 8 is linguistically implausible for\n",
        "    the narrow domain of biblical text\n",
        "  - Silhouette threshold: if best_silhouette < SILHOUETTE_THRESHOLD,\n",
        "    k is set to 1 (monosemous)\n",
        "  - UMAP dimensionality reduction to 50D before clustering improves\n",
        "    silhouette stability (McInnes et al. 2018)\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.preprocessing import normalize\n",
        "from typing import Tuple, Dict\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "# ─── Configuration ────────────────────────────────────────────────────────────\n",
        "\n",
        "DATA_DIR    = Path(\"/content\") / \"bible_data\"\n",
        "# DATA_DIR             = Path(__file__).parent.parent / \"data\"\n",
        "K_RANGE              = range(2, 9)           # Test k = 2, 3, …, 8\n",
        "SILHOUETTE_THRESHOLD = 0.30                  # Below this → monosemous (k=1)\n",
        "# NOTE: 0.05 caused 0% monosemy — XLM-R embeddings always score > 0.05\n",
        "# even for genuinely monosemous words. 0.30 is the literature standard\n",
        "# for meaningful cluster structure (Ustalov et al. 2019; Neelakantan et al. 2014).\n",
        "# Run sensitivity check at SILHOUETTE_THRESHOLD = 0.20 and report both.\n",
        "MIN_INSTANCES        = 60                    # Min occurrences to attempt clustering\n",
        "# Raised from 5 to 60 to ensure UMAP dimensionality reduction works reliably.\n",
        "# UMAP requires n_samples > n_components (60 > 50). Words with 30-59 occurrences\n",
        "# are excluded from clustering as they lack sufficient contextual diversity for\n",
        "# robust sense induction (cf. Ustalov et al. 2019 who use threshold of 50).\n",
        "USE_UMAP             = True                  # Reduce to 50D before clustering\n",
        "UMAP_N_COMPONENTS    = 50\n",
        "RANDOM_STATE         = 42\n",
        "\n",
        "# ─── Optional UMAP reduction ─────────────────────────────────────────────────\n",
        "\n",
        "def reduce_embeddings(embeddings: np.ndarray) -> np.ndarray:\n",
        "#     \"\"\"\n",
        "#     Optionally reduce embedding dimensionality with UMAP before clustering.\n",
        "#     UMAP (McInnes et al. 2018) improves cluster separation for high-dim data.\n",
        "#     Falls back to PCA if umap-learn is not installed or if UMAP fails.\n",
        "#     \"\"\"\n",
        "#     if not USE_UMAP or embeddings.shape[0] <= MIN_INSTANCES: # Added <= MIN_INSTANCES for robustness\n",
        "#         return embeddings\n",
        "\n",
        "#     try:\n",
        "#         import umap\n",
        "#         # Ensure n_components is always less than the number of samples\n",
        "#         n_components_umap = min(UMAP_N_COMPONENTS, embeddings.shape[0] - 1)\n",
        "#         if n_components_umap <= 0: # Handle cases where n_samples is 1\n",
        "#             return embeddings\n",
        "\n",
        "#         reducer = umap.UMAP(\n",
        "#             n_components=n_components_umap,\n",
        "#             metric=\"cosine\",\n",
        "#             random_state=RANDOM_STATE,\n",
        "#             n_jobs=1,\n",
        "#         )\n",
        "#         return reducer.fit_transform(embeddings)\n",
        "#     except (ImportError, TypeError) as e: # Catch both ImportError and TypeError\n",
        "#         if isinstance(e, ImportError):\n",
        "#             print(\"    [warn] umap-learn not found — using PCA fallback. \"\n",
        "#                   \"Install: pip install umap-learn --break-system-packages\")\n",
        "#         else:\n",
        "#             print(f\"    [warn] UMAP failed ({e}) — using PCA fallback for this lemma. \"\n",
        "#                   f\"Embeddings shape: {embeddings.shape}\")\n",
        "#         from sklearn.decomposition import PCA\n",
        "#         # Ensure n_components for PCA is also less than number of samples and features\n",
        "#         n_components_pca = min(UMAP_N_COMPONENTS, embeddings.shape[0] - 1, embeddings.shape[1])\n",
        "#         if n_components_pca <= 0: # Handle cases where n_samples is 1\n",
        "#             return embeddings\n",
        "#         return PCA(n_components=n_components_pca, random_state=RANDOM_STATE).fit_transform(embeddings)\n",
        "    \"\"\"\n",
        "    Reduce embedding dimensionality with UMAP before clustering.\n",
        "    UMAP (McInnes et al. 2018) improves cluster separation for high-dim data.\n",
        "    Falls back to PCA if umap-learn is not installed.\n",
        "\n",
        "    With MIN_INSTANCES=60, we always have n_samples ≥ 60 > 50 = n_components,\n",
        "    so UMAP will never fail with the k >= N error.\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    Reduce embedding dimensionality with UMAP before clustering.\n",
        "    UMAP (McInnes et al. 2018) improves cluster separation for high-dim data.\n",
        "    Falls back to PCA if umap-learn is not installed.\n",
        "\n",
        "    With MIN_INSTANCES=60, we always have n_samples ≥ 60 > 50 = n_components,\n",
        "    so UMAP will never fail with the k >= N error.\n",
        "    \"\"\"\n",
        "    if not USE_UMAP or embeddings.shape[0] < UMAP_N_COMPONENTS:\n",
        "        return embeddings\n",
        "    try:\n",
        "        import umap\n",
        "        reducer = umap.UMAP(\n",
        "            n_components=UMAP_N_COMPONENTS,\n",
        "            metric=\"cosine\",\n",
        "            random_state=RANDOM_STATE,\n",
        "            n_jobs=1,\n",
        "        )\n",
        "        return reducer.fit_transform(embeddings)\n",
        "    except ImportError:\n",
        "        print(\"    [warn] umap-learn not found — using PCA fallback. \"\n",
        "              \"Install: pip install umap-learn --break-system-packages\")\n",
        "        from sklearn.decomposition import PCA\n",
        "        n = min(UMAP_N_COMPONENTS, embeddings.shape[0] - 1, embeddings.shape[1])\n",
        "        return PCA(n_components=n, random_state=RANDOM_STATE).fit_transform(embeddings)\n",
        "\n",
        "\n",
        "# ─── Core WSI for a single lemma ──────────────────────────────────────────────\n",
        "\n",
        "def induce_senses_for_lemma(\n",
        "    embeddings: np.ndarray,\n",
        ") -> Tuple[int, int, float, np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Given embeddings for all occurrences of a lemma, find the optimal k\n",
        "    using silhouette score for both Ward and KMeans clustering.\n",
        "\n",
        "    Returns:\n",
        "        k_ward, k_kmeans, best_silhouette_ward,\n",
        "        labels_ward (np.ndarray), labels_kmeans (np.ndarray)\n",
        "    \"\"\"\n",
        "    n = len(embeddings)\n",
        "\n",
        "    # Insufficient data → monosemous\n",
        "    if n < MIN_INSTANCES:\n",
        "        ones = np.zeros(n, dtype=int)\n",
        "        return 1, 1, 0.0, ones, ones\n",
        "\n",
        "    reduced = reduce_embeddings(embeddings)\n",
        "\n",
        "    best_k_ward, best_sil_ward, best_labels_ward   = 1, -1.0, np.zeros(n, dtype=int)\n",
        "    best_k_km,   best_sil_km,   best_labels_km     = 1, -1.0, np.zeros(n, dtype=int)\n",
        "\n",
        "    for k in K_RANGE:\n",
        "        if k >= n:\n",
        "            break  # Can't have more clusters than data points\n",
        "\n",
        "        # Ward agglomerative\n",
        "        try:\n",
        "            ward = AgglomerativeClustering(n_clusters=k, linkage=\"ward\")\n",
        "            labels_w = ward.fit_predict(reduced)\n",
        "            if len(np.unique(labels_w)) > 1:\n",
        "                sil_w = silhouette_score(reduced, labels_w, metric=\"euclidean\",\n",
        "                                         sample_size=min(1000, n))\n",
        "                if sil_w > best_sil_ward:\n",
        "                    best_sil_ward, best_k_ward, best_labels_ward = sil_w, k, labels_w\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        # K-Means\n",
        "        try:\n",
        "            km = KMeans(n_clusters=k, random_state=RANDOM_STATE,\n",
        "                        n_init=10, max_iter=300)\n",
        "            labels_k = km.fit_predict(reduced)\n",
        "            if len(np.unique(labels_k)) > 1:\n",
        "                sil_k = silhouette_score(reduced, labels_k, metric=\"euclidean\",\n",
        "                                          sample_size=min(1000, n))\n",
        "                if sil_k > best_sil_km:\n",
        "                    best_sil_km, best_k_km, best_labels_km = sil_k, k, labels_k\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # Apply monosemy threshold\n",
        "    if best_sil_ward < SILHOUETTE_THRESHOLD:\n",
        "        best_k_ward    = 1\n",
        "        best_labels_ward = np.zeros(n, dtype=int)\n",
        "\n",
        "    if best_sil_km < SILHOUETTE_THRESHOLD:\n",
        "        best_k_km    = 1\n",
        "        best_labels_km = np.zeros(n, dtype=int)\n",
        "\n",
        "    return (best_k_ward, best_k_km, max(best_sil_ward, 0.0),\n",
        "            best_labels_ward, best_labels_km)\n",
        "\n",
        "\n",
        "# ─── Run WSI for entire language ──────────────────────────────────────────────\n",
        "\n",
        "def run_wsi_for_language(lang: str):\n",
        "    \"\"\"\n",
        "    Load embeddings and run WSI for all lemmas in a language.\n",
        "    Saves two files:\n",
        "      1. {lang}_wsi_results.csv    - summary stats per lemma\n",
        "      2. {lang}_sense_labels.csv   - cluster assignments per occurrence\n",
        "    \"\"\"\n",
        "    embeddings_file = DATA_DIR / f\"{lang}_embeddings.npz\"\n",
        "    nouns_file      = DATA_DIR / f\"{lang}_nouns.csv\"\n",
        "\n",
        "    if not embeddings_file.exists():\n",
        "        print(f\"  [{lang.upper()}] ERROR: {embeddings_file.name} not found. Run Step 3 first.\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\n  [{lang.upper()}] Loading embeddings…\")\n",
        "    data = np.load(embeddings_file, allow_pickle=True)\n",
        "    lemmas_array = data[\"lemmas\"]\n",
        "    embeddings   = data[\"embeddings\"]\n",
        "    verse_ids    = data[\"verse_ids\"]\n",
        "    unique_lemmas = np.unique(lemmas_array)\n",
        "\n",
        "    print(f\"  [{lang.upper()}] Running WSI for {len(unique_lemmas):,} lemmas…\")\n",
        "\n",
        "    results = []\n",
        "    all_labels = []  # Collect per-instance labels for sense_labels.csv\n",
        "\n",
        "    for i, lemma in enumerate(unique_lemmas):\n",
        "        mask = (lemmas_array == lemma)\n",
        "        lemma_embeds = embeddings[mask]\n",
        "        lemma_vids   = verse_ids[mask]\n",
        "\n",
        "        k_ward, k_km, sil_ward, labels_ward, labels_km = induce_senses_for_lemma(lemma_embeds)\n",
        "\n",
        "        # Agreement: what % of instances assigned to same cluster by both methods?\n",
        "        if len(labels_ward) > 0:\n",
        "            agree = (labels_ward == labels_km).mean() * 100\n",
        "        else:\n",
        "            agree = 100.0\n",
        "\n",
        "        results.append({\n",
        "            \"lemma\":            lemma,\n",
        "            \"n_instances\":      len(lemma_embeds),\n",
        "            \"k_ward\":           k_ward,\n",
        "            \"k_kmeans\":         k_km,\n",
        "            \"silhouette_ward\":  round(sil_ward, 4),\n",
        "            \"agreement_pct\":    round(agree, 2),\n",
        "        })\n",
        "\n",
        "        # Collect per-instance cluster assignments\n",
        "        for vid, cluster_w, cluster_k in zip(lemma_vids, labels_ward, labels_km):\n",
        "            all_labels.append({\n",
        "                \"lemma\":          lemma,\n",
        "                \"verse_id\":       vid,\n",
        "                \"cluster_ward\":   int(cluster_w),\n",
        "                \"cluster_kmeans\": int(cluster_k),\n",
        "            })\n",
        "\n",
        "        if (i + 1) % 50 == 0 or (i + 1) == len(unique_lemmas):\n",
        "            print(f\"    … {i + 1:,}/{len(unique_lemmas):,} lemmas\", end=\"\\r\")\n",
        "\n",
        "    # Save summary results\n",
        "    df = pd.DataFrame(results)\n",
        "    out_path = DATA_DIR / f\"{lang}_wsi_results.csv\"\n",
        "    df.to_csv(out_path, index=False)\n",
        "    print(f\"\\n  [{lang.upper()}] Saved: {out_path.name}\")\n",
        "\n",
        "    # Save per-instance cluster labels\n",
        "    labels_df = pd.DataFrame(all_labels)\n",
        "    labels_path = DATA_DIR / f\"{lang}_sense_labels.csv\"\n",
        "    labels_df.to_csv(labels_path, index=False)\n",
        "    print(f\"  [{lang.upper()}] Saved: {labels_path.name}\")\n",
        "\n",
        "    print(f\"  [{lang.upper()}] Mean k (Ward): {df['k_ward'].mean():.2f}  \"\n",
        "          f\"Median: {df['k_ward'].median():.0f}  \"\n",
        "          f\"Monosemous (k=1): {(df['k_ward']==1).sum()} / {len(df)}\")\n",
        "\n",
        "\n",
        "# ─── Main ─────────────────────────────────────────────────────────────────────\n",
        "\n",
        "# def main():\n",
        "#     print(\"=\" * 60)\n",
        "#     print(\"Step 4: Word Sense Induction (WSI)\")\n",
        "#     print(\"=\" * 60)\n",
        "\n",
        "#     run_wsi_for_language(\"english\")\n",
        "#     run_wsi_for_language(\"chinese\")\n",
        "\n",
        "#     # Quick comparison preview\n",
        "#     en = pd.read_csv(DATA_DIR / \"english_wsi_results.csv\")\n",
        "#     zh = pd.read_csv(DATA_DIR / \"chinese_wsi_results.csv\")\n",
        "\n",
        "#     print(\"\\n── Quick Comparison Preview ──\")\n",
        "#     print(f\"  EN | mean senses/lemma (Ward) : {en['k_ward'].mean():.3f}\")\n",
        "#     print(f\"  ZH | mean senses/lemma (Ward) : {zh['k_ward'].mean():.3f}\")\n",
        "#     print(f\"  EN | % polysemous lemmas       : {(en['k_ward'] > 1).mean()*100:.1f}%\")\n",
        "#     print(f\"  ZH | % polysemous lemmas       : {(zh['k_ward'] > 1).mean()*100:.1f}%\")\n",
        "\n",
        "#     print(\"\\n✓ Step 4 complete.\\n\")\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()\n"
      ],
      "metadata": {
        "id": "wCI4OOXjYSNs"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"Step 4: Word Sense Induction (WSI)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "run_wsi_for_language(\"english\")\n",
        "run_wsi_for_language(\"chinese\")\n",
        "\n",
        "# Quick comparison preview\n",
        "en = pd.read_csv(DATA_DIR / \"english_wsi_results.csv\")\n",
        "zh = pd.read_csv(DATA_DIR / \"chinese_wsi_results.csv\")\n",
        "\n",
        "print(\"\\n── Quick Comparison Preview ──\")\n",
        "print(f\"  EN | mean senses/lemma (Ward) : {en['k_ward'].mean():.3f}\")\n",
        "print(f\"  ZH | mean senses/lemma (Ward) : {zh['k_ward'].mean():.3f}\")\n",
        "print(f\"  EN | % polysemous lemmas       : {(en['k_ward'] > 1).mean()*100:.1f}%\")\n",
        "print(f\"  ZH | % polysemous lemmas       : {(zh['k_ward'] > 1).mean()*100:.1f}%\")\n",
        "\n",
        "print(\"\\n✓ Step 4 complete.\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUNKMUx1aRI5",
        "outputId": "8d68a90c-6d99-4c64-e687-8f081d81b578"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Step 4: Word Sense Induction (WSI)\n",
            "============================================================\n",
            "\n",
            "  [ENGLISH] Loading embeddings…\n",
            "  [ENGLISH] Running WSI for 636 lemmas…\n",
            "    … 636/636 lemmas\n",
            "  [ENGLISH] Saved: english_wsi_results.csv\n",
            "  [ENGLISH] Saved: english_sense_labels.csv\n",
            "  [ENGLISH] Mean k (Ward): 2.13  Median: 2  Monosemous (k=1): 252 / 636\n",
            "\n",
            "  [CHINESE] Loading embeddings…\n",
            "  [CHINESE] Running WSI for 572 lemmas…\n",
            "    … 572/572 lemmas\n",
            "  [CHINESE] Saved: chinese_wsi_results.csv\n",
            "  [CHINESE] Saved: chinese_sense_labels.csv\n",
            "  [CHINESE] Mean k (Ward): 2.14  Median: 2  Monosemous (k=1): 273 / 572\n",
            "\n",
            "── Quick Comparison Preview ──\n",
            "  EN | mean senses/lemma (Ward) : 2.126\n",
            "  ZH | mean senses/lemma (Ward) : 2.136\n",
            "  EN | % polysemous lemmas       : 60.4%\n",
            "  ZH | % polysemous lemmas       : 52.3%\n",
            "\n",
            "✓ Step 4 complete.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Validation and Statistical Analysis"
      ],
      "metadata": {
        "id": "lX7M7-Toc3Dl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk scipy matplotlib seaborn pandas numpy --break-system-packages"
      ],
      "metadata": {
        "id": "BvfDY1GpdndP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "yu113JDddzl4",
        "outputId": "cd49472e-313d-4837-a74f-c29ef4a0dba6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Step 5: Validation and Statistical Analysis\n",
        "============================================\n",
        "Validates WSI-induced sense counts against gold-standard lexical resources:\n",
        "  - English: Princeton WordNet (via NLTK)\n",
        "  - Chinese:  Chinese WordNet (via Taiwanese CWN or HowNet)\n",
        "\n",
        "Also performs the core statistical comparison between languages:\n",
        "  - Mann-Whitney U test (non-parametric, appropriate for skewed count data)\n",
        "  - Cohen's d effect size\n",
        "  - Spearman correlation with WordNet sense counts (validation)\n",
        "  - Distribution plots (saved as PNG for inclusion in paper)\n",
        "\n",
        "Outputs:\n",
        "  - output/validation_correlation_en.csv\n",
        "  - output/validation_correlation_zh.csv   (if CWN available)\n",
        "  - output/statistical_comparison.csv\n",
        "  - output/figures/sense_distribution_en.png\n",
        "  - output/figures/sense_distribution_zh.png\n",
        "  - output/figures/comparison_boxplot.png\n",
        "  - output/figures/wordnet_correlation_en.png\n",
        "\n",
        "Usage:\n",
        "  pip install nltk scipy matplotlib seaborn pandas numpy --break-system-packages\n",
        "  python -c \"import nltk; nltk.download('wordnet')\"\n",
        "  python 05_validation_statistics.py\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "from scipy import stats\n",
        "from typing import Optional, Tuple\n",
        "\n",
        "# ─── Configuration ────────────────────────────────────────────────────────────\n",
        "\n",
        "DATA_DIR    = Path(\"/content\") / \"bible_data\"\n",
        "OUTPUT_DIR = Path(\"/content\") / \"output\"\n",
        "# DATA_DIR   = Path(__file__).parent.parent / \"data\"\n",
        "# OUTPUT_DIR = Path(__file__).parent.parent / \"output\"\n",
        "FIG_DIR    = OUTPUT_DIR / \"figures\"\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "FIG_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "plt.rcParams.update({\n",
        "    \"font.family\":  \"DejaVu Sans\",\n",
        "    \"font.size\":    11,\n",
        "    \"axes.titlesize\": 13,\n",
        "    \"axes.labelsize\": 12,\n",
        "})\n",
        "\n",
        "\n",
        "# ─── WordNet Validation (English) ────────────────────────────────────────────\n",
        "\n",
        "def get_wordnet_sense_counts(lemmas: list) -> dict:\n",
        "    \"\"\"\n",
        "    Look up the number of synsets for each lemma in Princeton WordNet.\n",
        "    Noun synsets only (pos='n').\n",
        "    \"\"\"\n",
        "    try:\n",
        "        from nltk.corpus import wordnet as wn\n",
        "    except ImportError:\n",
        "        raise ImportError(\"Run: pip install nltk --break-system-packages && \"\n",
        "                          \"python -c \\\"import nltk; nltk.download('wordnet')\\\"\")\n",
        "\n",
        "    counts = {}\n",
        "    for lemma in lemmas:\n",
        "        synsets = wn.synsets(lemma.lower(), pos=wn.NOUN)\n",
        "        counts[lemma] = len(synsets)\n",
        "    return counts\n",
        "\n",
        "\n",
        "def validate_english(en_results: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Correlate WSI-induced k_ward with WordNet sense counts for English nouns.\n",
        "    Returns a merged DataFrame with both counts.\n",
        "    \"\"\"\n",
        "    print(\"  [validate] Looking up Princeton WordNet sense counts…\")\n",
        "    lemmas = en_results[\"lemma\"].tolist()\n",
        "    wn_counts = get_wordnet_sense_counts(lemmas)\n",
        "\n",
        "    en_results = en_results.copy()\n",
        "    en_results[\"wn_senses\"] = en_results[\"lemma\"].map(wn_counts).fillna(0).astype(int)\n",
        "\n",
        "    # Keep only lemmas with at least 1 WordNet entry\n",
        "    valid = en_results[en_results[\"wn_senses\"] > 0].copy()\n",
        "\n",
        "    rho, p = stats.spearmanr(valid[\"k_ward\"], valid[\"wn_senses\"])\n",
        "    print(f\"  [validate] EN Spearman ρ(k_ward, WN_senses) = {rho:.3f}  p = {p:.4f}  \"\n",
        "          f\"(n={len(valid)})\")\n",
        "\n",
        "    return valid, rho, p\n",
        "\n",
        "\n",
        "# ─── Statistical Comparison ──────────────────────────────────────────────────\n",
        "\n",
        "def mann_whitney_comparison(\n",
        "    en_k: np.ndarray,\n",
        "    zh_k: np.ndarray,\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    Mann-Whitney U test comparing mean sense counts between EN and ZH.\n",
        "    Also computes Cohen's d and the common language effect size (CLES).\n",
        "    \"\"\"\n",
        "    u_stat, p_val = stats.mannwhitneyu(en_k, zh_k, alternative=\"two-sided\")\n",
        "    n1, n2        = len(en_k), len(zh_k)\n",
        "    cles          = u_stat / (n1 * n2)  # Common Language Effect Size\n",
        "\n",
        "    # Cohen's d (for reference alongside CLES)\n",
        "    pooled_std = np.sqrt(\n",
        "        ((n1 - 1) * en_k.std(ddof=1) ** 2 + (n2 - 1) * zh_k.std(ddof=1) ** 2)\n",
        "        / (n1 + n2 - 2)\n",
        "    )\n",
        "    cohens_d = (en_k.mean() - zh_k.mean()) / (pooled_std + 1e-9)\n",
        "\n",
        "    return {\n",
        "        \"en_mean_k\":   round(en_k.mean(), 4),\n",
        "        \"zh_mean_k\":   round(zh_k.mean(), 4),\n",
        "        \"en_median_k\": round(float(np.median(en_k)), 4),\n",
        "        \"zh_median_k\": round(float(np.median(zh_k)), 4),\n",
        "        \"en_std_k\":    round(en_k.std(ddof=1), 4),\n",
        "        \"zh_std_k\":    round(zh_k.std(ddof=1), 4),\n",
        "        \"U_statistic\": round(u_stat, 2),\n",
        "        \"p_value\":     round(p_val, 6),\n",
        "        \"CLES\":        round(cles, 4),\n",
        "        \"cohens_d\":    round(cohens_d, 4),\n",
        "        \"n_en\":        int(n1),\n",
        "        \"n_zh\":        int(n2),\n",
        "    }\n",
        "\n",
        "\n",
        "# ─── Figures ─────────────────────────────────────────────────────────────────\n",
        "\n",
        "def plot_sense_distribution(df: pd.DataFrame, lang: str, col: str = \"k_ward\") -> None:\n",
        "    fig, ax = plt.subplots(figsize=(8, 4))\n",
        "    counts = df[col].value_counts().sort_index()\n",
        "    ax.bar(counts.index, counts.values, color=\"#4C72B0\", edgecolor=\"white\", linewidth=0.5)\n",
        "    ax.set_xlabel(\"Number of Induced Senses (k)\")\n",
        "    ax.set_ylabel(\"Number of Lemmas\")\n",
        "    ax.set_title(f\"{lang.capitalize()} — Distribution of Induced Senses per Noun Lemma\")\n",
        "    ax.set_xticks(range(1, df[col].max() + 1))\n",
        "    fig.tight_layout()\n",
        "    path = FIG_DIR / f\"sense_distribution_{lang}.png\"\n",
        "    fig.savefig(path, dpi=150)\n",
        "    plt.close(fig)\n",
        "    print(f\"  [fig] Saved: {path.name}\")\n",
        "\n",
        "\n",
        "def plot_comparison_boxplot(en_df: pd.DataFrame, zh_df: pd.DataFrame) -> None:\n",
        "    combined = pd.concat([\n",
        "        en_df[[\"k_ward\"]].assign(Language=\"English\"),\n",
        "        zh_df[[\"k_ward\"]].assign(Language=\"Chinese\"),\n",
        "    ])\n",
        "    fig, ax = plt.subplots(figsize=(7, 5))\n",
        "    sns.violinplot(data=combined, x=\"Language\", y=\"k_ward\",\n",
        "                   palette=[\"#4C72B0\", \"#DD8452\"], inner=\"box\", ax=ax)\n",
        "    ax.set_ylabel(\"Induced Senses per Lemma (k, Ward)\")\n",
        "    ax.set_title(\"Distribution of Polysemy Degree: English vs. Chinese Common Nouns\\n\"\n",
        "                 \"(Bible Parallel Corpus, WSI via XLM-R + Agglomerative Clustering)\")\n",
        "    fig.tight_layout()\n",
        "    path = FIG_DIR / \"comparison_violinplot.png\"\n",
        "    fig.savefig(path, dpi=150)\n",
        "    plt.close(fig)\n",
        "    print(f\"  [fig] Saved: {path.name}\")\n",
        "\n",
        "\n",
        "def plot_wordnet_correlation(valid_df: pd.DataFrame, rho: float, p: float) -> None:\n",
        "    fig, ax = plt.subplots(figsize=(6, 5))\n",
        "    ax.scatter(valid_df[\"wn_senses\"], valid_df[\"k_ward\"],\n",
        "               alpha=0.4, s=20, color=\"#4C72B0\")\n",
        "    ax.set_xlabel(\"WordNet Noun Synset Count\")\n",
        "    ax.set_ylabel(\"WSI-Induced k (Ward)\")\n",
        "    ax.set_title(f\"Validation: WSI k vs. WordNet Senses (English Nouns)\\n\"\n",
        "                 f\"Spearman ρ = {rho:.3f}, p = {p:.4f}\")\n",
        "    # Trend line\n",
        "    m, b = np.polyfit(valid_df[\"wn_senses\"], valid_df[\"k_ward\"], 1)\n",
        "    x_line = np.linspace(valid_df[\"wn_senses\"].min(), valid_df[\"wn_senses\"].max(), 100)\n",
        "    ax.plot(x_line, m * x_line + b, color=\"red\", linewidth=1.5, linestyle=\"--\")\n",
        "    fig.tight_layout()\n",
        "    path = FIG_DIR / \"wordnet_correlation_en.png\"\n",
        "    fig.savefig(path, dpi=150)\n",
        "    plt.close(fig)\n",
        "    print(f\"  [fig] Saved: {path.name}\")\n",
        "\n",
        "\n",
        "# ─── Main ─────────────────────────────────────────────────────────────────────\n",
        "\n",
        "# def main():\n",
        "#     print(\"=\" * 60)\n",
        "#     print(\"Step 5: Validation and Statistical Analysis\")\n",
        "#     print(\"=\" * 60)\n",
        "\n",
        "#     en = pd.read_csv(DATA_DIR / \"english_wsi_results.csv\")\n",
        "#     zh = pd.read_csv(DATA_DIR / \"chinese_wsi_results.csv\")\n",
        "\n",
        "#     # ── Validation (English vs. WordNet) ──────────────────────────\n",
        "#     valid_en, rho, p = validate_english(en)\n",
        "#     valid_en.to_csv(OUTPUT_DIR / \"validation_correlation_en.csv\", index=False)\n",
        "\n",
        "#     # ── Statistical Comparison ────────────────────────────────────\n",
        "#     stats_result = mann_whitney_comparison(\n",
        "#         en[\"k_ward\"].values,\n",
        "#         zh[\"k_ward\"].values,\n",
        "#     )\n",
        "#     stats_df = pd.DataFrame([stats_result])\n",
        "#     stats_df.to_csv(OUTPUT_DIR / \"statistical_comparison.csv\", index=False)\n",
        "\n",
        "#     print(\"\\n── Statistical Comparison Results ──\")\n",
        "#     for k, v in stats_result.items():\n",
        "#         print(f\"  {k:22s}: {v}\")\n",
        "\n",
        "#     # ── Figures ───────────────────────────────────────────────────\n",
        "#     plot_sense_distribution(en, \"english\")\n",
        "#     plot_sense_distribution(zh, \"chinese\")\n",
        "#     plot_comparison_boxplot(en, zh)\n",
        "#     plot_wordnet_correlation(valid_en, rho, p)\n",
        "\n",
        "#     # ── Interpretation ────────────────────────────────────────────\n",
        "#     print(\"\\n── Interpretation ──\")\n",
        "#     if stats_result[\"p_value\"] < 0.05:\n",
        "#         direction = \"English\" if stats_result[\"en_mean_k\"] > stats_result[\"zh_mean_k\"] else \"Chinese\"\n",
        "#         print(f\"  Significant difference found (p={stats_result['p_value']:.4f}).\")\n",
        "#         print(f\"  {direction} nouns show higher mean polysemy degree.\")\n",
        "#     else:\n",
        "#         print(f\"  No significant difference found (p={stats_result['p_value']:.4f}).\")\n",
        "\n",
        "#     d = abs(stats_result[\"cohens_d\"])\n",
        "#     magnitude = \"small\" if d < 0.2 else (\"medium\" if d < 0.5 else \"large\")\n",
        "#     print(f\"  Effect size: Cohen's d = {stats_result['cohens_d']:.3f} ({magnitude})\")\n",
        "#     print(f\"  Spearman ρ (EN WSI vs. WordNet): {rho:.3f} (p={p:.4f})\")\n",
        "\n",
        "#     print(\"\\n✓ Step 5 complete.\\n\")\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()\n"
      ],
      "metadata": {
        "id": "A2wqaZynajnY"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"Step 5: Validation and Statistical Analysis\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "en = pd.read_csv(DATA_DIR / \"english_wsi_results.csv\")\n",
        "zh = pd.read_csv(DATA_DIR / \"chinese_wsi_results.csv\")\n",
        "\n",
        "# ── Validation (English vs. WordNet) ──────────────────────────\n",
        "valid_en, rho, p = validate_english(en)\n",
        "valid_en.to_csv(OUTPUT_DIR / \"validation_correlation_en.csv\", index=False)\n",
        "\n",
        "# ── Statistical Comparison ────────────────────────────────────\n",
        "stats_result = mann_whitney_comparison(\n",
        "    en[\"k_ward\"].values,\n",
        "    zh[\"k_ward\"].values,\n",
        ")\n",
        "stats_df = pd.DataFrame([stats_result])\n",
        "stats_df.to_csv(OUTPUT_DIR / \"statistical_comparison.csv\", index=False)\n",
        "\n",
        "print(\"\\n── Statistical Comparison Results ──\")\n",
        "for k, v in stats_result.items():\n",
        "    print(f\"  {k:22s}: {v}\")\n",
        "\n",
        "# ── Figures ───────────────────────────────────────────────────\n",
        "plot_sense_distribution(en, \"english\")\n",
        "plot_sense_distribution(zh, \"chinese\")\n",
        "plot_comparison_boxplot(en, zh)\n",
        "plot_wordnet_correlation(valid_en, rho, p)\n",
        "\n",
        "# ── Interpretation ────────────────────────────────────────────\n",
        "print(\"\\n── Interpretation ──\")\n",
        "if stats_result[\"p_value\"] < 0.05:\n",
        "    direction = \"English\" if stats_result[\"en_mean_k\"] > stats_result[\"zh_mean_k\"] else \"Chinese\"\n",
        "    print(f\"  Significant difference found (p={stats_result['p_value']:.4f}).\")\n",
        "    print(f\"  {direction} nouns show higher mean polysemy degree.\")\n",
        "else:\n",
        "    print(f\"  No significant difference found (p={stats_result['p_value']:.4f}).\")\n",
        "\n",
        "d = abs(stats_result[\"cohens_d\"])\n",
        "magnitude = \"small\" if d < 0.2 else (\"medium\" if d < 0.5 else \"large\")\n",
        "print(f\"  Effect size: Cohen's d = {stats_result['cohens_d']:.3f} ({magnitude})\")\n",
        "print(f\"  Spearman ρ (EN WSI vs. WordNet): {rho:.3f} (p={p:.4f})\")\n",
        "\n",
        "print(\"\\n✓ Step 5 complete.\\n\")\n"
      ],
      "metadata": {
        "id": "M1bzCPW5dCC_",
        "outputId": "fe6f7f28-0096-48e6-c1c2-50f41556855a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Step 5: Validation and Statistical Analysis\n",
            "============================================================\n",
            "  [validate] Looking up Princeton WordNet sense counts…\n",
            "  [validate] EN Spearman ρ(k_ward, WN_senses) = 0.115  p = 0.0039  (n=630)\n",
            "\n",
            "── Statistical Comparison Results ──\n",
            "  en_mean_k             : 2.1258\n",
            "  zh_mean_k             : 2.1364\n",
            "  en_median_k           : 2.0\n",
            "  zh_median_k           : 2.0\n",
            "  en_std_k              : 1.5724\n",
            "  zh_std_k              : 1.7692\n",
            "  U_statistic           : 192412.0\n",
            "  p_value               : 0.062546\n",
            "  CLES                  : 0.5289\n",
            "  cohens_d              : -0.0063\n",
            "  n_en                  : 636\n",
            "  n_zh                  : 572\n",
            "  [fig] Saved: sense_distribution_english.png\n",
            "  [fig] Saved: sense_distribution_chinese.png\n",
            "  [fig] Saved: comparison_violinplot.png\n",
            "  [fig] Saved: wordnet_correlation_en.png\n",
            "\n",
            "── Interpretation ──\n",
            "  No significant difference found (p=0.0625).\n",
            "  Effect size: Cohen's d = -0.006 (small)\n",
            "  Spearman ρ (EN WSI vs. WordNet): 0.115 (p=0.0039)\n",
            "\n",
            "✓ Step 5 complete.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6: Qualitative Analysis"
      ],
      "metadata": {
        "id": "O3m1UJnseHjn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Step 6: Qualitative Analysis — Sense Cluster Inspection\n",
        "=========================================================\n",
        "For each polysemous lemma, retrieves representative example verses\n",
        "for each induced sense cluster. This supports the qualitative\n",
        "analysis section of the paper, demonstrating that clusters correspond\n",
        "to meaningful, interpretable senses.\n",
        "\n",
        "Also generates a LaTeX-ready table of top polysemous words\n",
        "for both languages (for paper Table 3).\n",
        "\n",
        "Outputs:\n",
        "  - output/qualitative_en_top_polysemous.txt   (sense examples)\n",
        "  - output/qualitative_zh_top_polysemous.txt\n",
        "  - output/table_top_polysemous_latex.tex       (LaTeX table)\n",
        "  - output/polysemy_profile_comparison.csv      (wide comparison table)\n",
        "\n",
        "Usage:\n",
        "  python 06_qualitative_analysis.py\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "\n",
        "# ─── Configuration ────────────────────────────────────────────────────────────\n",
        "\n",
        "DATA_DIR    = Path(\"/content\") / \"bible_data\"\n",
        "OUTPUT_DIR = Path(\"/content\") / \"output\"\n",
        "# DATA_DIR   = Path(__file__).parent.parent / \"data\"\n",
        "# OUTPUT_DIR = Path(__file__).parent.parent / \"output\"\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "TOP_N_WORDS       = 20   # Top N most polysemous lemmas per language\n",
        "EXAMPLES_PER_SENSE = 2   # Number of example verses per cluster\n",
        "\n",
        "\n",
        "# ─── Sense Example Retrieval ─────────────────────────────────────────────────\n",
        "\n",
        "def get_sense_examples(\n",
        "    lang: str,\n",
        "    top_n: int = TOP_N_WORDS,\n",
        "    examples_per_sense: int = EXAMPLES_PER_SENSE,\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    For the top_n most polysemous lemmas, retrieve example contexts\n",
        "    for each induced sense cluster.\n",
        "\n",
        "    Returns a formatted string ready for a paper's qualitative appendix.\n",
        "    \"\"\"\n",
        "    results_df = pd.read_csv(DATA_DIR / f\"{lang}_wsi_results.csv\")\n",
        "    labels_df  = pd.read_csv(DATA_DIR / f\"{lang}_sense_labels.csv\")\n",
        "    nouns_df   = pd.read_csv(DATA_DIR / f\"{lang}_nouns.csv\")\n",
        "\n",
        "    # Top polysemous by k_ward\n",
        "    poly = results_df[results_df[\"k_ward\"] > 1].nlargest(top_n, \"k_ward\")\n",
        "\n",
        "    # Merge labels with original contexts\n",
        "    merged = labels_df.merge(\n",
        "        nouns_df[[\"lemma\", \"verse_id\", \"context\"]].drop_duplicates(),\n",
        "        on=[\"lemma\", \"verse_id\"],\n",
        "        how=\"left\",\n",
        "    )\n",
        "\n",
        "    lines = []\n",
        "    lines.append(f\"{'='*60}\")\n",
        "    lines.append(f\"QUALITATIVE SENSE ANALYSIS — {lang.upper()}\")\n",
        "    lines.append(f\"Top {top_n} Most Polysemous Nouns (WSI, Ward Clustering)\")\n",
        "    lines.append(f\"{'='*60}\\n\")\n",
        "\n",
        "    for _, row in poly.iterrows():\n",
        "        lemma = row[\"lemma\"]\n",
        "        k     = int(row[\"k_ward\"])\n",
        "        n_occ = int(row[\"n_occurrences\"])\n",
        "        sil   = row.get(\"silhouette_ward\", \"N/A\")\n",
        "\n",
        "        lines.append(f\"Lemma: '{lemma}'  |  k={k}  |  n={n_occ}  |  silhouette={sil}\")\n",
        "        lines.append(\"-\" * 50)\n",
        "\n",
        "        lemma_data = merged[merged[\"lemma\"] == lemma]\n",
        "\n",
        "        for cluster_id in range(k):\n",
        "            cluster_rows = lemma_data[lemma_data[\"cluster_ward\"] == cluster_id]\n",
        "            lines.append(f\"  Sense {cluster_id + 1} ({len(cluster_rows)} occurrences):\")\n",
        "\n",
        "            # Sample diverse examples\n",
        "            sample = cluster_rows.dropna(subset=[\"context\"]).head(examples_per_sense)\n",
        "            for _, ex in sample.iterrows():\n",
        "                ctx = str(ex[\"context\"])[:200].replace(\"\\n\", \" \")\n",
        "                lines.append(f\"    • {ex['verse_id']}: {ctx}\")\n",
        "\n",
        "        lines.append(\"\")\n",
        "\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "\n",
        "# ─── LaTeX Table Generation ──────────────────────────────────────────────────\n",
        "\n",
        "def generate_latex_table(top_n: int = 15) -> str:\n",
        "    \"\"\"\n",
        "    Generate a LaTeX longtable comparing top polysemous nouns in both languages.\n",
        "    Format:\n",
        "      Rank | English Lemma | EN k | Chinese Lemma | ZH k\n",
        "    \"\"\"\n",
        "    en = pd.read_csv(DATA_DIR / \"english_wsi_results.csv\")\n",
        "    zh = pd.read_csv(DATA_DIR / \"chinese_wsi_results.csv\")\n",
        "\n",
        "    en_top = en.nlargest(top_n, \"k_ward\")[[\"lemma\", \"k_ward\", \"n_occurrences\"]].reset_index(drop=True)\n",
        "    zh_top = zh.nlargest(top_n, \"k_ward\")[[\"lemma\", \"k_ward\", \"n_occurrences\"]].reset_index(drop=True)\n",
        "\n",
        "    lines = [\n",
        "        r\"\\begin{table}[h]\",\n",
        "        r\"\\centering\",\n",
        "        r\"\\caption{Top Polysemous Common Nouns by Induced Sense Count (k): English vs. Chinese}\",\n",
        "        r\"\\label{tab:top_polysemous}\",\n",
        "        r\"\\begin{tabular}{clccclcc}\",\n",
        "        r\"\\toprule\",\n",
        "        r\"Rank & English Lemma & EN $k$ & EN $n$ & & Chinese Lemma & ZH $k$ & ZH $n$ \\\\\",\n",
        "        r\"\\midrule\",\n",
        "    ]\n",
        "\n",
        "    for i in range(top_n):\n",
        "        en_row = en_top.iloc[i] if i < len(en_top) else None\n",
        "        zh_row = zh_top.iloc[i] if i < len(zh_top) else None\n",
        "\n",
        "        en_lemma = en_row[\"lemma\"]                  if en_row is not None else \"\"\n",
        "        en_k     = int(en_row[\"k_ward\"])            if en_row is not None else \"\"\n",
        "        en_n     = int(en_row[\"n_occurrences\"])     if en_row is not None else \"\"\n",
        "        zh_lemma = zh_row[\"lemma\"]                  if zh_row is not None else \"\"\n",
        "        zh_k     = int(zh_row[\"k_ward\"])            if zh_row is not None else \"\"\n",
        "        zh_n     = int(zh_row[\"n_occurrences\"])     if zh_row is not None else \"\"\n",
        "\n",
        "        lines.append(\n",
        "            f\"{i+1} & {en_lemma} & {en_k} & {en_n} & & {zh_lemma} & {zh_k} & {zh_n} \\\\\\\\\"\n",
        "        )\n",
        "\n",
        "    lines += [\n",
        "        r\"\\bottomrule\",\n",
        "        r\"\\end{tabular}\",\n",
        "        r\"\\end{table}\",\n",
        "    ]\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "\n",
        "# ─── Wide Comparison Profile ─────────────────────────────────────────────────\n",
        "\n",
        "def generate_comparison_profile() -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Create a summary comparison table for paper Table 2.\n",
        "    \"\"\"\n",
        "    en = pd.read_csv(DATA_DIR / \"english_wsi_results.csv\")\n",
        "    zh = pd.read_csv(DATA_DIR / \"chinese_wsi_results.csv\")\n",
        "\n",
        "    def profile(df: pd.DataFrame, lang: str) -> dict:\n",
        "        return {\n",
        "            \"Language\":            lang,\n",
        "            \"Total lemmas\":        len(df),\n",
        "            \"Mean k (Ward)\":       round(df[\"k_ward\"].mean(), 3),\n",
        "            \"Median k (Ward)\":     round(df[\"k_ward\"].median(), 3),\n",
        "            \"Std k (Ward)\":        round(df[\"k_ward\"].std(ddof=1), 3),\n",
        "            \"% Monosemous (k=1)\":  round((df[\"k_ward\"] == 1).mean() * 100, 1),\n",
        "            \"% Polysemous (k>1)\":  round((df[\"k_ward\"] > 1).mean() * 100, 1),\n",
        "            \"Max k\":               int(df[\"k_ward\"].max()),\n",
        "            \"Mean k (KMeans)\":     round(df[\"k_kmeans\"].mean(), 3),\n",
        "            \"Agreement (Ward=KM)\": round((df[\"k_ward\"] == df[\"k_kmeans\"]).mean() * 100, 1),\n",
        "        }\n",
        "\n",
        "    rows = [profile(en, \"English\"), profile(zh, \"Chinese\")]\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "\n",
        "# ─── Main ─────────────────────────────────────────────────────────────────────\n",
        "\n",
        "# def main():\n",
        "#     print(\"=\" * 60)\n",
        "#     print(\"Step 6: Qualitative Analysis\")\n",
        "#     print(\"=\" * 60)\n",
        "\n",
        "#     # ── Sense examples ────────────────────────────────────────────\n",
        "#     for lang in [\"english\", \"chinese\"]:\n",
        "#         text = get_sense_examples(lang)\n",
        "#         out  = OUTPUT_DIR / f\"qualitative_{lang}_top_polysemous.txt\"\n",
        "#         out.write_text(text, encoding=\"utf-8\")\n",
        "#         print(f\"  [saved] {out.name}\")\n",
        "\n",
        "#     # ── LaTeX table ───────────────────────────────────────────────\n",
        "#     latex = generate_latex_table(top_n=15)\n",
        "#     tex_path = OUTPUT_DIR / \"table_top_polysemous_latex.tex\"\n",
        "#     tex_path.write_text(latex, encoding=\"utf-8\")\n",
        "#     print(f\"  [saved] {tex_path.name}\")\n",
        "\n",
        "#     # ── Comparison profile ────────────────────────────────────────\n",
        "#     profile = generate_comparison_profile()\n",
        "#     csv_path = OUTPUT_DIR / \"polysemy_profile_comparison.csv\"\n",
        "#     profile.to_csv(csv_path, index=False)\n",
        "#     print(f\"  [saved] {csv_path.name}\")\n",
        "#     print()\n",
        "#     print(profile.to_string(index=False))\n",
        "\n",
        "#     print(\"\\n✓ Step 6 complete.\\n\")\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()"
      ],
      "metadata": {
        "id": "d6Rm4f_vdq7A"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "\n",
        "# ─── Configuration ────────────────────────────────────────────────────────────\n",
        "\n",
        "# DATA_DIR and OUTPUT_DIR are defined in previous cells and are globally accessible.\n",
        "# For example, they are set in cell d6Rm4f_vdq7A.\n",
        "# DATA_DIR    = Path(\"/content\") / \"bible_data\"\n",
        "# OUTPUT_DIR = Path(\"/content\") / \"output\"\n",
        "# OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "TOP_N_WORDS       = 20   # Top N most polysemous lemmas per language\n",
        "EXAMPLES_PER_SENSE = 2   # Number of example verses per cluster\n",
        "\n",
        "# ─── Sense Example Retrieval (Corrected) ─────────────────────────────────────────────────\n",
        "\n",
        "def get_sense_examples(\n",
        "    lang: str,\n",
        "    top_n: int = TOP_N_WORDS,\n",
        "    examples_per_sense: int = EXAMPLES_PER_SENSE,\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    For the top_n most polysemous lemmas, retrieve example contexts\n",
        "    for each induced sense cluster.\n",
        "\n",
        "    Returns a formatted string ready for a paper's qualitative appendix.\n",
        "    \"\"\"\n",
        "    results_df = pd.read_csv(DATA_DIR / f\"{lang}_wsi_results.csv\")\n",
        "    labels_df  = pd.read_csv(DATA_DIR / f\"{lang}_sense_labels.csv\")\n",
        "    nouns_df   = pd.read_csv(DATA_DIR / f\"{lang}_nouns.csv\")\n",
        "\n",
        "    # Top polysemous by k_ward\n",
        "    poly = results_df[results_df[\"k_ward\"] > 1].nlargest(top_n, \"k_ward\")\n",
        "\n",
        "    # Merge labels with original contexts\n",
        "    merged = labels_df.merge(\n",
        "        nouns_df[[\"lemma\", \"verse_id\", \"context\"]].drop_duplicates(),\n",
        "        on=[\"lemma\", \"verse_id\"],\n",
        "        how=\"left\",\n",
        "    )\n",
        "\n",
        "    lines = []\n",
        "    lines.append(f\"{'='*60}\")\n",
        "    lines.append(f\"QUALITATIVE SENSE ANALYSIS — {lang.upper()}\")\n",
        "    lines.append(f\"Top {top_n} Most Polysemous Nouns (WSI, Ward Clustering)\")\n",
        "    lines.append(f\"{'='*60}\\n\")\n",
        "\n",
        "    for _, row in poly.iterrows():\n",
        "        lemma = row[\"lemma\"]\n",
        "        k     = int(row[\"k_ward\"])\n",
        "        # Corrected column name from 'n_occurrences' to 'n_instances'\n",
        "        n_occ = int(row[\"n_instances\"])\n",
        "        sil   = row.get(\"silhouette_ward\", \"N/A\")\n",
        "\n",
        "        lines.append(f\"Lemma: '{lemma}'  |  k={k}  |  n={n_occ}  |  silhouette={sil}\")\n",
        "        lines.append(\"-\" * 50)\n",
        "\n",
        "        lemma_data = merged[merged[\"lemma\"] == lemma]\n",
        "\n",
        "        for cluster_id in range(k):\n",
        "            cluster_rows = lemma_data[lemma_data[\"cluster_ward\"] == cluster_id]\n",
        "            lines.append(f\"  Sense {cluster_id + 1} ({len(cluster_rows)} occurrences):\")\n",
        "\n",
        "            # Sample diverse examples\n",
        "            sample = cluster_rows.dropna(subset=[\"context\"]).head(examples_per_sense)\n",
        "            for _, ex in sample.iterrows():\n",
        "                ctx = str(ex[\"context\"])[:200].replace(\"\\n\", \" \")\n",
        "                lines.append(f\"    • {ex['verse_id']}: {ctx}\")\n",
        "\n",
        "        lines.append(\"\")\n",
        "\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "\n",
        "# ─── LaTeX Table Generation (Corrected) ──────────────────────────────────────────────────\n",
        "\n",
        "def generate_latex_table(top_n: int = 15) -> str:\n",
        "    \"\"\"\n",
        "    Generate a LaTeX longtable comparing top polysemous nouns in both languages.\n",
        "    Format:\n",
        "      Rank | English Lemma | EN k | Chinese Lemma | ZH k\n",
        "    \"\"\"\n",
        "    en = pd.read_csv(DATA_DIR / \"english_wsi_results.csv\")\n",
        "    zh = pd.read_csv(DATA_DIR / \"chinese_wsi_results.csv\")\n",
        "\n",
        "    # Corrected column name from 'n_occurrences' to 'n_instances'\n",
        "    en_top = en.nlargest(top_n, \"k_ward\")[[\"lemma\", \"k_ward\", \"n_instances\"]].reset_index(drop=True)\n",
        "    zh_top = zh.nlargest(top_n, \"k_ward\")[[\"lemma\", \"k_ward\", \"n_instances\"]].reset_index(drop=True)\n",
        "\n",
        "    lines = [\n",
        "        r\"\\begin{table}[h]\",\n",
        "        r\"\\centering\",\n",
        "        r\"\\caption{Top Polysemous Common Nouns by Induced Sense Count (k): English vs. Chinese}\",\n",
        "        r\"\\label{tab:top_polysemous}\",\n",
        "        r\"\\begin{tabular}{clccclcc}\",\n",
        "        r\"\\toprule\",\n",
        "        r\"Rank & English Lemma & EN $k$ & EN $n$ & & Chinese Lemma & ZH $k$ & ZH $n$ \\\\\",\n",
        "        r\"\\midrule\",\n",
        "    ]\n",
        "\n",
        "    for i in range(top_n):\n",
        "        en_row = en_top.iloc[i] if i < len(en_top) else None\n",
        "        zh_row = zh_top.iloc[i] if i < len(zh_top) else None\n",
        "\n",
        "        en_lemma = en_row[\"lemma\"]                  if en_row is not None else \"\"\n",
        "        en_k     = int(en_row[\"k_ward\"])            if en_row is not None else \"\"\n",
        "        # Corrected column name from 'n_occurrences' to 'n_instances'\n",
        "        en_n     = int(en_row[\"n_instances\"])     if en_row is not None else \"\"\n",
        "        zh_lemma = zh_row[\"lemma\"]                  if zh_row is not None else \"\"\n",
        "        zh_k     = int(zh_row[\"k_ward\"])            if zh_row is not None else \"\"\n",
        "        # Corrected column name from 'n_occurrences' to 'n_instances'\n",
        "        zh_n     = int(zh_row[\"n_instances\"])     if zh_row is not None else \"\"\n",
        "\n",
        "        lines.append(\n",
        "            f\"{i+1} & {en_lemma} & {en_k} & {en_n} & & {zh_lemma} & {zh_k} & {zh_n} \\\\\"\n",
        "        )\n",
        "\n",
        "    lines += [\n",
        "        r\"\\bottomrule\",\n",
        "        r\"\\end{tabular}\",\n",
        "        r\"\\end{table}\",\n",
        "    ]\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "# ─── Wide Comparison Profile (Copied for context) ─────────────────────────────────────────────────\n",
        "\n",
        "def generate_comparison_profile() -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Create a summary comparison table for paper Table 2.\n",
        "    \"\"\"\n",
        "    en = pd.read_csv(DATA_DIR / \"english_wsi_results.csv\")\n",
        "    zh = pd.read_csv(DATA_DIR / \"chinese_wsi_results.csv\")\n",
        "\n",
        "    def profile(df: pd.DataFrame, lang: str) -> dict:\n",
        "        return {\n",
        "            \"Language\":            lang,\n",
        "            \"Total lemmas\":        len(df),\n",
        "            \"Mean k (Ward)\":       round(df[\"k_ward\"].mean(), 3),\n",
        "            \"Median k (Ward)\":     round(df[\"k_ward\"].median(), 3),\n",
        "            \"Std k (Ward)\":        round(df[\"k_ward\"].std(ddof=1), 3),\n",
        "            \"% Monosemous (k=1)\":  round((df[\"k_ward\"] == 1).mean() * 100, 1),\n",
        "            \"% Polysemous (k>1)\":  round((df[\"k_ward\"] > 1).mean() * 100, 1),\n",
        "            \"Max k\":               int(df[\"k_ward\"].max()),\n",
        "            \"Mean k (KMeans)\":     round(df[\"k_kmeans\"].mean(), 3),\n",
        "            \"Agreement (Ward=KM)\": round((df[\"k_ward\"] == df[\"k_kmeans\"]).mean() * 100, 1),\n",
        "        }\n",
        "\n",
        "    rows = [profile(en, \"English\"), profile(zh, \"Chinese\")]\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Step 6: Qualitative Analysis\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ── Sense examples ────────────────────────────────────────────\n",
        "for lang in [\"english\", \"chinese\"]:\n",
        "    text = get_sense_examples(lang)\n",
        "    out  = OUTPUT_DIR / f\"qualitative_{lang}_top_polysemous.txt\"\n",
        "    out.write_text(text, encoding=\"utf-8\")\n",
        "    print(f\"  [saved] {out.name}\")\n",
        "\n",
        "# ── LaTeX table ───────────────────────────────────────────────\n",
        "latex = generate_latex_table(top_n=15)\n",
        "tex_path = OUTPUT_DIR / \"table_top_polysemous_latex.tex\"\n",
        "tex_path.write_text(latex, encoding=\"utf-8\")\n",
        "print(f\"  [saved] {tex_path.name}\")\n",
        "\n",
        "# ── Comparison profile ────────────────────────────────────────\n",
        "profile = generate_comparison_profile()\n",
        "csv_path = OUTPUT_DIR / \"polysemy_profile_comparison.csv\"\n",
        "profile.to_csv(csv_path, index=False)\n",
        "print(f\"  [saved] {csv_path.name}\")\n",
        "print()\n",
        "print(profile.to_string(index=False))\n",
        "\n",
        "print(\"\\n✓ Step 6 complete.\\n\")\n"
      ],
      "metadata": {
        "id": "K9Gd4DVWeOYw",
        "outputId": "25e0f912-40ea-4776-e311-19e9e3a15b06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Step 6: Qualitative Analysis\n",
            "============================================================\n",
            "  [saved] qualitative_english_top_polysemous.txt\n",
            "  [saved] qualitative_chinese_top_polysemous.txt\n",
            "  [saved] table_top_polysemous_latex.tex\n",
            "  [saved] polysemy_profile_comparison.csv\n",
            "\n",
            "Language  Total lemmas  Mean k (Ward)  Median k (Ward)  Std k (Ward)  % Monosemous (k=1)  % Polysemous (k>1)  Max k  Mean k (KMeans)  Agreement (Ward=KM)\n",
            " English           636          2.126              2.0         1.572                39.6                60.4      8            2.104                 92.5\n",
            " Chinese           572          2.136              2.0         1.769                47.7                52.3      8            2.131                 91.4\n",
            "\n",
            "✓ Step 6 complete.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r output_ot-nt.zip /content/output/"
      ],
      "metadata": {
        "id": "wnUe-3lgedhv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r data_niv_cuv_ot-nt.zip /content/bible_data/"
      ],
      "metadata": {
        "id": "Pj8aPu8Cfp1f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}