{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c07b74d5d48845cbac6a1fb50b2a778f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f4dc1318baa9443cac4bb23b3349f4af",
              "IPY_MODEL_eedc270216c94bdb9fd0b75829fe29d0",
              "IPY_MODEL_8a41ecd831f34d8ba4235d792e5dd3b6"
            ],
            "layout": "IPY_MODEL_ce56a9109291444f845ae7bbba63ec7f"
          }
        },
        "f4dc1318baa9443cac4bb23b3349f4af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b7f8e26f310a4b3289795438e60e783c",
            "placeholder": "​",
            "style": "IPY_MODEL_6604615d85f24ac9b7ec04e0b8e57b8f",
            "value": "config.json: 100%"
          }
        },
        "eedc270216c94bdb9fd0b75829fe29d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5994cfa868254ac38d6849e744829974",
            "max": 615,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_419a5bb6e054413fa3fc07e8028dd32f",
            "value": 615
          }
        },
        "8a41ecd831f34d8ba4235d792e5dd3b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f4463bba1234ccca814d7bbdf96a888",
            "placeholder": "​",
            "style": "IPY_MODEL_07407c9708584b939c0f9eb5f58297c2",
            "value": " 615/615 [00:00&lt;00:00, 70.0kB/s]"
          }
        },
        "ce56a9109291444f845ae7bbba63ec7f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7f8e26f310a4b3289795438e60e783c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6604615d85f24ac9b7ec04e0b8e57b8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5994cfa868254ac38d6849e744829974": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "419a5bb6e054413fa3fc07e8028dd32f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6f4463bba1234ccca814d7bbdf96a888": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07407c9708584b939c0f9eb5f58297c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b93166c752cf44689ca60c91e03c0171": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a7d825ed7e114ab1867571a335b01aec",
              "IPY_MODEL_f768cf03dd6a4d98952de94f51fac834",
              "IPY_MODEL_d8b2dd9fefa440aa95143669e1fca025"
            ],
            "layout": "IPY_MODEL_5aa508402a9541809e25b45f71583ba8"
          }
        },
        "a7d825ed7e114ab1867571a335b01aec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_50dd69b6e6954295899ec481cb04dc27",
            "placeholder": "​",
            "style": "IPY_MODEL_e7a8f3d54c13478093fdeebc7757a984",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "f768cf03dd6a4d98952de94f51fac834": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e54c6b472b0c44bf95ee74b393316beb",
            "max": 25,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3402c3d2bca54014b31e9e16d3351252",
            "value": 25
          }
        },
        "d8b2dd9fefa440aa95143669e1fca025": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d83a432cc79f4a00a6d33cbceab0cec6",
            "placeholder": "​",
            "style": "IPY_MODEL_cdd9e93d463c46e99b77ae3268e3d993",
            "value": " 25.0/25.0 [00:00&lt;00:00, 3.12kB/s]"
          }
        },
        "5aa508402a9541809e25b45f71583ba8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50dd69b6e6954295899ec481cb04dc27": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7a8f3d54c13478093fdeebc7757a984": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e54c6b472b0c44bf95ee74b393316beb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3402c3d2bca54014b31e9e16d3351252": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d83a432cc79f4a00a6d33cbceab0cec6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cdd9e93d463c46e99b77ae3268e3d993": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d484bb3954e84d1da9b195028999d3a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_19f3dd01454f489fbfda2e42e80a652a",
              "IPY_MODEL_47d0729b78e24c1cbf953cd52b05503f",
              "IPY_MODEL_a6e3323d8c5a4af08b3555b66f48699e"
            ],
            "layout": "IPY_MODEL_f53aee84d3c34581a2b4aecdbe1f9582"
          }
        },
        "19f3dd01454f489fbfda2e42e80a652a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d017ab2cdd424678838d1d22ce19723d",
            "placeholder": "​",
            "style": "IPY_MODEL_367ba03aee4c4db1a7edd731fa1648b3",
            "value": "sentencepiece.bpe.model: 100%"
          }
        },
        "47d0729b78e24c1cbf953cd52b05503f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0cfd063f8bb7459596a95fbd700d5399",
            "max": 5069051,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9a8d7fc8581d4bbd866a6bdaf191d521",
            "value": 5069051
          }
        },
        "a6e3323d8c5a4af08b3555b66f48699e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_092f14d00da246d6868c42bed94f9d63",
            "placeholder": "​",
            "style": "IPY_MODEL_4a1be9e86e0a44bda0cf21a9d40d059c",
            "value": " 5.07M/5.07M [00:00&lt;00:00, 16.1MB/s]"
          }
        },
        "f53aee84d3c34581a2b4aecdbe1f9582": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d017ab2cdd424678838d1d22ce19723d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "367ba03aee4c4db1a7edd731fa1648b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0cfd063f8bb7459596a95fbd700d5399": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a8d7fc8581d4bbd866a6bdaf191d521": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "092f14d00da246d6868c42bed94f9d63": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a1be9e86e0a44bda0cf21a9d40d059c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d75d123cc8434702b949444f6b986e6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_13f6108d817b4cfe91d127ab8b935698",
              "IPY_MODEL_1642f3053fe64c6393d451410b2591e2",
              "IPY_MODEL_9c247ebbf33f411aa3f067ba63d9f6b4"
            ],
            "layout": "IPY_MODEL_91bd5dab7c41401db92bf4614ea394aa"
          }
        },
        "13f6108d817b4cfe91d127ab8b935698": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a34d1b4264c14f29a8b1f10092d5cd50",
            "placeholder": "​",
            "style": "IPY_MODEL_a42ea110cd794cb5a9b2ec1aa7179f49",
            "value": "tokenizer.json: 100%"
          }
        },
        "1642f3053fe64c6393d451410b2591e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4451d51e84a44e46a884bac416ab4a70",
            "max": 9096718,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_27a88156fa064722aecb009b23099b40",
            "value": 9096718
          }
        },
        "9c247ebbf33f411aa3f067ba63d9f6b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bfb2e2ec2f1146de969468e7d49db324",
            "placeholder": "​",
            "style": "IPY_MODEL_7a20089e47f842b78cd4d4edf6c79c03",
            "value": " 9.10M/9.10M [00:00&lt;00:00, 22.5MB/s]"
          }
        },
        "91bd5dab7c41401db92bf4614ea394aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a34d1b4264c14f29a8b1f10092d5cd50": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a42ea110cd794cb5a9b2ec1aa7179f49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4451d51e84a44e46a884bac416ab4a70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27a88156fa064722aecb009b23099b40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bfb2e2ec2f1146de969468e7d49db324": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a20089e47f842b78cd4d4edf6c79c03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "84959ac020824f22bcb975cc36f2a9b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c66860a6ee2b4c0bae1ef9539417469d",
              "IPY_MODEL_08811ab1e9fd4d858ff4e279efef3642",
              "IPY_MODEL_f58b31ee598748469ad4c34522a8f311"
            ],
            "layout": "IPY_MODEL_e1d51a3f34de424ab5b0607faec77807"
          }
        },
        "c66860a6ee2b4c0bae1ef9539417469d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e82f53fd62d40558ccc08cae5c8aa50",
            "placeholder": "​",
            "style": "IPY_MODEL_97dec1bc8dc24ddeb408390302cfa2d2",
            "value": "model.safetensors: 100%"
          }
        },
        "08811ab1e9fd4d858ff4e279efef3642": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1527ee104c494dabbba9a292d3d320a9",
            "max": 1115567652,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3120009ddce44f50985b12901c5b640d",
            "value": 1115567652
          }
        },
        "f58b31ee598748469ad4c34522a8f311": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45f62729f36146d5af88915d959d3616",
            "placeholder": "​",
            "style": "IPY_MODEL_eb6a748521014e6aaa03394b0f59991b",
            "value": " 1.12G/1.12G [00:02&lt;00:00, 784MB/s]"
          }
        },
        "e1d51a3f34de424ab5b0607faec77807": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e82f53fd62d40558ccc08cae5c8aa50": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97dec1bc8dc24ddeb408390302cfa2d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1527ee104c494dabbba9a292d3d320a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3120009ddce44f50985b12901c5b640d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "45f62729f36146d5af88915d959d3616": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb6a748521014e6aaa03394b0f59991b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d79a1159cd4f4bcca284a18bc3a50f65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_db7a03c769834a6288afd509e7ed36fc",
              "IPY_MODEL_de6b1a88be834a89a8cb6d54a383c4b9",
              "IPY_MODEL_8729b4748cd54a049b6d4a590930bf64"
            ],
            "layout": "IPY_MODEL_be83b8017f4c43dcb7ea11b9f0c1549e"
          }
        },
        "db7a03c769834a6288afd509e7ed36fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3de547ffb30a4c6aa69c2b3c402aa4aa",
            "placeholder": "​",
            "style": "IPY_MODEL_ae97b302b9594058bccffd2d41e54675",
            "value": "Loading weights: 100%"
          }
        },
        "de6b1a88be834a89a8cb6d54a383c4b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2af5b2683eb349c3989d7edb27daea8f",
            "max": 199,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4109151e0f054c4cb59d4964f77715e7",
            "value": 199
          }
        },
        "8729b4748cd54a049b6d4a590930bf64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac0d87bf58d7437697aba8fc38ca855a",
            "placeholder": "​",
            "style": "IPY_MODEL_d93ec0930ca843a4b8a81b722264177a",
            "value": " 199/199 [00:00&lt;00:00, 997.51it/s, Materializing param=pooler.dense.weight]"
          }
        },
        "be83b8017f4c43dcb7ea11b9f0c1549e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3de547ffb30a4c6aa69c2b3c402aa4aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae97b302b9594058bccffd2d41e54675": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2af5b2683eb349c3989d7edb27daea8f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4109151e0f054c4cb59d4964f77715e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ac0d87bf58d7437697aba8fc38ca855a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d93ec0930ca843a4b8a81b722264177a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cs-iuu/word-sense-2025-fall-ai/blob/main/notebooks/15.1.wsi_niv_cht_hdbscan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Preprocessing: Extract Common Nouns"
      ],
      "metadata": {
        "id": "OwnCnpqk0M7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy jieba pandas --break-system-packages\n",
        "!python -m spacy download en_core_web_sm\n",
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ZDh6BuKL3z-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch transformers numpy pandas --break-system-packages"
      ],
      "metadata": {
        "id": "1uzzTQ5ogE7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn numpy pandas --break-system-packages"
      ],
      "metadata": {
        "id": "SATWTZIEgLXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk scipy matplotlib seaborn pandas numpy --break-system-packages"
      ],
      "metadata": {
        "id": "p8FfnACQgObB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install hdbscan"
      ],
      "metadata": {
        "id": "2FohS4GdIdxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Step 2: Preprocessing\n",
        "======================\n",
        "Tokenizes, POS-tags, and lemmatizes both corpora.\n",
        "Extracts common nouns only (no proper nouns, pronouns, or stopwords).\n",
        "Applies a minimum frequency threshold to filter rare words.\n",
        "\n",
        "Outputs:\n",
        "  - data/english_nouns.csv   : lemma, verse_id, token, context (full verse)\n",
        "  - data/chinese_nouns.csv   : lemma, verse_id, token, context\n",
        "  - data/english_noun_freq.csv\n",
        "  - data/chinese_noun_freq.csv\n",
        "\n",
        "Usage:\n",
        "  pip install spacy jieba pandas --break-system-packages\n",
        "  python -m spacy download en_core_web_sm\n",
        "  python 02_preprocessing.py\n",
        "\n",
        "Design decisions (paper §3.2):\n",
        "  - English: spaCy en_core_web_sm for tokenization, POS, lemmatization\n",
        "  - Chinese: jieba for word segmentation + custom POS (jieba.posseg)\n",
        "  - POS filters: English NOUN tag; Chinese POS prefix 'n' (common noun)\n",
        "  - Proper noun exclusion: English PROPN tag excluded; Chinese 'nr','ns','nt','nz' excluded\n",
        "  - Minimum frequency: MIN_FREQ = 30 (ensures sufficient WSI context)\n",
        "  - Stopwords: NLTK English stopwords; custom Chinese stopword list\n",
        "\"\"\"\n",
        "\n",
        "import re\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "\n",
        "# ─── Configuration ────────────────────────────────────────────────────────────\n",
        "\n",
        "DATA_DIR = Path(\"/content\") / \"bible_data\"\n",
        "MIN_FREQ = 30          # Minimum occurrences per lemma for WSI\n",
        "MAX_CONTEXT_LEN = 512  # Characters — prevents overlong inputs to transformers\n",
        "\n",
        "# Chinese POS tags for common nouns (jieba.posseg notation)\n",
        "ZH_NOUN_PREFIXES = {\"n\"}           # Common noun prefix\n",
        "ZH_EXCLUDE_TAGS  = {\"nr\", \"ns\", \"nt\", \"nz\", \"nw\"}  # Proper nouns to exclude\n",
        "\n",
        "# ── Theological proper noun exclusion lists ───────────────────────────────────\n",
        "# These terms are proper nouns in English (God, Lord, Christ etc.) — excluded\n",
        "# by spaCy's PROPN tag — but are tagged as common nouns n by jieba in Chinese\n",
        "# due to the absence of capitalisation. They must be excluded explicitly from\n",
        "# the Chinese data to ensure cross-lingual comparability.\n",
        "#\n",
        "# English side: spaCy correctly tags God/Lord/Christ as PROPN (excluded).\n",
        "# Exception: \"Spirit\" (Holy Spirit) is sometimes tagged NOUN by spaCy, so it\n",
        "# is added to EN_THEOLOGICAL_EXCLUDE as a lemma-level backstop.\n",
        "#\n",
        "# Borderline cases kept in both languages:\n",
        "#   先知/prophet  — generic occupational noun, polysemous, common in both\n",
        "#   天使/angel    — generic supernatural being, common noun in both\n",
        "#   魔鬼/devil    — common noun in EN; jieba tags n in ZH\n",
        "\n",
        "ZH_THEOLOGICAL_EXCLUDE = {\n",
        "    # Core deity names / titles\n",
        "    \"神\",     # God (most frequent — 1244 occurrences)\n",
        "    \"主\",     # Lord\n",
        "    \"上帝\",   # God (formal)\n",
        "    \"耶和華\", # Yahweh / LORD\n",
        "    \"基督\",   # Christ\n",
        "    \"耶穌\",   # Jesus (also usually tagged nr, but belt-and-suspenders)\n",
        "    \"聖靈\",   # Holy Spirit\n",
        "    \"聖神\",   # Holy Spirit (alternate form in some CUV editions)\n",
        "    \"彌賽亞\", # Messiah\n",
        "    # Adversarial proper nouns\n",
        "    \"撒但\",   # Satan\n",
        "    \"別西卜\", # Beelzebub\n",
        "}\n",
        "\n",
        "EN_THEOLOGICAL_EXCLUDE = {\n",
        "    # Lemma-level backstop for cases where spaCy tags as NOUN not PROPN\n",
        "    \"spirit\",    # \"Holy Spirit\" — spaCy inconsistently tags as NOUN\n",
        "    \"ghost\",     # \"Holy Ghost\" (KJV form; rare in NIV but present)\n",
        "}\n",
        "\n",
        "# Path to custom jieba dictionary for biblical proper names\n",
        "JIEBA_DICT_PATH = Path(\"/content\") / \"bible_data\" / \"jieba_biblical_dict.txt\"\n",
        "\n",
        "# ── Post-segmentation POS correction ─────────────────────────────────────────\n",
        "# jieba's POS tagger runs independently of the segmentation dictionary and\n",
        "# can assign incorrect tags even for dictionary entries. These words are\n",
        "# forced to tag n after segmentation regardless of what the POS tagger assigned.\n",
        "#\n",
        "# 地: jieba assigns uv (虛詞/copular particle) in classical subject-predicate\n",
        "#     constructions like 地是空虛混沌 (Gen.1.2) because it parses 地 as a\n",
        "#     topic marker rather than a subject noun. This is a known jieba limitation\n",
        "#     with literary Chinese. Since English \"earth\" (freq=739) is always tagged\n",
        "#     NOUN by spaCy, forcing 地 to n is required for cross-lingual comparability.\n",
        "ZH_FORCE_NOUN_TAG = {\n",
        "    \"地\",   # earth/ground/land — incorrectly tagged uv in copular constructions\n",
        "}\n",
        "\n",
        "# ─── English Preprocessing ────────────────────────────────────────────────────\n",
        "\n",
        "\n",
        "def preprocess_english(verse_csv: Path) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Process English verses with spaCy.\n",
        "    Returns long-format DataFrame: one row per noun occurrence.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        import spacy\n",
        "    except ImportError:\n",
        "        raise ImportError(\"Run: pip install spacy --break-system-packages && python -m spacy download en_core_web_sm\")\n",
        "\n",
        "    print(\"  [EN] Loading spaCy model…\")\n",
        "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"textcat\"])\n",
        "\n",
        "    df = pd.read_csv(verse_csv)\n",
        "    print(f\"  [EN] Processing {len(df):,} verses…\")\n",
        "\n",
        "    import time\n",
        "    records   = []\n",
        "    texts     = df[\"text\"].tolist()\n",
        "    verse_ids = df[\"verse_id\"].tolist()\n",
        "    total     = len(texts)\n",
        "    t0        = time.time()\n",
        "\n",
        "    for i, (doc, vid) in enumerate(zip(nlp.pipe(texts, batch_size=512), verse_ids), 1):\n",
        "        context = doc.text[:MAX_CONTEXT_LEN]\n",
        "        for token in doc:\n",
        "            # Keep only common nouns; exclude proper nouns and pronouns\n",
        "            if (\n",
        "                token.pos_ == \"NOUN\"\n",
        "                and not token.is_stop\n",
        "                and not token.is_punct\n",
        "                and len(token.lemma_) > 1\n",
        "                and token.lemma_.isalpha()\n",
        "                and token.lemma_.lower() not in EN_THEOLOGICAL_EXCLUDE\n",
        "            ):\n",
        "                records.append({\n",
        "                    \"verse_id\": vid,\n",
        "                    \"token\":    token.text,\n",
        "                    \"lemma\":    token.lemma_.lower(),\n",
        "                    \"context\":  context,\n",
        "                })\n",
        "        if i % 1000 == 0 or i == total:\n",
        "            elapsed = time.time() - t0\n",
        "            rate    = i / elapsed if elapsed > 0 else 0\n",
        "            eta_min = (total - i) / rate / 60 if rate > 0 else 0\n",
        "            print(f\"    … {i:,}/{total:,} verses  \"\n",
        "                  f\"({rate:.0f} v/s)  \"\n",
        "                  f\"ETA {eta_min:.1f} min  \"\n",
        "                  f\"nouns so far: {len(records):,}\",\n",
        "                  end=\"\\r\")\n",
        "\n",
        "    elapsed_total = time.time() - t0\n",
        "    print(f\"\\n  [EN] Done in {elapsed_total/60:.1f} min. \"\n",
        "          f\"Extracted {len(records):,} noun occurrences.\")\n",
        "    return pd.DataFrame(records)\n",
        "\n",
        "\n",
        "# ─── Chinese Preprocessing ────────────────────────────────────────────────────\n",
        "\n",
        "\n",
        "def preprocess_chinese(verse_csv: Path) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Process Chinese verses with jieba (word segmentation + POS tagging).\n",
        "    Returns long-format DataFrame: one row per noun occurrence.\n",
        "    \"\"\"\n",
        "    import time\n",
        "    try:\n",
        "        import jieba\n",
        "        import jieba.posseg as pseg\n",
        "    except ImportError:\n",
        "        raise ImportError(\"Run: pip install jieba --break-system-packages\")\n",
        "\n",
        "    # Silence jieba logging FIRST, before any other jieba calls\n",
        "    jieba.setLogLevel(\"ERROR\")\n",
        "\n",
        "    # Load custom dictionary\n",
        "    if JIEBA_DICT_PATH.exists():\n",
        "        jieba.load_userdict(str(JIEBA_DICT_PATH))\n",
        "        print(f\"  [ZH] Loaded custom dictionary: {JIEBA_DICT_PATH.name}\", flush=True)\n",
        "    else:\n",
        "        print(f\"  [ZH] WARNING: custom dictionary not found at {JIEBA_DICT_PATH}\", flush=True)\n",
        "\n",
        "    # Force jieba to build its internal trie NOW with a visible message.\n",
        "    # Without this explicit call, the first pseg.cut() silently blocks\n",
        "    # for 30-60 seconds before any progress lines appear.\n",
        "    print(\"  [ZH] Building jieba model (one-time, ~10-30s)...\", flush=True)\n",
        "    jieba.initialize()\n",
        "    print(\"  [ZH] Model ready.\", flush=True)\n",
        "\n",
        "    zh_stopwords = _load_chinese_stopwords()\n",
        "\n",
        "    df    = pd.read_csv(verse_csv)\n",
        "    total = len(df)\n",
        "    print(f\"  [ZH] Processing {total:,} verses...\", flush=True)\n",
        "    t0 = time.time()\n",
        "\n",
        "    records = []\n",
        "    for i, row in enumerate(df.itertuples(index=False), 1):\n",
        "        verse_id = row.verse_id\n",
        "        text     = str(row.text)\n",
        "        context  = text[:MAX_CONTEXT_LEN]\n",
        "\n",
        "        for word, flag in pseg.cut(text):\n",
        "            flag_str = str(flag)\n",
        "            # Force known mis-tagged tokens to correct POS\n",
        "            if word in ZH_FORCE_NOUN_TAG:\n",
        "                flag_str = \"n\"\n",
        "            if (\n",
        "                flag_str[:1] in ZH_NOUN_PREFIXES\n",
        "                and flag_str not in ZH_EXCLUDE_TAGS\n",
        "                and word not in zh_stopwords\n",
        "                and word not in ZH_THEOLOGICAL_EXCLUDE\n",
        "                and len(word) >= 1\n",
        "                and not word.isdigit()\n",
        "            ):\n",
        "                records.append({\n",
        "                    \"verse_id\": verse_id,\n",
        "                    \"token\":    word,\n",
        "                    \"lemma\":    word,\n",
        "                    \"context\":  context,\n",
        "                })\n",
        "\n",
        "        # Progress every 100 verses — print on new lines so nothing is missed\n",
        "        if i % 100 == 0 or i == total:\n",
        "            elapsed = time.time() - t0\n",
        "            rate    = i / elapsed if elapsed > 0 else 0\n",
        "            eta_min = (total - i) / rate / 60 if rate > 0 else 0\n",
        "            print(\n",
        "                f\"    ... {i:,}/{total:,} verses\"\n",
        "                f\"  ({rate:.0f} v/s)\"\n",
        "                f\"  ETA {eta_min:.1f} min\"\n",
        "                f\"  nouns: {len(records):,}\",\n",
        "                flush=True\n",
        "            )\n",
        "\n",
        "    elapsed_total = time.time() - t0\n",
        "    print(f\"  [ZH] Done in {elapsed_total/60:.1f} min. \"\n",
        "          f\"Extracted {len(records):,} noun occurrences.\")\n",
        "    return pd.DataFrame(records)\n",
        "\n",
        "def _load_chinese_stopwords() -> set:\n",
        "    \"\"\"\n",
        "    Chinese function word stoplist for CUV Traditional (CHT) text.\n",
        "\n",
        "    Design decisions:\n",
        "    ─────────────────────────────────────────────────────────────\n",
        "    1. CHT variants included alongside CHS equivalents for all\n",
        "       characters that differ between scripts (說/说, 會/会, etc.)\n",
        "\n",
        "    2. 人 is NOT a stopword. It is a genuine common noun meaning\n",
        "       \"person / people / man / humanity\" and is highly polysemous\n",
        "       in biblical text. Jieba tags it as n (common noun) in most\n",
        "       contexts, so it passes the POS filter correctly. Removing it\n",
        "       would discard one of the most semantically rich words in the\n",
        "       corpus.\n",
        "\n",
        "    3. Pronouns (他/她/祂/你/我 etc.) are NOT listed here. They are\n",
        "       tagged by jieba as r (pronoun), which is already excluded by\n",
        "       the POS filter (we keep only n* tags). Listing them would be\n",
        "       redundant. The various gendered and honorific variants\n",
        "       (他/她/它/祂) all carry the r tag and are excluded uniformly.\n",
        "\n",
        "    4. This list covers only high-frequency grammatical function\n",
        "       words that jieba may occasionally mis-tag as nouns.\n",
        "       It is intentionally conservative.\n",
        "    ─────────────────────────────────────────────────────────────\n",
        "    \"\"\"\n",
        "    return {\n",
        "        # Structural particles (occasionally mis-tagged as n by jieba)\n",
        "        # NOTE: 地 is intentionally NOT listed here.\n",
        "        # In CUV literary style, 地 is overwhelmingly used as a noun\n",
        "        # (earth/land/ground) matching English \"earth\" (freq=739).\n",
        "        # The adverbial particle use of 地 is rare in classical biblical text.\n",
        "        # Removing it would create an asymmetry with English where \"earth\"\n",
        "        # is correctly retained as a high-frequency common noun.\n",
        "        \"的\", \"得\",\n",
        "        # Aspect markers — CHT: 著, CHS: 着\n",
        "        \"了\", \"著\", \"着\",\n",
        "        # Conjunctions / connectives\n",
        "        \"和\", \"與\", \"与\", \"及\", \"或\", \"但\", \"而\", \"且\",\n",
        "        # Adverbs sometimes mis-tagged\n",
        "        \"也\", \"都\", \"就\", \"才\", \"又\", \"還\", \"还\", \"已\",\n",
        "        \"很\", \"更\", \"最\", \"太\", \"非常\",\n",
        "        # Negation\n",
        "        \"不\", \"沒有\", \"没有\", \"未\", \"無\", \"无\",\n",
        "        # Existential / copular\n",
        "        \"是\", \"有\", \"在\",\n",
        "        # Determiners / quantifiers\n",
        "        \"一\", \"這\", \"这\", \"那\", \"各\", \"每\", \"某\", \"其\",\n",
        "        # Directional / locative words with no sense variation\n",
        "        \"上\", \"下\", \"中\", \"內\", \"内\", \"外\", \"前\", \"後\", \"后\",\n",
        "        \"裡\", \"里\", \"間\", \"间\",\n",
        "        # Common verbs jieba occasionally tags as nouns in CUV\n",
        "        \"說\", \"说\", \"看\", \"去\", \"來\", \"来\", \"到\", \"給\", \"给\",\n",
        "        \"要\", \"會\", \"会\",\n",
        "    }\n",
        "\n",
        "\n",
        "# ─── Frequency Filtering ──────────────────────────────────────────────────────\n",
        "\n",
        "\n",
        "def apply_frequency_filter(df: pd.DataFrame, min_freq: int = MIN_FREQ) -> tuple:\n",
        "    \"\"\"\n",
        "    Keep only lemmas appearing at least `min_freq` times.\n",
        "    Returns (filtered_df, freq_df).\n",
        "    \"\"\"\n",
        "    freq = df.groupby(\"lemma\").size().reset_index(name=\"count\")\n",
        "    freq = freq.sort_values(\"count\", ascending=False)\n",
        "    valid_lemmas = set(freq[freq[\"count\"] >= min_freq][\"lemma\"])\n",
        "    filtered = df[df[\"lemma\"].isin(valid_lemmas)].copy()\n",
        "    return filtered, freq"
      ],
      "metadata": {
        "id": "iApWnLav0Ltb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"Step 2: Preprocessing\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ── English ──────────────────────────────────────────────────\n",
        "en_raw = preprocess_english(DATA_DIR / \"english_verses.csv\")\n",
        "en_filtered, en_freq = apply_frequency_filter(en_raw)\n",
        "en_filtered.to_csv(DATA_DIR / \"english_nouns.csv\", index=False, encoding=\"utf-8\")\n",
        "en_freq.to_csv(DATA_DIR / \"english_noun_freq.csv\", index=False, encoding=\"utf-8\")\n",
        "print(f\"  [EN] {en_filtered['lemma'].nunique():,} lemmas ≥ {MIN_FREQ} occurrences retained.\")\n",
        "\n",
        "# ── Chinese ───────────────────────────────────────────────────\n",
        "zh_raw = preprocess_chinese(DATA_DIR / \"chinese_verses.csv\")\n",
        "zh_filtered, zh_freq = apply_frequency_filter(zh_raw)\n",
        "zh_filtered.to_csv(DATA_DIR / \"chinese_nouns.csv\", index=False, encoding=\"utf-8\")\n",
        "zh_freq.to_csv(DATA_DIR / \"chinese_noun_freq.csv\", index=False, encoding=\"utf-8\")\n",
        "print(f\"  [ZH] {zh_filtered['lemma'].nunique():,} lemmas ≥ {MIN_FREQ} occurrences retained.\")\n",
        "\n",
        "# ── Summary ───────────────────────────────────────────────────\n",
        "print(\"\\n── Preprocessing Summary ──\")\n",
        "print(f\"  EN noun tokens (filtered) : {len(en_filtered):,}\")\n",
        "print(f\"  EN unique lemmas          : {en_filtered['lemma'].nunique():,}\")\n",
        "print(f\"  ZH noun tokens (filtered) : {len(zh_filtered):,}\")\n",
        "print(f\"  ZH unique lemmas          : {zh_filtered['lemma'].nunique():,}\")\n",
        "\n",
        "print(\"\\n  Top 10 English nouns:\")\n",
        "print(en_freq.head(10).to_string(index=False))\n",
        "print(\"\\n  Top 10 Chinese nouns:\")\n",
        "print(zh_freq.head(10).to_string(index=False))\n",
        "\n",
        "print(\"\\n✓ Step 2 complete.\\n\")\n"
      ],
      "metadata": {
        "id": "ldNNKu9q0emn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Extract Context Embeddings"
      ],
      "metadata": {
        "id": "1CZa8ZJVWMeN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from pathlib import Path\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from typing import List, Tuple\n",
        "\n",
        "# ─── Configuration ────────────────────────────────────────────────────────────\n",
        "\n",
        "DATA_DIR    = Path(\"/content\") / \"bible_data\"\n",
        "MODEL_NAME  = \"xlm-roberta-base\"   # Multilingual; same model for EN and ZH\n",
        "BATCH_SIZE  = 32                   # Reduce to 8-16 if OOM on CPU\n",
        "LAYERS      = [-1, -2, -3, -4]     # Last 4 layers averaged (standard WSI practice)\n",
        "DEVICE      = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "MAX_SEQ_LEN = 128                  # Max subword tokens per sentence\n",
        "\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# ─── Model Loading ────────────────────────────────────────────────────────────\n",
        "\n",
        "def load_model():\n",
        "    print(f\"  [model] Loading {MODEL_NAME}…\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "    model     = AutoModel.from_pretrained(MODEL_NAME, output_hidden_states=True)\n",
        "    model.to(DEVICE)\n",
        "    model.eval()\n",
        "    return tokenizer, model\n",
        "\n",
        "\n",
        "# ─── Embedding Extraction ─────────────────────────────────────────────────────\n",
        "\n",
        "def get_target_embedding(\n",
        "    tokenizer,\n",
        "    model,\n",
        "    sentences:  List[str],\n",
        "    target_words: List[str],\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    For each (sentence, target_word) pair, extract the contextual embedding\n",
        "    of the target by:\n",
        "      1. Tokenizing the sentence\n",
        "      2. Finding subword token positions for the target word\n",
        "      3. Averaging hidden states across the last 4 layers at those positions\n",
        "      4. Mean-pooling across subwords for multi-token targets\n",
        "\n",
        "    Returns: np.ndarray of shape (N, hidden_dim)\n",
        "    \"\"\"\n",
        "    all_embeddings = []\n",
        "\n",
        "    for i in range(0, len(sentences), BATCH_SIZE):\n",
        "        batch_sents  = sentences[i : i + BATCH_SIZE]\n",
        "        batch_targets = target_words[i : i + BATCH_SIZE]\n",
        "\n",
        "        encoded = tokenizer(\n",
        "            batch_sents,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=MAX_SEQ_LEN,\n",
        "            return_offsets_mapping=True,\n",
        "        )\n",
        "        offset_mappings = encoded.pop(\"offset_mapping\")  # not passed to model\n",
        "\n",
        "        encoded = {k: v.to(DEVICE) for k, v in encoded.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**encoded)\n",
        "\n",
        "        # Stack selected hidden layers: shape (n_layers, batch, seq_len, hidden)\n",
        "        hidden_states = torch.stack(\n",
        "            [outputs.hidden_states[l] for l in LAYERS], dim=0\n",
        "        )\n",
        "        # Mean over selected layers: (batch, seq_len, hidden)\n",
        "        layer_mean = hidden_states.mean(dim=0).cpu().numpy()\n",
        "\n",
        "        input_ids = encoded[\"input_ids\"].cpu().numpy()\n",
        "\n",
        "        for j, (target, offsets_j) in enumerate(zip(batch_targets, offset_mappings)):\n",
        "            # Re-encode the target word alone to find its subword tokens\n",
        "            target_enc = tokenizer.encode(\n",
        "                target, add_special_tokens=False\n",
        "            )\n",
        "            # Find target subword positions in the sentence encoding\n",
        "            target_positions = _find_subword_positions(\n",
        "                input_ids[j].tolist(), target_enc\n",
        "            )\n",
        "            if target_positions:\n",
        "                token_emb = layer_mean[j][target_positions].mean(axis=0)\n",
        "            else:\n",
        "                # Fallback: mean-pool entire sequence (excluding [CLS]/[SEP])\n",
        "                seq_len = (input_ids[j] != tokenizer.pad_token_id).sum()\n",
        "                token_emb = layer_mean[j][1 : seq_len - 1].mean(axis=0)\n",
        "\n",
        "            all_embeddings.append(token_emb)\n",
        "\n",
        "        if (i // BATCH_SIZE) % 10 == 0:\n",
        "            print(f\"    … batch {i//BATCH_SIZE} / {len(sentences)//BATCH_SIZE}\", end=\"\\r\")\n",
        "\n",
        "    embeddings = np.array(all_embeddings, dtype=np.float32)\n",
        "    # L2 normalize for cosine-based clustering\n",
        "    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
        "    norms = np.where(norms == 0, 1, norms)\n",
        "    return embeddings / norms\n",
        "\n",
        "\n",
        "def _find_subword_positions(\n",
        "    sentence_ids: List[int], target_ids: List[int]\n",
        ") -> List[int]:\n",
        "    \"\"\"Find the start position of `target_ids` as a subsequence in `sentence_ids`.\"\"\"\n",
        "    n, m = len(sentence_ids), len(target_ids)\n",
        "    for start in range(n - m + 1):\n",
        "        if sentence_ids[start : start + m] == target_ids:\n",
        "            return list(range(start, start + m))\n",
        "    return []\n",
        "\n",
        "\n",
        "# ─── Per-language Pipeline ────────────────────────────────────────────────────\n",
        "\n",
        "def extract_embeddings_for_language(\n",
        "    lang: str,\n",
        "    noun_csv: Path,\n",
        "    tokenizer,\n",
        "    model,\n",
        ") -> None:\n",
        "    \"\"\"Load nouns, extract embeddings, and save as .npz.\"\"\"\n",
        "    out_path = DATA_DIR / f\"{lang}_embeddings.npz\"\n",
        "    if out_path.exists():\n",
        "        print(f\"  [{lang.upper()}] Embeddings already exist — skipping.\")\n",
        "        return\n",
        "\n",
        "    df = pd.read_csv(noun_csv)\n",
        "    print(f\"  [{lang.upper()}] Extracting embeddings for {len(df):,} noun occurrences…\")\n",
        "\n",
        "    embeddings = get_target_embedding(\n",
        "        tokenizer,\n",
        "        model,\n",
        "        sentences    = df[\"context\"].tolist(),\n",
        "        target_words = df[\"token\"].tolist(),\n",
        "    )\n",
        "    print()  # newline after progress indicator\n",
        "\n",
        "    np.savez_compressed(\n",
        "        out_path,\n",
        "        embeddings = embeddings,\n",
        "        lemmas     = df[\"lemma\"].to_numpy(dtype=str),\n",
        "        verse_ids  = df[\"verse_id\"].to_numpy(dtype=str),\n",
        "        tokens     = df[\"token\"].to_numpy(dtype=str),\n",
        "    )\n",
        "    print(f\"  [{lang.upper()}] Saved {embeddings.shape} embeddings → {out_path.name}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJF9kXsnWQDa",
        "outputId": "69c8d7b2-475c-44b6-b46c-a753625c8f31"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"Step 3: Contextual Embedding Extraction (XLM-R)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "tokenizer, model = load_model()\n",
        "\n",
        "extract_embeddings_for_language(\n",
        "    \"english\",\n",
        "    DATA_DIR / \"english_nouns.csv\",\n",
        "    tokenizer, model,\n",
        ")\n",
        "extract_embeddings_for_language(\n",
        "    \"chinese\",\n",
        "    DATA_DIR / \"chinese_nouns.csv\",\n",
        "    tokenizer, model,\n",
        ")\n",
        "\n",
        "print(\"\\n✓ Step 3 complete.\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 783,
          "referenced_widgets": [
            "c07b74d5d48845cbac6a1fb50b2a778f",
            "f4dc1318baa9443cac4bb23b3349f4af",
            "eedc270216c94bdb9fd0b75829fe29d0",
            "8a41ecd831f34d8ba4235d792e5dd3b6",
            "ce56a9109291444f845ae7bbba63ec7f",
            "b7f8e26f310a4b3289795438e60e783c",
            "6604615d85f24ac9b7ec04e0b8e57b8f",
            "5994cfa868254ac38d6849e744829974",
            "419a5bb6e054413fa3fc07e8028dd32f",
            "6f4463bba1234ccca814d7bbdf96a888",
            "07407c9708584b939c0f9eb5f58297c2",
            "b93166c752cf44689ca60c91e03c0171",
            "a7d825ed7e114ab1867571a335b01aec",
            "f768cf03dd6a4d98952de94f51fac834",
            "d8b2dd9fefa440aa95143669e1fca025",
            "5aa508402a9541809e25b45f71583ba8",
            "50dd69b6e6954295899ec481cb04dc27",
            "e7a8f3d54c13478093fdeebc7757a984",
            "e54c6b472b0c44bf95ee74b393316beb",
            "3402c3d2bca54014b31e9e16d3351252",
            "d83a432cc79f4a00a6d33cbceab0cec6",
            "cdd9e93d463c46e99b77ae3268e3d993",
            "d484bb3954e84d1da9b195028999d3a5",
            "19f3dd01454f489fbfda2e42e80a652a",
            "47d0729b78e24c1cbf953cd52b05503f",
            "a6e3323d8c5a4af08b3555b66f48699e",
            "f53aee84d3c34581a2b4aecdbe1f9582",
            "d017ab2cdd424678838d1d22ce19723d",
            "367ba03aee4c4db1a7edd731fa1648b3",
            "0cfd063f8bb7459596a95fbd700d5399",
            "9a8d7fc8581d4bbd866a6bdaf191d521",
            "092f14d00da246d6868c42bed94f9d63",
            "4a1be9e86e0a44bda0cf21a9d40d059c",
            "d75d123cc8434702b949444f6b986e6f",
            "13f6108d817b4cfe91d127ab8b935698",
            "1642f3053fe64c6393d451410b2591e2",
            "9c247ebbf33f411aa3f067ba63d9f6b4",
            "91bd5dab7c41401db92bf4614ea394aa",
            "a34d1b4264c14f29a8b1f10092d5cd50",
            "a42ea110cd794cb5a9b2ec1aa7179f49",
            "4451d51e84a44e46a884bac416ab4a70",
            "27a88156fa064722aecb009b23099b40",
            "bfb2e2ec2f1146de969468e7d49db324",
            "7a20089e47f842b78cd4d4edf6c79c03",
            "84959ac020824f22bcb975cc36f2a9b8",
            "c66860a6ee2b4c0bae1ef9539417469d",
            "08811ab1e9fd4d858ff4e279efef3642",
            "f58b31ee598748469ad4c34522a8f311",
            "e1d51a3f34de424ab5b0607faec77807",
            "4e82f53fd62d40558ccc08cae5c8aa50",
            "97dec1bc8dc24ddeb408390302cfa2d2",
            "1527ee104c494dabbba9a292d3d320a9",
            "3120009ddce44f50985b12901c5b640d",
            "45f62729f36146d5af88915d959d3616",
            "eb6a748521014e6aaa03394b0f59991b",
            "d79a1159cd4f4bcca284a18bc3a50f65",
            "db7a03c769834a6288afd509e7ed36fc",
            "de6b1a88be834a89a8cb6d54a383c4b9",
            "8729b4748cd54a049b6d4a590930bf64",
            "be83b8017f4c43dcb7ea11b9f0c1549e",
            "3de547ffb30a4c6aa69c2b3c402aa4aa",
            "ae97b302b9594058bccffd2d41e54675",
            "2af5b2683eb349c3989d7edb27daea8f",
            "4109151e0f054c4cb59d4964f77715e7",
            "ac0d87bf58d7437697aba8fc38ca855a",
            "d93ec0930ca843a4b8a81b722264177a"
          ]
        },
        "id": "ZwpfEQinX-ri",
        "outputId": "f8320c38-37ea-4418-fde7-dca3725ef467"
      },
      "execution_count": 9,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "Step 3: Contextual Embedding Extraction (XLM-R)\n",
            "============================================================\n",
            "  [model] Loading xlm-roberta-base…\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
            "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
            "You are not authenticated with the Hugging Face Hub in this notebook.\n",
            "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c07b74d5d48845cbac6a1fb50b2a778f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b93166c752cf44689ca60c91e03c0171",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d484bb3954e84d1da9b195028999d3a5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d75d123cc8434702b949444f6b986e6f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
            "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "84959ac020824f22bcb975cc36f2a9b8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d79a1159cd4f4bcca284a18bc3a50f65",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "XLMRobertaModel LOAD REPORT from: xlm-roberta-base\n",
            "Key                       | Status     |  | \n",
            "--------------------------+------------+--+-\n",
            "lm_head.layer_norm.bias   | UNEXPECTED |  | \n",
            "lm_head.dense.weight      | UNEXPECTED |  | \n",
            "lm_head.dense.bias        | UNEXPECTED |  | \n",
            "lm_head.layer_norm.weight | UNEXPECTED |  | \n",
            "lm_head.bias              | UNEXPECTED |  | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [ENGLISH] Extracting embeddings for 98,195 noun occurrences…\n",
            "\n",
            "  [ENGLISH] Saved (98195, 768) embeddings → english_embeddings.npz\n",
            "  [CHINESE] Extracting embeddings for 73,177 noun occurrences…\n",
            "\n",
            "  [CHINESE] Saved (73177, 768) embeddings → chinese_embeddings.npz\n",
            "\n",
            "✓ Step 3 complete.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: WSI Clustering"
      ],
      "metadata": {
        "id": "meHGlTr8aErA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.preprocessing import normalize\n",
        "from typing import Tuple, Dict\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "# ─── Configuration ────────────────────────────────────────────────────────────\n",
        "\n",
        "DATA_DIR    = Path(\"/content\") / \"bible_data\"\n",
        "# DATA_DIR             = Path(__file__).parent.parent / \"data\"\n",
        "K_RANGE              = range(2, 9)           # Test k = 2, 3, …, 8\n",
        "SILHOUETTE_THRESHOLD = 0.30                  # Below this → monosemous (k=1)\n",
        "# NOTE: 0.05 caused 0% monosemy — XLM-R embeddings always score > 0.05\n",
        "# even for genuinely monosemous words. 0.30 is the literature standard\n",
        "# for meaningful cluster structure (Ustalov et al. 2019; Neelakantan et al. 2014).\n",
        "# Run sensitivity check at SILHOUETTE_THRESHOLD = 0.20 and report both.\n",
        "MIN_INSTANCES        = 30                    # Min occurrences to attempt clustering\n",
        "# Raised from 5 to 30 to ensure UMAP dimensionality reduction works reliably.\n",
        "# UMAP requires n_samples > n_components (60 > 50). Words with 30-59 occurrences\n",
        "# are excluded from clustering as they lack sufficient contextual diversity for\n",
        "# robust sense induction (cf. Ustalov et al. 2019 who use threshold of 50).\n",
        "USE_UMAP             = True                  # Reduce to 50D before clustering\n",
        "UMAP_N_COMPONENTS    = 30\n",
        "RANDOM_STATE         = 42\n",
        "\n",
        "# ─── Optional UMAP reduction ─────────────────────────────────────────────────\n",
        "#     \"\"\"\n",
        "#     Optionally reduce embedding dimensionality with UMAP before clustering.\n",
        "#     UMAP (McInnes et al. 2018) improves cluster separation for high-dim data.\n",
        "#     Falls back to PCA if umap-learn is not installed or if UMAP fails.\n",
        "#     \"\"\"\n",
        "#     if not USE_UMAP or embeddings.shape[0] <= MIN_INSTANCES: # Added <= MIN_INSTANCES for robustness\n",
        "#         return embeddings\n",
        "\n",
        "#     try:\n",
        "#         import umap\n",
        "#         # Ensure n_components is always less than the number of samples\n",
        "#         n_components_umap = min(UMAP_N_COMPONENTS, embeddings.shape[0] - 1)\n",
        "#         if n_components_umap <= 0: # Handle cases where n_samples is 1\n",
        "#             return embeddings\n",
        "\n",
        "#         reducer = umap.UMAP(\n",
        "#             n_components=n_components_umap,\n",
        "#             metric=\"cosine\",\n",
        "#             random_state=RANDOM_STATE,\n",
        "#             n_jobs=1,\n",
        "#         )\n",
        "#         return reducer.fit_transform(embeddings)\n",
        "#     except (ImportError, TypeError) as e: # Catch both ImportError and TypeError\n",
        "#         if isinstance(e, ImportError):\n",
        "#             print(\"    [warn] umap-learn not found — using PCA fallback. \"\n",
        "#                   \"Install: pip install umap-learn --break-system-packages\")\n",
        "#         else:\n",
        "#             print(f\"    [warn] UMAP failed ({e}) — using PCA fallback for this lemma. \"\n",
        "#                   f\"Embeddings shape: {embeddings.shape}\")\n",
        "#         from sklearn.decomposition import PCA\n",
        "#         # Ensure n_components for PCA is also less than number of samples and features\n",
        "#         n_components_pca = min(UMAP_N_COMPONENTS, embeddings.shape[0] - 1, embeddings.shape[1])\n",
        "#         if n_components_pca <= 0: # Handle cases where n_samples is 1\n",
        "#             return embeddings\n",
        "#         return PCA(n_components=n_components_pca, random_state=RANDOM_STATE).fit_transform(embeddings)\n",
        "\n",
        "def reduce_embeddings(embeddings: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Reduce embedding dimensionality with UMAP before clustering.\n",
        "    UMAP (McInnes et al. 2018) improves cluster separation for high-dim data.\n",
        "    Falls back to PCA if umap-learn is not installed.\n",
        "\n",
        "    With MIN_INSTANCES=60, we always have n_samples ≥ 60 > 50 = n_components,\n",
        "    so UMAP will never fail with the k >= N error.\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    Reduce embedding dimensionality with UMAP before clustering.\n",
        "    UMAP (McInnes et al. 2018) improves cluster separation for high-dim data.\n",
        "    Falls back to PCA if umap-learn is not installed.\n",
        "\n",
        "    With MIN_INSTANCES=60, we always have n_samples ≥ 60 > 50 = n_components,\n",
        "    so UMAP will never fail with the k >= N error.\n",
        "    \"\"\"\n",
        "    # Define n from embeddings shape\n",
        "    n = embeddings.shape[0]\n",
        "\n",
        "    # Adaptive n_components based on sample size\n",
        "    n_components = min(50, n - 10)  # Leave buffer\n",
        "\n",
        "    # if not USE_UMAP or embeddings.shape[0] < UMAP_N_COMPONENTS:\n",
        "    if not USE_UMAP or n < n_components + 1:  # ✓ Uses adaptive value\n",
        "        return embeddings\n",
        "    try:\n",
        "        import umap\n",
        "        reducer = umap.UMAP(\n",
        "            n_components=n_components,\n",
        "            n_neighbors=min(15, n-1),  # Also adaptive\n",
        "            metric=\"cosine\",\n",
        "            random_state=RANDOM_STATE,\n",
        "            n_jobs=1,\n",
        "        )\n",
        "        return reducer.fit_transform(embeddings)\n",
        "    except ImportError:\n",
        "        print(\"    [warn] umap-learn not found — using PCA fallback. \"\n",
        "              \"Install: pip install umap-learn --break-system-packages\")\n",
        "        from sklearn.decomposition import PCA\n",
        "        n = min(UMAP_N_COMPONENTS, embeddings.shape[0] - 1, embeddings.shape[1])\n",
        "        return PCA(n_components=n, random_state=RANDOM_STATE).fit_transform(embeddings)\n",
        "\n",
        "\n",
        "# ─── Core WSI for a single lemma ──────────────────────────────────────────────\n",
        "def induce_senses_direct(\n",
        "    embeddings: np.ndarray,\n",
        ") -> Tuple[int, int, float, np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    WSI using direct hierarchical clustering on embeddings.\n",
        "    No UMAP - works for any n.\n",
        "\n",
        "    Returns same format as induce_senses_for_lemma:\n",
        "        k_ward, k_kmeans, best_silhouette_ward,\n",
        "        labels_ward (np.ndarray), labels_kmeans (np.ndarray)\n",
        "    \"\"\"\n",
        "    from scipy.spatial.distance import pdist, squareform\n",
        "    from sklearn.cluster import AgglomerativeClustering, KMeans\n",
        "    from sklearn.metrics import silhouette_score\n",
        "\n",
        "    n = len(embeddings)\n",
        "\n",
        "    # Insufficient data → monosemous\n",
        "    if n < 2:\n",
        "        ones = np.zeros(n, dtype=int)\n",
        "        return 1, 1, 0.0, ones, ones\n",
        "\n",
        "    # Compute pairwise cosine distances\n",
        "    distances = squareform(pdist(embeddings, metric='cosine'))\n",
        "\n",
        "    # Initialize best results for both methods\n",
        "    best_k_ward, best_sil_ward, best_labels_ward = 1, -1.0, np.zeros(n, dtype=int)\n",
        "    best_k_km, best_sil_km, best_labels_km = 1, -1.0, np.zeros(n, dtype=int)\n",
        "\n",
        "    for k in range(2, min(9, n)):\n",
        "        # Ward hierarchical clustering\n",
        "        try:\n",
        "            ward = AgglomerativeClustering(\n",
        "                n_clusters=k,\n",
        "                metric='precomputed',\n",
        "                linkage='average'  # Use 'average' instead of 'ward' for precomputed\n",
        "            )\n",
        "            labels_w = ward.fit_predict(distances)\n",
        "\n",
        "            if len(np.unique(labels_w)) > 1:\n",
        "                sil_w = silhouette_score(\n",
        "                    distances, labels_w,\n",
        "                    metric='precomputed',\n",
        "                    sample_size=min(1000, n)\n",
        "                )\n",
        "                if sil_w > best_sil_ward:\n",
        "                    best_sil_ward, best_k_ward, best_labels_ward = sil_w, k, labels_w\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        # K-Means clustering on raw embeddings\n",
        "        try:\n",
        "            km = KMeans(\n",
        "                n_clusters=k,\n",
        "                random_state=RANDOM_STATE,\n",
        "                n_init=10,\n",
        "                max_iter=300\n",
        "            )\n",
        "            labels_k = km.fit_predict(embeddings)\n",
        "\n",
        "            if len(np.unique(labels_k)) > 1:\n",
        "                sil_k = silhouette_score(\n",
        "                    embeddings, labels_k,\n",
        "                    metric='euclidean',\n",
        "                    sample_size=min(1000, n)\n",
        "                )\n",
        "                if sil_k > best_sil_km:\n",
        "                    best_sil_km, best_k_km, best_labels_km = sil_k, k, labels_k\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # Apply monosemy threshold\n",
        "    if best_sil_ward < SILHOUETTE_THRESHOLD:\n",
        "        best_k_ward = 1\n",
        "        best_labels_ward = np.zeros(n, dtype=int)\n",
        "\n",
        "    if best_sil_km < SILHOUETTE_THRESHOLD:\n",
        "        best_k_km = 1\n",
        "        best_labels_km = np.zeros(n, dtype=int)\n",
        "\n",
        "    return (best_k_ward, best_k_km, max(best_sil_ward, 0.0),\n",
        "            best_labels_ward, best_labels_km)\n",
        "\n",
        "def induce_senses_for_lemma(\n",
        "    embeddings: np.ndarray,\n",
        ") -> Tuple[int, int, float, np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Given embeddings for all occurrences of a lemma, find the optimal k\n",
        "    using silhouette score for both Ward and KMeans clustering.\n",
        "\n",
        "    Returns:\n",
        "        k_ward, k_kmeans, best_silhouette_ward,\n",
        "        labels_ward (np.ndarray), labels_kmeans (np.ndarray)\n",
        "    \"\"\"\n",
        "    n = len(embeddings)\n",
        "\n",
        "    # Insufficient data → monosemous\n",
        "    if n < MIN_INSTANCES:\n",
        "        ones = np.zeros(n, dtype=int)\n",
        "        return 1, 1, 0.0, ones, ones\n",
        "\n",
        "    reduced = reduce_embeddings(embeddings)\n",
        "\n",
        "    best_k_ward, best_sil_ward, best_labels_ward   = 1, -1.0, np.zeros(n, dtype=int)\n",
        "    best_k_km,   best_sil_km,   best_labels_km     = 1, -1.0, np.zeros(n, dtype=int)\n",
        "\n",
        "    for k in K_RANGE:\n",
        "        if k >= n:\n",
        "            break  # Can't have more clusters than data points\n",
        "\n",
        "        # Ward agglomerative\n",
        "        try:\n",
        "            ward = AgglomerativeClustering(n_clusters=k, linkage=\"ward\")\n",
        "            labels_w = ward.fit_predict(reduced)\n",
        "            if len(np.unique(labels_w)) > 1:\n",
        "                sil_w = silhouette_score(reduced, labels_w, metric=\"euclidean\",\n",
        "                                         sample_size=min(1000, n))\n",
        "                if sil_w > best_sil_ward:\n",
        "                    best_sil_ward, best_k_ward, best_labels_ward = sil_w, k, labels_w\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        # K-Means\n",
        "        try:\n",
        "            km = KMeans(n_clusters=k, random_state=RANDOM_STATE,\n",
        "                        n_init=10, max_iter=300)\n",
        "            labels_k = km.fit_predict(reduced)\n",
        "            if len(np.unique(labels_k)) > 1:\n",
        "                sil_k = silhouette_score(reduced, labels_k, metric=\"euclidean\",\n",
        "                                          sample_size=min(1000, n))\n",
        "                if sil_k > best_sil_km:\n",
        "                    best_sil_km, best_k_km, best_labels_km = sil_k, k, labels_k\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # Apply monosemy threshold\n",
        "    if best_sil_ward < SILHOUETTE_THRESHOLD:\n",
        "        best_k_ward    = 1\n",
        "        best_labels_ward = np.zeros(n, dtype=int)\n",
        "\n",
        "    if best_sil_km < SILHOUETTE_THRESHOLD:\n",
        "        best_k_km    = 1\n",
        "        best_labels_km = np.zeros(n, dtype=int)\n",
        "\n",
        "    return (best_k_ward, best_k_km, max(best_sil_ward, 0.0),\n",
        "            best_labels_ward, best_labels_km)\n",
        "\n",
        "# ─── HDBSCAN ──────────────────────────────────────────────\n",
        "\n",
        "import hdbscan\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "def induce_senses_hdbscan(embeddings: np.ndarray):\n",
        "    n_samples, n_features = embeddings.shape\n",
        "\n",
        "    if n_samples < 20:\n",
        "        return 1, np.zeros(n_samples, dtype=int)\n",
        "\n",
        "    # L2 normalize\n",
        "    embeddings = normalize(embeddings)\n",
        "\n",
        "    # Optional PCA only if large sample size\n",
        "    if n_samples > 200:\n",
        "        n_components = min(100, n_samples - 1, n_features)\n",
        "        embeddings = PCA(\n",
        "            n_components=n_components,\n",
        "            random_state=RANDOM_STATE\n",
        "        ).fit_transform(embeddings)\n",
        "        embeddings = normalize(embeddings)\n",
        "\n",
        "    min_cluster_size = max(8, int(0.05 * n_samples))\n",
        "\n",
        "    clusterer = hdbscan.HDBSCAN(\n",
        "        metric=\"euclidean\", # Changed from \"cosine\" to \"euclidean\"\n",
        "        min_cluster_size=min_cluster_size,\n",
        "        min_samples=max(5, min_cluster_size // 2),\n",
        "        cluster_selection_method=\"eom\"\n",
        "    )\n",
        "\n",
        "    labels = clusterer.fit_predict(embeddings)\n",
        "\n",
        "    clusters = set(labels)\n",
        "    clusters.discard(-1)\n",
        "\n",
        "    k = len(clusters)\n",
        "\n",
        "    if k <= 1:\n",
        "        return 1, np.zeros(n_samples, dtype=int)\n",
        "\n",
        "    sizes = sorted([sum(labels == c) for c in clusters], reverse=True)\n",
        "\n",
        "    if sizes[1] / n_samples < 0.12:\n",
        "        return 1, np.zeros(n_samples, dtype=int)\n",
        "\n",
        "    return k, labels\n",
        "\n",
        "# def induce_senses_hdbscan(embeddings: np.ndarray):\n",
        "#     n = len(embeddings)\n",
        "\n",
        "#     if n < 20:\n",
        "#         labels = np.zeros(n, dtype=int)\n",
        "#         return 1, labels\n",
        "\n",
        "#     # L2 normalize\n",
        "#     embeddings = normalize(embeddings)\n",
        "\n",
        "#     # Optional PCA to 100D (NOT UMAP)\n",
        "#     if embeddings.shape[1] > 100:\n",
        "#         embeddings = PCA(n_components=100, random_state=RANDOM_STATE).fit_transform(embeddings)\n",
        "#         embeddings = normalize(embeddings)\n",
        "\n",
        "#     min_cluster_size = max(8, int(0.05 * n))\n",
        "\n",
        "#     clusterer = hdbscan.HDBSCAN(\n",
        "#         metric=\"cosine\",\n",
        "#         min_cluster_size=min_cluster_size,\n",
        "#         min_samples=max(5, min_cluster_size // 2),\n",
        "#         cluster_selection_method=\"eom\"\n",
        "#     )\n",
        "\n",
        "#     labels = clusterer.fit_predict(embeddings)\n",
        "\n",
        "#     unique_clusters = set(labels)\n",
        "#     if -1 in unique_clusters:\n",
        "#         unique_clusters.remove(-1)\n",
        "\n",
        "#     k = len(unique_clusters)\n",
        "\n",
        "#     # Conservative monosemy guardrail\n",
        "#     if k <= 1:\n",
        "#         return 1, np.zeros(n, dtype=int)\n",
        "\n",
        "#     cluster_sizes = [sum(labels == c) for c in unique_clusters]\n",
        "#     cluster_sizes.sort(reverse=True)\n",
        "\n",
        "#     if cluster_sizes[1] / n < 0.12:  # tiny second cluster\n",
        "#         return 1, np.zeros(n, dtype=int)\n",
        "\n",
        "#     return k, labels\n",
        "\n",
        "# ─── Run WSI for entire language ──────────────────────────────────────────────\n",
        "\n",
        "def run_wsi_for_language(lang: str):\n",
        "    \"\"\"\n",
        "    L Load embeddings and run WSI for all lemmas in a language.\n",
        "    Saves two files:\n",
        "      1. {lang}_wsi_results.csv    - summary stats per lemma\n",
        "      2. {lang}_sense_labels.csv   - cluster assignments per occurrence\n",
        "    \"\"\"\n",
        "    embeddings_file = DATA_DIR / f\"{lang}_embeddings.npz\"\n",
        "    nouns_file      = DATA_DIR / f\"{lang}_nouns.csv\"\n",
        "\n",
        "    if not embeddings_file.exists():\n",
        "        print(f\"  [{lang.upper()}] ERROR: {embeddings_file.name} not found. Run Step 3 first.\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\n  [{lang.upper()}] Loading embeddings…\")\n",
        "    data = np.load(embeddings_file, allow_pickle=True)\n",
        "    lemmas_array = data[\"lemmas\"]\n",
        "    embeddings   = data[\"embeddings\"]\n",
        "    verse_ids    = data[\"verse_ids\"]\n",
        "    unique_lemmas = np.unique(lemmas_array)\n",
        "\n",
        "    print(f\"  [{lang.upper()}] Running WSI for {len(unique_lemmas):,} lemmas…\")\n",
        "\n",
        "    results = []\n",
        "    all_labels = []  # Collect per-instance labels for sense_labels.csv\n",
        "\n",
        "    for i, lemma in enumerate(unique_lemmas):\n",
        "        mask = (lemmas_array == lemma)\n",
        "        lemma_embeds = embeddings[mask]\n",
        "        lemma_vids   = verse_ids[mask]\n",
        "\n",
        "        # k_ward, k_km, sil_ward, labels_ward, labels_km = induce_senses_for_lemma(lemma_embeds)\n",
        "        # k_ward, k_km, sil_ward, labels_ward, labels_km = induce_senses_direct(lemma_embeds)\n",
        "        k_hdb, labels_hdb = induce_senses_hdbscan(lemma_embeds)\n",
        "\n",
        "        # # Agreement: what % of instances assigned to same cluster by both methods?\n",
        "        # if len(labels_ward) > 0:\n",
        "        #     agree = (labels_ward == labels_km).mean() * 100\n",
        "        # else:\n",
        "        #     agree = 100.0\n",
        "\n",
        "        # results.append({\n",
        "        #     \"lemma\":            lemma,\n",
        "        #     \"n_instances\":      len(lemma_embeds),\n",
        "        #     \"k_ward\":           k_ward,\n",
        "        #     \"k_kmeans\":         k_km,\n",
        "        #     \"silhouette_ward\":  round(sil_ward, 4),\n",
        "        #     \"agreement_pct\":    round(agree, 2),\n",
        "        # })\n",
        "        results.append({\n",
        "            \"lemma\": lemma,\n",
        "            \"n_instances\": len(lemma_embeds),\n",
        "            \"k_hdbscan\": k_hdb,\n",
        "        })\n",
        "\n",
        "        # # Collect per-instance cluster assignments\n",
        "        # for vid, cluster_w, cluster_k in zip(lemma_vids, labels_ward, labels_km):\n",
        "        #     all_labels.append({\n",
        "        #         \"lemma\":          lemma,\n",
        "        #         \"verse_id\":       vid,\n",
        "        #         \"cluster_ward\":   int(cluster_w),\n",
        "        #         \"cluster_kmeans\": int(cluster_k),\n",
        "        #     })\n",
        "        for vid, cluster in zip(lemma_vids, labels_hdb):\n",
        "            all_labels.append({\n",
        "                \"lemma\": lemma,\n",
        "                \"verse_id\": vid,\n",
        "                \"cluster_hdbscan\": int(cluster),\n",
        "            })\n",
        "\n",
        "        if (i + 1) % 50 == 0 or (i + 1) == len(unique_lemmas):\n",
        "            print(f\"    … {i + 1:,}/{len(unique_lemmas):,} lemmas\", end=\"\\r\")\n",
        "\n",
        "\n",
        "    # Save summary results\n",
        "    df = pd.DataFrame(results)\n",
        "    out_path = DATA_DIR / f\"{lang}_wsi_results.csv\"\n",
        "    df.to_csv(out_path, index=False)\n",
        "    print(f\"\\n  [{lang.upper()}] Saved: {out_path.name}\")\n",
        "\n",
        "    # Save per-instance cluster labels\n",
        "    labels_df = pd.DataFrame(all_labels)\n",
        "    labels_path = DATA_DIR / f\"{lang}_sense_labels.csv\"\n",
        "    labels_df.to_csv(labels_path, index=False)\n",
        "    print(f\"  [{lang.upper()}] Saved: {labels_path.name}\")\n",
        "\n",
        "    # print(f\"  [{lang.upper()}] Mean k (Ward): {df['k_ward'].mean():.2f}  \"\n",
        "    #       f\"Median: {df['k_ward'].median():.0f}  \"\n",
        "    #       f\"Monosemous (k=1): {(df['k_ward']==1).sum()} / {len(df)}\")\n",
        "    print(f\"  [{lang.upper()}] Mean k (HDBSCAN): {df['k_hdbscan'].mean():.2f}  \"\n",
        "      f\"Median: {df['k_hdbscan'].median():.0f}  \"\n",
        "      f\"Monosemous (k=1): {(df['k_hdbscan']==1).sum()} / {len(df)}\")"
      ],
      "metadata": {
        "id": "wCI4OOXjYSNs"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"Step 4: Word Sense Induction (WSI)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# run_wsi_for_language(\"english\")\n",
        "# run_wsi_for_language(\"chinese\")\n",
        "\n",
        "# Quick comparison preview\n",
        "en = pd.read_csv(DATA_DIR / \"english_wsi_results.csv\")\n",
        "zh = pd.read_csv(DATA_DIR / \"chinese_wsi_results.csv\")\n",
        "\n",
        "# print(\"\\n── Quick Comparison Preview ──\")\n",
        "# print(f\"  EN | mean senses/lemma (Ward) : {en['k_ward'].mean():.3f}\")\n",
        "# print(f\"  ZH | mean senses/lemma (Ward) : {zh['k_ward'].mean():.3f}\")\n",
        "# print(f\"  EN | % polysemous lemmas       : {(en['k_ward'] > 1).mean()*100:.1f}%\")\n",
        "# print(f\"  ZH | % polysemous lemmas       : {(zh['k_ward'] > 1).mean()*100:.1f}%\")\n",
        "print(\"\\n── Quick Comparison Preview ──\")\n",
        "print(f\"  EN | mean senses/lemma (Ward) : {en['k_hdbscan'].mean():.3f}\")\n",
        "print(f\"  ZH | mean senses/lemma (Ward) : {zh['k_hdbscan'].mean():.3f}\")\n",
        "print(f\"  EN | % polysemous lemmas       : {(en['k_hdbscan'] > 1).mean()*100:.1f}%\")\n",
        "print(f\"  ZH | % polysemous lemmas       : {(zh['k_hdbscan'] > 1).mean()*100:.1f}%\")\n",
        "\n",
        "print(\"\\n✓ Step 4 complete.\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUNKMUx1aRI5",
        "outputId": "5e9156a3-dfa9-4bc4-b91d-e9c3a908bbac"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Step 4: Word Sense Induction (WSI)\n",
            "============================================================\n",
            "\n",
            "── Quick Comparison Preview ──\n",
            "  EN | mean senses/lemma (Ward) : 1.569\n",
            "  ZH | mean senses/lemma (Ward) : 1.138\n",
            "  EN | % polysemous lemmas       : 47.2%\n",
            "  ZH | % polysemous lemmas       : 11.8%\n",
            "\n",
            "✓ Step 4 complete.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Validation and Statistical Analysis"
      ],
      "metadata": {
        "id": "lX7M7-Toc3Dl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "from scipy import stats\n",
        "from typing import Optional, Tuple\n",
        "\n",
        "# ─── Configuration ────────────────────────────────────────────────────────────\n",
        "\n",
        "DATA_DIR    = Path(\"/content\") / \"bible_data\"\n",
        "OUTPUT_DIR = Path(\"/content\") / \"output\"\n",
        "# DATA_DIR   = Path(__file__).parent.parent / \"data\"\n",
        "# OUTPUT_DIR = Path(__file__).parent.parent / \"output\"\n",
        "FIG_DIR    = OUTPUT_DIR / \"figures\"\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "FIG_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "plt.rcParams.update({\n",
        "    \"font.family\":  \"DejaVu Sans\",\n",
        "    \"font.size\":    11,\n",
        "    \"axes.titlesize\": 13,\n",
        "    \"axes.labelsize\": 12,\n",
        "})\n",
        "\n",
        "\n",
        "# ─── WordNet Validation (English) ────────────────────────────────────────────\n",
        "\n",
        "def get_wordnet_sense_counts(lemmas: list) -> dict:\n",
        "    \"\"\"\n",
        "    Look up the number of synsets for each lemma in Princeton WordNet.\n",
        "    Noun synsets only (pos='n').\n",
        "    \"\"\"\n",
        "    try:\n",
        "        from nltk.corpus import wordnet as wn\n",
        "    except ImportError:\n",
        "        raise ImportError(\"Run: pip install nltk --break-system-packages && \"\n",
        "                          \"python -c \\\"import nltk; nltk.download('wordnet')\\\"\")\n",
        "\n",
        "    counts = {}\n",
        "    for lemma in lemmas:\n",
        "        synsets = wn.synsets(lemma.lower(), pos=wn.NOUN)\n",
        "        counts[lemma] = len(synsets)\n",
        "    return counts\n",
        "\n",
        "\n",
        "def validate_english(en_results: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Correlate WSI-induced k_ward with WordNet sense counts for English nouns.\n",
        "    Returns a merged DataFrame with both counts.\n",
        "    \"\"\"\n",
        "    print(\"  [validate] Looking up Princeton WordNet sense counts…\")\n",
        "    lemmas = en_results[\"lemma\"].tolist()\n",
        "    wn_counts = get_wordnet_sense_counts(lemmas)\n",
        "\n",
        "    en_results = en_results.copy()\n",
        "    en_results[\"wn_senses\"] = en_results[\"lemma\"].map(wn_counts).fillna(0).astype(int)\n",
        "\n",
        "    # Keep only lemmas with at least 1 WordNet entry\n",
        "    valid = en_results[en_results[\"wn_senses\"] > 0].copy()\n",
        "\n",
        "    # rho, p = stats.spearmanr(valid[\"k_ward\"], valid[\"wn_senses\"])\n",
        "    rho, p = stats.spearmanr(valid[\"k_hdbscan\"], valid[\"wn_senses\"])\n",
        "    # print(f\"  [validate] EN Spearman ρ(k_ward, WN_senses) = {rho:.3f}  p = {p:.4f}  \"\n",
        "    print(f\"  [validate] EN Spearman ρ(k_hdbscan, WN_senses) = {rho:.3f}  p = {p:.4f}  \"\n",
        "          f\"(n={len(valid)})\")\n",
        "\n",
        "    return valid, rho, p\n",
        "\n",
        "\n",
        "# ─── WordNet Validation (Chinese) ────────────────────────────────────────────\n",
        "\n",
        "def get_chinese_wordnet_sense_counts(lemmas: list) -> dict:\n",
        "    \"\"\"\n",
        "    Look up the number of synsets for each lemma in Chinese WordNet (cmn).\n",
        "    Noun synsets only (pos='n').\n",
        "    \"\"\"\n",
        "    try:\n",
        "        from nltk.corpus import wordnet as wn\n",
        "    except ImportError:\n",
        "        raise ImportError(\"Run: pip install nltk --break-system-packages && \"\n",
        "                          \"python -c \\\"import nltk; nltk.download('wordnet')\\\"\")\n",
        "\n",
        "    counts = {}\n",
        "    for lemma in lemmas:\n",
        "        # 'cmn' is the language code for Mandarin Chinese in WordNet\n",
        "        # Note: Chinese WordNet coverage in NLTK might be limited compared to English\n",
        "        synsets = wn.synsets(lemma, lang=\"cmn\", pos=wn.NOUN)\n",
        "        counts[lemma] = len(synsets)\n",
        "    return counts\n",
        "\n",
        "\n",
        "def validate_chinese(zh_results: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Correlate WSI-induced k_ward with Chinese WordNet sense counts for Chinese nouns.\n",
        "    Returns a merged DataFrame with both counts.\n",
        "    \"\"\"\n",
        "    print(\"  [validate] Looking up Chinese WordNet sense counts…\")\n",
        "    lemmas = zh_results[\"lemma\"].tolist()\n",
        "    cwn_counts = get_chinese_wordnet_sense_counts(lemmas)\n",
        "\n",
        "    zh_results = zh_results.copy()\n",
        "    zh_results[\"cwn_senses\"] = zh_results[\"lemma\"].map(cwn_counts).fillna(0).astype(int)\n",
        "\n",
        "    # Keep only lemmas with at least 1 WordNet entry\n",
        "    valid = zh_results[zh_results[\"cwn_senses\"] > 0].copy()\n",
        "\n",
        "    if len(valid) == 0:\n",
        "        print(\"  [validate] No Chinese lemmas found in NLTK's Chinese WordNet. Skipping Spearman correlation.\")\n",
        "        rho, p = np.nan, np.nan # No correlation if no data\n",
        "    elif len(valid) == 1:\n",
        "        print(\"  [validate] Only one Chinese lemma found in NLTK's Chinese WordNet. Skipping Spearman correlation.\")\n",
        "        rho, p = np.nan, np.nan # Spearman requires at least two data points\n",
        "    else:\n",
        "        # rho, p = stats.spearmanr(valid[\"k_ward\"], valid[\"cwn_senses\"])\n",
        "        rho, p = stats.spearmanr(valid[\"k_hdbscan\"], valid[\"cwn_senses\"])\n",
        "        # print(f\"  [validate] ZH Spearman ρ(k_ward, CWN_senses) = {rho:.3f}  p = {p:.4f}  \"\n",
        "        print(f\"  [validate] ZH Spearman ρ(k_hdbscan, CWN_senses) = {rho:.3f}  p = {p:.4f}  \"\n",
        "              f\"(n={len(valid)})\")\n",
        "\n",
        "    return valid, rho, p\n",
        "\n",
        "\n",
        "# ─── Statistical Comparison ──────────────────────────────────────────────────\n",
        "\n",
        "def mann_whitney_comparison(\n",
        "    en_k: np.ndarray,\n",
        "    zh_k: np.ndarray,\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    Mann-Whitney U test comparing mean sense counts between EN and ZH.\n",
        "    Also computes Cohen's d and the common language effect size (CLES).\n",
        "    \"\"\"\n",
        "    u_stat, p_val = stats.mannwhitneyu(en_k, zh_k, alternative=\"two-sided\")\n",
        "    n1, n2        = len(en_k), len(zh_k)\n",
        "    cles          = u_stat / (n1 * n2)  # Common Language Effect Size\n",
        "\n",
        "    # Cohen's d (for reference alongside CLES)\n",
        "    pooled_std = np.sqrt(\n",
        "        ((n1 - 1) * en_k.std(ddof=1) ** 2 + (n2 - 1) * zh_k.std(ddof=1) ** 2)\n",
        "        / (n1 + n2 - 2)\n",
        "    )\n",
        "    cohens_d = (en_k.mean() - zh_k.mean()) / (pooled_std + 1e-9)\n",
        "\n",
        "    return {\n",
        "        \"en_mean_k\":   round(en_k.mean(), 4),\n",
        "        \"zh_mean_k\":   round(zh_k.mean(), 4),\n",
        "        \"en_median_k\": round(float(np.median(en_k)), 4),\n",
        "        \"zh_median_k\": round(float(np.median(zh_k)), 4),\n",
        "        \"en_std_k\":    round(en_k.std(ddof=1), 4),\n",
        "        \"zh_std_k\":    round(zh_k.std(ddof=1), 4),\n",
        "        \"U_statistic\": round(u_stat, 2),\n",
        "        \"p_value\":     round(p_val, 6),\n",
        "        \"CLES\":        round(cles, 4),\n",
        "        \"cohens_d\":    round(cohens_d, 4),\n",
        "        \"n_en\":        int(n1),\n",
        "        \"n_zh\":        int(n2),\n",
        "    }\n",
        "\n",
        "\n",
        "# ─── Figures ─────────────────────────────────────────────────────────────────\n",
        "\n",
        "# def plot_sense_distribution(df: pd.DataFrame, lang: str, col: str = \"k_ward\") -> None:\n",
        "def plot_sense_distribution(df: pd.DataFrame, lang: str, col: str = \"k_hdbscan\") -> None:\n",
        "    fig, ax = plt.subplots(figsize=(8, 4))\n",
        "    counts = df[col].value_counts().sort_index()\n",
        "    ax.bar(counts.index, counts.values, color=\"#4C72B0\", edgecolor=\"white\", linewidth=0.5)\n",
        "    ax.set_xlabel(\"Number of Induced Senses (k)\")\n",
        "    ax.set_ylabel(\"Number of Lemmas\")\n",
        "    ax.set_title(f\"{lang.capitalize()} — Distribution of Induced Senses per Noun Lemma\")\n",
        "    ax.set_xticks(range(1, df[col].max() + 1))\n",
        "    fig.tight_layout()\n",
        "    path = FIG_DIR / f\"sense_distribution_{lang}.png\"\n",
        "    fig.savefig(path, dpi=150)\n",
        "    plt.close(fig)\n",
        "    print(f\"  [fig] Saved: {path.name}\")\n",
        "\n",
        "\n",
        "def plot_comparison_boxplot(en_df: pd.DataFrame, zh_df: pd.DataFrame) -> None:\n",
        "    combined = pd.concat([\n",
        "        # en_df[[\"k_ward\"]].assign(Language=\"English\"),\n",
        "        # zh_df[[\"k_ward\"]].assign(Language=\"Chinese\"),\n",
        "        en_df[[\"k_hdbscan\"]].assign(Language=\"English\"),\n",
        "        zh_df[[\"k_hdbscan\"]].assign(Language=\"Chinese\"),\n",
        "    ])\n",
        "    fig, ax = plt.subplots(figsize=(7, 5))\n",
        "    # sns.violinplot(data=combined, x=\"Language\", y=\"k_ward\",\n",
        "    sns.violinplot(data=combined, x=\"Language\", y=\"k_hdbscan\",\n",
        "                   palette=[\"#4C72B0\", \"#DD8452\"], inner=\"box\",\n",
        "                   cut=0,   # ← add this to fix sns artifact of extending beyond 8\n",
        "                   ax=ax)\n",
        "    ax.set_ylabel(\"Induced Senses per Lemma (k, Ward)\")\n",
        "    ax.set_title(\"Distribution of Polysemy Degree: English vs. Chinese Common Nouns\\n\"\n",
        "                 \"(Bible Parallel Corpus, WSI via XLM-R + Agglomerative Clustering)\")\n",
        "    fig.tight_layout()\n",
        "    path = FIG_DIR / \"comparison_violinplot.png\"\n",
        "    fig.savefig(path, dpi=150)\n",
        "    plt.close(fig)\n",
        "    print(f\"  [fig] Saved: {path.name}\")\n",
        "\n",
        "\n",
        "def plot_wordnet_correlation(valid_df: pd.DataFrame, rho: float, p: float) -> None:\n",
        "    fig, ax = plt.subplots(figsize=(6, 5))\n",
        "    # ax.scatter(valid_df[\"wn_senses\"], valid_df[\"k_ward\"],\n",
        "    ax.scatter(valid_df[\"wn_senses\"], valid_df[\"k_hdbscan\"],\n",
        "               alpha=0.4, s=20, color=\"#4C72B0\")\n",
        "    ax.set_xlabel(\"WordNet Noun Synset Count\")\n",
        "    ax.set_ylabel(\"WSI-Induced k (Ward)\")\n",
        "    ax.set_title(f\"Validation: WSI k vs. WordNet Senses (English Nouns)\\n\"\n",
        "                 f\"Spearman ρ = {rho:.3f}, p = {p:.4f}\")\n",
        "    # Trend line\n",
        "    # m, b = np.polyfit(valid_df[\"wn_senses\"], valid_df[\"k_ward\"], 1)\n",
        "    m, b = np.polyfit(valid_df[\"wn_senses\"], valid_df[\"k_hdbscan\"], 1)\n",
        "    x_line = np.linspace(valid_df[\"wn_senses\"].min(), valid_df[\"wn_senses\"].max(), 100)\n",
        "    ax.plot(x_line, m * x_line + b, color=\"red\", linewidth=1.5, linestyle=\"--\")\n",
        "    fig.tight_layout()\n",
        "    path = FIG_DIR / \"wordnet_correlation_en.png\"\n",
        "    fig.savefig(path, dpi=150)\n",
        "    plt.close(fig)\n",
        "    print(f\"  [fig] Saved: {path.name}\")\n",
        "\n",
        "def plot_wordnet_correlation_chinese(valid_df: pd.DataFrame, rho: float, p: float) -> None:\n",
        "    fig, ax = plt.subplots(figsize=(6, 5))\n",
        "    # ax.scatter(valid_df[\"cwn_senses\"], valid_df[\"k_ward\"],\n",
        "    ax.scatter(valid_df[\"cwn_senses\"], valid_df[\"k_hdbscan\"],\n",
        "               alpha=0.4, s=20, color=\"#DD8452\") # Using a different color for Chinese\n",
        "    ax.set_xlabel(\"Chinese WordNet Noun Synset Count\")\n",
        "    ax.set_ylabel(\"WSI-Induced k (Ward)\")\n",
        "    ax.set_title(f\"Validation: WSI k vs. Chinese WordNet Senses\\n\"\n",
        "                 f\"Spearman ρ = {rho:.3f}, p = {p:.4f}\")\n",
        "    # Trend line\n",
        "    # Only plot trend line if there's enough data and correlation is valid\n",
        "    if not np.isnan(rho) and len(valid_df) > 1:\n",
        "        # m, b = np.polyfit(valid_df[\"cwn_senses\"], valid_df[\"k_ward\"], 1)\n",
        "        m, b = np.polyfit(valid_df[\"cwn_senses\"], valid_df[\"k_hdbscan\"], 1)\n",
        "        x_line = np.linspace(valid_df[\"cwn_senses\"].min(), valid_df[\"cwn_senses\"].max(), 100)\n",
        "        ax.plot(x_line, m * x_line + b, color=\"red\", linewidth=1.5, linestyle=\"--\")\n",
        "\n",
        "    fig.tight_layout()\n",
        "    path = FIG_DIR / \"wordnet_correlation_zh.png\"\n",
        "    fig.savefig(path, dpi=150)\n",
        "    plt.close(fig)\n",
        "    print(f\"  [fig] Saved: {path.name}\")\n",
        "\n",
        "\n",
        "def plot_silhouette_distribution(en_df: pd.DataFrame, zh_df: pd.DataFrame,\n",
        "                                  threshold: float = 0.30) -> None:\n",
        "    \"\"\"\n",
        "    Plot silhouette score distributions for both languages.\n",
        "    Shows the threshold τ that separates monosemous (k=1) from polysemous words.\n",
        "\n",
        "    Why this figure matters:\n",
        "      - Validates that τ=0.30 is correctly positioned in the distribution\n",
        "      - Shows monosemous vs polysemous words have different silhouette profiles\n",
        "      - Confirms the threshold is not arbitrary\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 4.5), sharey=False)\n",
        "    fig.suptitle(\"Silhouette Score Distributions with Monosemy Threshold τ = 0.30\",\n",
        "                 fontsize=13, fontweight=\"bold\")\n",
        "\n",
        "    for ax, df, lang, color in zip(\n",
        "        axes,\n",
        "        [en_df, zh_df],\n",
        "        [\"English\", \"Chinese\"],\n",
        "        [\"#4C72B0\", \"#DD8452\"]\n",
        "    ):\n",
        "        mono = df[df[\"k_ward\"] == 1][\"silhouette_ward\"]\n",
        "        poly = df[df[\"k_ward\"] >  1][\"silhouette_ward\"]\n",
        "\n",
        "        # Monosemous words: silhouette = 0 (they were never clustered)\n",
        "        # Only plot polysemous words' silhouette scores — monosemous are always 0\n",
        "        ax.hist(poly, bins=20, color=color, alpha=0.75,\n",
        "                edgecolor=\"white\", linewidth=0.5,\n",
        "                label=f\"Polysemous (k≥2, n={len(poly)})\")\n",
        "        ax.hist(mono, bins=20, color=\"lightgray\", alpha=0.75,\n",
        "                edgecolor=\"white\", linewidth=0.5,\n",
        "                label=f\"Monosemous (k=1, n={len(mono)})\")\n",
        "\n",
        "        # Draw threshold line\n",
        "        ax.axvline(threshold, color=\"red\", linewidth=2, linestyle=\"--\",\n",
        "                   label=f\"Threshold τ={threshold}\")\n",
        "\n",
        "        # Annotate monosemy rate\n",
        "        mono_rate = len(mono) / len(df) * 100\n",
        "        ax.text(threshold + 0.01, ax.get_ylim()[1] * 0.9 if ax.get_ylim()[1] > 0 else 10,\n",
        "                f\"{mono_rate:.1f}%\\nmonosemous\",\n",
        "                color=\"red\", fontsize=9, va=\"top\")\n",
        "\n",
        "        ax.set_xlabel(\"Silhouette Score\", fontsize=11)\n",
        "        ax.set_ylabel(\"Number of Lemmas\", fontsize=11)\n",
        "        ax.set_title(f\"{lang} (n={len(df)})\", fontsize=12)\n",
        "        ax.set_xlim(-0.05, 0.75)\n",
        "        ax.legend(fontsize=9, loc=\"upper right\")\n",
        "        ax.grid(True, alpha=0.3, axis=\"y\")\n",
        "\n",
        "    fig.tight_layout()\n",
        "    path = FIG_DIR / \"silhouette_distribution.png\"\n",
        "    fig.savefig(path, dpi=150, bbox_inches=\"tight\")\n",
        "    plt.close(fig)\n",
        "    print(f\"  [fig] Saved: {path.name}\")"
      ],
      "metadata": {
        "id": "A2wqaZynajnY"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fe3cad1",
        "outputId": "80b9870a-6c15-4820-e069-7ef0c3a9db3c"
      },
      "source": [
        "import nltk\n",
        "nltk.download('omw-1.4')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cad16907",
        "outputId": "86585d22-cfe9-4bc8-aea4-e194f9710715"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "import pandas as pd\n",
        "\n",
        "# Ensure the 'zh' DataFrame is loaded, if not already in memory\n",
        "# (assuming DATA_DIR is defined from previous cells)\n",
        "if 'zh' not in locals():\n",
        "    DATA_DIR = Path(\"/content\") / \"bible_data\"\n",
        "    zh = pd.read_csv(DATA_DIR / \"chinese_wsi_results.csv\")\n",
        "\n",
        "chinese_lemmas = zh['lemma'].tolist()\n",
        "n_with_synsets = sum(1 for lemma in chinese_lemmas\n",
        "                     if len(wn.synsets(lemma, lang=\"cmn\")) > 0)\n",
        "\n",
        "total_lemmas = len(chinese_lemmas)\n",
        "print(f\"Coverage: {n_with_synsets}/{total_lemmas} = {n_with_synsets/total_lemmas:.1%}\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coverage: 146/574 = 25.4%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/nltk/corpus/reader/wordnet.py:2214: UserWarning: cmn: invalid offset 14869976-n in '14869976-n\tcmn:lemma\t污点\n",
            "'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/nltk/corpus/reader/wordnet.py:2214: UserWarning: cmn: invalid offset 14869977-n in '14869977-n\tcmn:lemma\t小斑\n",
            "'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/nltk/corpus/reader/wordnet.py:2214: UserWarning: cmn: invalid offset 15168570-n in '15168570-n\tcmn:lemma\t规定的睡觉时间\n",
            "'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/nltk/corpus/reader/wordnet.py:2214: UserWarning: cmn: invalid offset 15171146-n in '15171146-n\tcmn:lemma\t节日\n",
            "'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/nltk/corpus/reader/wordnet.py:2214: UserWarning: cmn: invalid offset 15171147-n in '15171147-n\tcmn:lemma\t纪念日\n",
            "'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/nltk/corpus/reader/wordnet.py:2214: UserWarning: cmn: invalid offset 15171739-n in '15171739-n\tcmn:lemma\t竞技状态不佳的日子\n",
            "'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/nltk/corpus/reader/wordnet.py:2214: UserWarning: cmn: invalid offset 15171858-n in '15171858-n\tcmn:lemma\t存取时间\n",
            "'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/nltk/corpus/reader/wordnet.py:2214: UserWarning: cmn: invalid offset 15172882-n in '15172882-n\tcmn:lemma\t选举日\n",
            "'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/nltk/corpus/reader/wordnet.py:2214: UserWarning: cmn: invalid offset 15173065-n in '15173065-n\tcmn:lemma\t教会年\n",
            "'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/nltk/corpus/reader/wordnet.py:2214: UserWarning: cmn: invalid offset 15176162-n in '15176162-n\tcmn:lemma\t雾月\n",
            "'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/nltk/corpus/reader/wordnet.py:2214: UserWarning: cmn: invalid offset 15177867-n in '15177867-n\tcmn:lemma\t希伯来历\n",
            "'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/nltk/corpus/reader/wordnet.py:2214: UserWarning: cmn: invalid offset 15178842-n in '15178842-n\tcmn:lemma\t回历\n",
            "'\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1f7bbc42",
        "outputId": "95be5781-a9b4-42cc-c0a0-218e1161b575"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# Ensure the 'en' DataFrame is loaded, if not already in memory\n",
        "# (assuming DATA_DIR is defined from previous cells)\n",
        "if 'en' not in locals():\n",
        "    DATA_DIR = Path(\"/content\") / \"bible_data\"\n",
        "    en = pd.read_csv(DATA_DIR / \"english_wsi_results.csv\")\n",
        "\n",
        "english_lemmas = en['lemma'].tolist()\n",
        "n_with_synsets_en = sum(1 for lemma in english_lemmas\n",
        "                          if len(wn.synsets(lemma.lower(), pos=wn.NOUN)) > 0)\n",
        "\n",
        "total_lemmas_en = len(english_lemmas)\n",
        "print(f\"English WordNet Coverage: {n_with_synsets_en}/{total_lemmas_en} = {n_with_synsets_en/total_lemmas_en:.1%}\")\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English WordNet Coverage: 630/636 = 99.1%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98c2852b",
        "outputId": "e917e0b6-157b-4372-d7c4-7ecb76650881"
      },
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"Step 5: Validation and Statistical Analysis\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "en = pd.read_csv(DATA_DIR / \"english_wsi_results.csv\")\n",
        "zh = pd.read_csv(DATA_DIR / \"chinese_wsi_results.csv\")\n",
        "\n",
        "# ── Validation (English vs. WordNet) ──────────────────────────\n",
        "valid_en, rho_en, p_en = validate_english(en)\n",
        "valid_en.to_csv(OUTPUT_DIR / \"validation_correlation_en.csv\", index=False)\n",
        "\n",
        "# ── Validation (Chinese vs. WordNet) ──────────────────────────\n",
        "valid_zh, rho_zh, p_zh = validate_chinese(zh)\n",
        "valid_zh.to_csv(OUTPUT_DIR / \"validation_correlation_zh.csv\", index=False)\n",
        "\n",
        "# ── Statistical Comparison ────────────────────────────────────\n",
        "# stats_result = mann_whitney_comparison(\n",
        "#     en[\"k_ward\"].values,\n",
        "\n",
        "\n",
        "#     zh[\"k_ward\"].values,\n",
        "# )\n",
        "stats_result = mann_whitney_comparison(\n",
        "    en[\"k_hdbscan\"].values,\n",
        "    zh[\"k_hdbscan\"].values,\n",
        ")\n",
        "stats_df = pd.DataFrame([stats_result])\n",
        "stats_df.to_csv(OUTPUT_DIR / \"statistical_comparison.csv\", index=False)\n",
        "\n",
        "print(\"\\n── Statistical Comparison Results ──\")\n",
        "for k, v in stats_result.items():\n",
        "    print(f\"  {k:22s}: {v}\")\n",
        "\n",
        "# # ── Silhouette score diagnostic ───────────────────────────────\n",
        "# # Shows whether the threshold is calibrated correctly.\n",
        "# # If monosemy rate is 0%, raise SILHOUETTE_THRESHOLD in 04_wsi_clustering.py\n",
        "# # Target: 20-50% monosemous words. Rerun Step 4 after changing threshold.\n",
        "# print(\"\\n── Silhouette Diagnostic (tune threshold if monosemy=0%) ──\")\n",
        "# for label, df_lang in [(\"EN\", en), (\"ZH\", zh)]:\n",
        "#     sil = df_lang[\"silhouette_ward\"]\n",
        "#     # mono_rate = (df_lang[\"k_ward\"] == 1).mean() * 100\n",
        "#     mono_rate = (df_lang[\"k_hdbscan\"] == 1).mean() * 100\n",
        "#     print(f\"  {label} silhouette: \"\n",
        "#             f\"min={sil.min():.3f}  \"\n",
        "#             f\"p25={sil.quantile(0.25):.3f}  \"\n",
        "#             f\"median={sil.median():.3f}  \"\n",
        "#             f\"p75={sil.quantile(0.75):.3f}  \"\n",
        "#             f\"max={sil.max():.3f}\")\n",
        "#     print(f\"  {label} monosemy rate (k=1): {mono_rate:.1f}%\")\n",
        "\n",
        "# ── Figures ───────────────────────────────────────────────────\n",
        "plot_sense_distribution(en, \"english\")\n",
        "plot_sense_distribution(zh, \"chinese\")\n",
        "plot_comparison_boxplot(en, zh)\n",
        "plot_wordnet_correlation(valid_en, rho_en, p_en)\n",
        "# You might want to create a plot for Chinese WordNet correlation as well, if coverage is good\n",
        "# plot_wordnet_correlation_chinese(valid_zh, rho_zh, p_zh) # This would require a new function\n",
        "# plot_silhouette_distribution(en, zh)\n",
        "\n",
        "# ── Interpretation ────────────────────────────────────────────\n",
        "print(\"\\n── Interpretation ──\")\n",
        "if stats_result[\"p_value\"] < 0.05:\n",
        "    direction = \"English\" if stats_result[\"en_mean_k\"] > stats_result[\"zh_mean_k\"] else \"Chinese\"\n",
        "    print(f\"  Significant difference found (p={stats_result['p_value']:.4f}).\")\n",
        "    print(f\"  {direction} nouns show higher mean polysemy degree.\")\n",
        "else:\n",
        "    print(f\"  No significant difference found (p={stats_result['p_value']:.4f}).\")\n",
        "\n",
        "d = abs(stats_result[\"cohens_d\"])\n",
        "magnitude = \"small\" if d < 0.2 else (\"medium\" if d < 0.5 else \"large\")\n",
        "print(f\"  Effect size: Cohen's d = {stats_result['cohens_d']:.3f} ({magnitude})\")\n",
        "print(f\"  Spearman ρ (EN WSI vs. WordNet): {rho_en:.3f} (p={p_en:.4f})\")\n",
        "print(f\"  Spearman ρ (ZH WSI vs. WordNet): {rho_zh:.3f} (p={p_zh:.4f})\")\n",
        "\n",
        "print(\"\\n✓ Step 5 complete.\\n\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Step 5: Validation and Statistical Analysis\n",
            "============================================================\n",
            "  [validate] Looking up Princeton WordNet sense counts…\n",
            "  [validate] EN Spearman ρ(k_hdbscan, WN_senses) = 0.147  p = 0.0002  (n=630)\n",
            "  [validate] Looking up Chinese WordNet sense counts…\n",
            "  [validate] ZH Spearman ρ(k_hdbscan, CWN_senses) = 0.100  p = 0.2481  (n=134)\n",
            "\n",
            "── Statistical Comparison Results ──\n",
            "  en_mean_k             : 1.5692\n",
            "  zh_mean_k             : 1.1376\n",
            "  en_median_k           : 1.0\n",
            "  zh_median_k           : 1.0\n",
            "  en_std_k              : 0.7054\n",
            "  zh_std_k              : 0.418\n",
            "  U_statistic           : 247498.0\n",
            "  p_value               : 0.0\n",
            "  CLES                  : 0.678\n",
            "  cohens_d              : 0.7353\n",
            "  n_en                  : 636\n",
            "  n_zh                  : 574\n",
            "  [fig] Saved: sense_distribution_english.png\n",
            "  [fig] Saved: sense_distribution_chinese.png\n",
            "  [fig] Saved: comparison_violinplot.png\n",
            "  [fig] Saved: wordnet_correlation_en.png\n",
            "\n",
            "── Interpretation ──\n",
            "  Significant difference found (p=0.0000).\n",
            "  English nouns show higher mean polysemy degree.\n",
            "  Effect size: Cohen's d = 0.735 (large)\n",
            "  Spearman ρ (EN WSI vs. WordNet): 0.147 (p=0.0002)\n",
            "  Spearman ρ (ZH WSI vs. WordNet): 0.100 (p=0.2481)\n",
            "\n",
            "✓ Step 5 complete.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6: Qualitative Analysis"
      ],
      "metadata": {
        "id": "O3m1UJnseHjn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Step 6: Qualitative Analysis — Sense Cluster Inspection\n",
        "=========================================================\n",
        "For each polysemous lemma, retrieves representative example verses\n",
        "for each induced sense cluster. This supports the qualitative\n",
        "analysis section of the paper, demonstrating that clusters correspond\n",
        "to meaningful, interpretable senses.\n",
        "\n",
        "Also generates a LaTeX-ready table of top polysemous words\n",
        "for both languages (for paper Table 3).\n",
        "\n",
        "Outputs:\n",
        "  - output/qualitative_en_top_polysemous.txt   (sense examples)\n",
        "  - output/qualitative_zh_top_polysemous.txt\n",
        "  - output/table_top_polysemous_latex.tex       (LaTeX table)\n",
        "  - output/polysemy_profile_comparison.csv      (wide comparison table)\n",
        "\n",
        "Usage:\n",
        "  python 06_qualitative_analysis.py\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "\n",
        "# ─── Configuration ────────────────────────────────────────────────────────────\n",
        "\n",
        "DATA_DIR    = Path(\"/content\") / \"bible_data\"\n",
        "OUTPUT_DIR = Path(\"/content\") / \"output\"\n",
        "# DATA_DIR   = Path(__file__).parent.parent / \"data\"\n",
        "# OUTPUT_DIR = Path(__file__).parent.parent / \"output\"\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "TOP_N_WORDS       = 20   # Top N most polysemous lemmas per language\n",
        "EXAMPLES_PER_SENSE = 2   # Number of example verses per cluster\n",
        "\n",
        "\n",
        "# ─── Sense Example Retrieval ─────────────────────────────────────────────────\n",
        "\n",
        "def get_sense_examples(\n",
        "    lang: str,\n",
        "    top_n: int = TOP_N_WORDS,\n",
        "    examples_per_sense: int = EXAMPLES_PER_SENSE,\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    For the top_n most polysemous lemmas, retrieve example contexts\n",
        "    for each induced sense cluster.\n",
        "\n",
        "    Returns a formatted string ready for a paper's qualitative appendix.\n",
        "    \"\"\"\n",
        "    results_df = pd.read_csv(DATA_DIR / f\"{lang}_wsi_results.csv\")\n",
        "    labels_df  = pd.read_csv(DATA_DIR / f\"{lang}_sense_labels.csv\")\n",
        "    nouns_df   = pd.read_csv(DATA_DIR / f\"{lang}_nouns.csv\")\n",
        "\n",
        "    # Top polysemous by k_ward\n",
        "    poly = results_df[results_df[\"k_ward\"] > 1].nlargest(top_n, \"k_ward\")\n",
        "\n",
        "    # Merge labels with original contexts\n",
        "    merged = labels_df.merge(\n",
        "        nouns_df[[\"lemma\", \"verse_id\", \"context\"]].drop_duplicates(),\n",
        "        on=[\"lemma\", \"verse_id\"],\n",
        "        how=\"left\",\n",
        "    )\n",
        "\n",
        "    lines = []\n",
        "    lines.append(f\"{'='*60}\")\n",
        "    lines.append(f\"QUALITATIVE SENSE ANALYSIS — {lang.upper()}\")\n",
        "    lines.append(f\"Top {top_n} Most Polysemous Nouns (WSI, Ward Clustering)\")\n",
        "    lines.append(f\"{'='*60}\\n\")\n",
        "\n",
        "    for _, row in poly.iterrows():\n",
        "        lemma = row[\"lemma\"]\n",
        "        k     = int(row[\"k_ward\"])\n",
        "        n_occ = int(row[\"n_occurrences\"])\n",
        "        sil   = row.get(\"silhouette_ward\", \"N/A\")\n",
        "\n",
        "        lines.append(f\"Lemma: '{lemma}'  |  k={k}  |  n={n_occ}  |  silhouette={sil}\")\n",
        "        lines.append(\"-\" * 50)\n",
        "\n",
        "        lemma_data = merged[merged[\"lemma\"] == lemma]\n",
        "\n",
        "        for cluster_id in range(k):\n",
        "            cluster_rows = lemma_data[lemma_data[\"cluster_ward\"] == cluster_id]\n",
        "            lines.append(f\"  Sense {cluster_id + 1} ({len(cluster_rows)} occurrences):\")\n",
        "\n",
        "            # Sample diverse examples\n",
        "            sample = cluster_rows.dropna(subset=[\"context\"]).head(examples_per_sense)\n",
        "            for _, ex in sample.iterrows():\n",
        "                ctx = str(ex[\"context\"])[:200].replace(\"\\n\", \" \")\n",
        "                lines.append(f\"    • {ex['verse_id']}: {ctx}\")\n",
        "\n",
        "        lines.append(\"\")\n",
        "\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "\n",
        "# ─── LaTeX Table Generation ──────────────────────────────────────────────────\n",
        "\n",
        "def generate_latex_table(top_n: int = 15) -> str:\n",
        "    \"\"\"\n",
        "    Generate a LaTeX longtable comparing top polysemous nouns in both languages.\n",
        "    Format:\n",
        "      Rank | English Lemma | EN k | Chinese Lemma | ZH k\n",
        "    \"\"\"\n",
        "    en = pd.read_csv(DATA_DIR / \"english_wsi_results.csv\")\n",
        "    zh = pd.read_csv(DATA_DIR / \"chinese_wsi_results.csv\")\n",
        "\n",
        "    en_top = en.nlargest(top_n, \"k_ward\")[[\"lemma\", \"k_ward\", \"n_occurrences\"]].reset_index(drop=True)\n",
        "    zh_top = zh.nlargest(top_n, \"k_ward\")[[\"lemma\", \"k_ward\", \"n_occurrences\"]].reset_index(drop=True)\n",
        "\n",
        "    lines = [\n",
        "        r\"\\begin{table}[h]\",\n",
        "        r\"\\centering\",\n",
        "        r\"\\caption{Top Polysemous Common Nouns by Induced Sense Count (k): English vs. Chinese}\",\n",
        "        r\"\\label{tab:top_polysemous}\",\n",
        "        r\"\\begin{tabular}{clccclcc}\",\n",
        "        r\"\\toprule\",\n",
        "        r\"Rank & English Lemma & EN $k$ & EN $n$ & & Chinese Lemma & ZH $k$ & ZH $n$ \\\\\",\n",
        "        r\"\\midrule\",\n",
        "    ]\n",
        "\n",
        "    for i in range(top_n):\n",
        "        en_row = en_top.iloc[i] if i < len(en_top) else None\n",
        "        zh_row = zh_top.iloc[i] if i < len(zh_top) else None\n",
        "\n",
        "        en_lemma = en_row[\"lemma\"]                  if en_row is not None else \"\"\n",
        "        en_k     = int(en_row[\"k_ward\"])            if en_row is not None else \"\"\n",
        "        en_n     = int(en_row[\"n_occurrences\"])     if en_row is not None else \"\"\n",
        "        zh_lemma = zh_row[\"lemma\"]                  if zh_row is not None else \"\"\n",
        "        zh_k     = int(zh_row[\"k_ward\"])            if zh_row is not None else \"\"\n",
        "        zh_n     = int(zh_row[\"n_occurrences\"])     if zh_row is not None else \"\"\n",
        "\n",
        "        lines.append(\n",
        "            f\"{i+1} & {en_lemma} & {en_k} & {en_n} & & {zh_lemma} & {zh_k} & {zh_n} \\\\\\\\\"\n",
        "        )\n",
        "\n",
        "    lines += [\n",
        "        r\"\\bottomrule\",\n",
        "        r\"\\end{tabular}\",\n",
        "        r\"\\end{table}\",\n",
        "    ]\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "\n",
        "# ─── Wide Comparison Profile ─────────────────────────────────────────────────\n",
        "\n",
        "def generate_comparison_profile() -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Create a summary comparison table for paper Table 2.\n",
        "    \"\"\"\n",
        "    en = pd.read_csv(DATA_DIR / \"english_wsi_results.csv\")\n",
        "    zh = pd.read_csv(DATA_DIR / \"chinese_wsi_results.csv\")\n",
        "\n",
        "    def profile(df: pd.DataFrame, lang: str) -> dict:\n",
        "        return {\n",
        "            \"Language\":            lang,\n",
        "            \"Total lemmas\":        len(df),\n",
        "            \"Mean k (Ward)\":       round(df[\"k_ward\"].mean(), 3),\n",
        "            \"Median k (Ward)\":     round(df[\"k_ward\"].median(), 3),\n",
        "            \"Std k (Ward)\":        round(df[\"k_ward\"].std(ddof=1), 3),\n",
        "            \"% Monosemous (k=1)\":  round((df[\"k_ward\"] == 1).mean() * 100, 1),\n",
        "            \"% Polysemous (k>1)\":  round((df[\"k_ward\"] > 1).mean() * 100, 1),\n",
        "            \"Max k\":               int(df[\"k_ward\"].max()),\n",
        "            \"Mean k (KMeans)\":     round(df[\"k_kmeans\"].mean(), 3),\n",
        "            \"Agreement (Ward=KM)\": round((df[\"k_ward\"] == df[\"k_kmeans\"]).mean() * 100, 1),\n",
        "        }\n",
        "\n",
        "    rows = [profile(en, \"English\"), profile(zh, \"Chinese\")]\n",
        "    return pd.DataFrame(rows)"
      ],
      "metadata": {
        "id": "d6Rm4f_vdq7A"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "\n",
        "# ─── Configuration ────────────────────────────────────────────────────────────\n",
        "\n",
        "# DATA_DIR and OUTPUT_DIR are defined in previous cells and are globally accessible.\n",
        "# For example, they are set in cell d6Rm4f_vdq7A.\n",
        "# DATA_DIR    = Path(\"/content\") / \"bible_data\"\n",
        "# OUTPUT_DIR = Path(\"/content\") / \"output\"\n",
        "# OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "TOP_N_WORDS       = 20   # Top N most polysemous lemmas per language\n",
        "EXAMPLES_PER_SENSE = 2   # Number of example verses per cluster\n",
        "\n",
        "# ─── Sense Example Retrieval (Corrected) ─────────────────────────────────────────────────\n",
        "\n",
        "def get_sense_examples(\n",
        "    lang: str,\n",
        "    top_n: int = TOP_N_WORDS,\n",
        "    examples_per_sense: int = EXAMPLES_PER_SENSE,\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    For the top_n most polysemous lemmas, retrieve example contexts\n",
        "    for each induced sense cluster.\n",
        "\n",
        "    Returns a formatted string ready for a paper's qualitative appendix.\n",
        "    \"\"\"\n",
        "    results_df = pd.read_csv(DATA_DIR / f\"{lang}_wsi_results.csv\")\n",
        "    labels_df  = pd.read_csv(DATA_DIR / f\"{lang}_sense_labels.csv\")\n",
        "    nouns_df   = pd.read_csv(DATA_DIR / f\"{lang}_nouns.csv\")\n",
        "\n",
        "    # Top polysemous by k_hdbscan (was k_ward)\n",
        "    poly = results_df[results_df[\"k_hdbscan\"] > 1].nlargest(top_n, \"k_hdbscan\")\n",
        "\n",
        "    # Merge labels with original contexts\n",
        "    merged = labels_df.merge(\n",
        "        nouns_df[[\"lemma\", \"verse_id\", \"context\"]].drop_duplicates(),\n",
        "        on=[\"lemma\", \"verse_id\"],\n",
        "        how=\"left\",\n",
        "    )\n",
        "\n",
        "    lines = []\n",
        "    lines.append(f\"{'='*60}\")\n",
        "    lines.append(f\"QUALITATIVE SENSE ANALYSIS — {lang.upper()}\")\n",
        "    # Updated title to reflect HDBSCAN\n",
        "    lines.append(f\"Top {top_n} Most Polysemous Nouns (WSI, HDBSCAN Clustering)\")\n",
        "    lines.append(f\"{'='*60}\\n\")\n",
        "\n",
        "    for _, row in poly.iterrows():\n",
        "        lemma = row[\"lemma\"]\n",
        "        # Use k_hdbscan\n",
        "        k     = int(row[\"k_hdbscan\"])\n",
        "        n_occ = int(row[\"n_instances\"])\n",
        "        # silhouette_ward is not available for HDBSCAN, so remove or set to N/A\n",
        "        # sil   = row.get(\"silhouette_ward\", \"N/A\") # Removed\n",
        "\n",
        "        # Updated output line, removed silhouette score\n",
        "        lines.append(f\"Lemma: '{lemma}'  |  k={k}  |  n={n_occ}\")\n",
        "        lines.append(\"-\" * 50)\n",
        "\n",
        "        lemma_data = merged[merged[\"lemma\"] == lemma]\n",
        "\n",
        "        for cluster_id in range(k):\n",
        "            # Use cluster_hdbscan\n",
        "            cluster_rows = lemma_data[lemma_data[\"cluster_hdbscan\"] == cluster_id]\n",
        "            lines.append(f\"  Sense {cluster_id + 1} ({len(cluster_rows)} occurrences):\")\n",
        "\n",
        "            # Sample diverse examples\n",
        "            sample = cluster_rows.dropna(subset=[\"context\"]).head(examples_per_sense)\n",
        "            for _, ex in sample.iterrows():\n",
        "                ctx = str(ex[\"context\"])[:200].replace(\"\\n\", \" \")\n",
        "                lines.append(f\"    • {ex['verse_id']}: {ctx}\")\n",
        "\n",
        "        lines.append(\"\")\n",
        "\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "\n",
        "# ─── LaTeX Table Generation (Corrected) ──────────────────────────────────────────────────\n",
        "\n",
        "def generate_latex_table(top_n: int = 15) -> str:\n",
        "    \"\"\"\n",
        "    Generate a LaTeX longtable comparing top polysemous nouns in both languages.\n",
        "    Format:\n",
        "      Rank | English Lemma | EN k | Chinese Lemma | ZH k\n",
        "    \"\"\"\n",
        "    en = pd.read_csv(DATA_DIR / \"english_wsi_results.csv\")\n",
        "    zh = pd.read_csv(DATA_DIR / \"chinese_wsi_results.csv\")\n",
        "\n",
        "    # Use k_hdbscan for nlargest and column selection\n",
        "    en_top = en.nlargest(top_n, \"k_hdbscan\")[[\"lemma\", \"k_hdbscan\", \"n_instances\"]].reset_index(drop=True)\n",
        "    zh_top = zh.nlargest(top_n, \"k_hdbscan\")[[\"lemma\", \"k_hdbscan\", \"n_instances\"]].reset_index(drop=True)\n",
        "\n",
        "    lines = [\n",
        "        r\"\\begin{table}[h]\",\n",
        "        r\"\\centering\",\n",
        "        r\"\\caption{Top Polysemous Common Nouns by Induced Sense Count (k, HDBSCAN): English vs. Chinese}\", # Updated caption\n",
        "        r\"\\label{tab:top_polysemous}\",\n",
        "        r\"\\begin{tabular}{clccclcc}\",\n",
        "        r\"\\toprule\",\n",
        "        r\"Rank & English Lemma & EN $k$ & EN $n$ & & Chinese Lemma & ZH $k$ & ZH $n$ \\\\\",\n",
        "        r\"\\midrule\",\n",
        "    ]\n",
        "\n",
        "    for i in range(top_n):\n",
        "        en_row = en_top.iloc[i] if i < len(en_top) else None\n",
        "        zh_row = zh_top.iloc[i] if i < len(zh_top) else None\n",
        "\n",
        "        en_lemma = en_row[\"lemma\"]                  if en_row is not None else \"\"\n",
        "        en_k     = int(en_row[\"k_hdbscan\"])            if en_row is not None else \"\" # Use k_hdbscan\n",
        "        en_n     = int(en_row[\"n_instances\"])     if en_row is not None else \"\"\n",
        "        zh_lemma = zh_row[\"lemma\"]                  if zh_row is not None else \"\"\n",
        "        zh_k     = int(zh_row[\"k_hdbscan\"])            if zh_row is not None else \"\" # Use k_hdbscan\n",
        "        zh_n     = int(zh_row[\"n_instances\"])     if zh_row is not None else \"\"\n",
        "\n",
        "        lines.append(\n",
        "            f\"{i+1} & {en_lemma} & {en_k} & {en_n} & & {zh_lemma} & {zh_k} & {zh_n} \\\\\"\n",
        "        )\n",
        "\n",
        "    lines += [\n",
        "        r\"\\bottomrule\",\n",
        "        r\"\\end{tabular}\",\n",
        "        r\"\\end{table}\",\n",
        "    ]\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "# ─── Wide Comparison Profile (Copied for context) ─────────────────────────────────────────────────\n",
        "\n",
        "def generate_comparison_profile() -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Create a summary comparison table for paper Table 2.\n",
        "    \"\"\"\n",
        "    en = pd.read_csv(DATA_DIR / \"english_wsi_results.csv\")\n",
        "    zh = pd.read_csv(DATA_DIR / \"chinese_wsi_results.csv\")\n",
        "\n",
        "    def profile(df: pd.DataFrame, lang: str) -> dict:\n",
        "        return {\n",
        "            \"Language\":            lang,\n",
        "            \"Total lemmas\":        len(df),\n",
        "            \"Mean k (HDBSCAN)\":    round(df[\"k_hdbscan\"].mean(), 3), # Use k_hdbscan\n",
        "            \"Median k (HDBSCAN)\":  round(df[\"k_hdbscan\"].median(), 3), # Use k_hdbscan\n",
        "            \"Std k (HDBSCAN)\":     round(df[\"k_hdbscan\"].std(ddof=1), 3), # Use k_hdbscan\n",
        "            \"% Monosemous (k=1)\":  round((df[\"k_hdbscan\"] == 1).mean() * 100, 1), # Use k_hdbscan\n",
        "            \"% Polysemous (k>1)\":  round((df[\"k_hdbscan\"] > 1).mean() * 100, 1), # Use k_hdbscan\n",
        "            \"Max k\":               int(df[\"k_hdbscan\"].max()), # Use k_hdbscan\n",
        "            # Removed KMeans related metrics as they are no longer generated\n",
        "            # \"Mean k (KMeans)\":     round(df[\"k_kmeans\"].mean(), 3),\n",
        "            # \"Agreement (Ward=KM)\": round((df[\"k_ward\"] == df[\"k_kmeans\"]).mean() * 100, 1),\n",
        "        }\n",
        "\n",
        "    rows = [profile(en, \"English\"), profile(zh, \"Chinese\")]\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Step 6: Qualitative Analysis\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ── Sense examples ────────────────────────────────────────────\n",
        "for lang in [\"english\", \"chinese\"]:\n",
        "    text = get_sense_examples(lang)\n",
        "    out  = OUTPUT_DIR / f\"qualitative_{lang}_top_polysemous.txt\"\n",
        "    out.write_text(text, encoding=\"utf-8\")\n",
        "    print(f\"  [saved] {out.name}\")\n",
        "\n",
        "# ── LaTeX table ───────────────────────────────────────────────\n",
        "latex = generate_latex_table(top_n=15)\n",
        "tex_path = OUTPUT_DIR / \"table_top_polysemous_latex.tex\"\n",
        "tex_path.write_text(latex, encoding=\"utf-8\")\n",
        "print(f\"  [saved] {tex_path.name}\")\n",
        "\n",
        "# ── Comparison profile ────────────────────────────────────────\n",
        "profile = generate_comparison_profile()\n",
        "csv_path = OUTPUT_DIR / \"polysemy_profile_comparison.csv\"\n",
        "profile.to_csv(csv_path, index=False)\n",
        "print(f\"  [saved] {csv_path.name}\")\n",
        "print()\n",
        "print(profile.to_string(index=False))\n",
        "\n",
        "print(\"\\n✓ Step 6 complete.\\n\")"
      ],
      "metadata": {
        "id": "K9Gd4DVWeOYw",
        "outputId": "b9b1d6e3-71e1-4bf8-ac55-c61180ed783c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Step 6: Qualitative Analysis\n",
            "============================================================\n",
            "  [saved] qualitative_english_top_polysemous.txt\n",
            "  [saved] qualitative_chinese_top_polysemous.txt\n",
            "  [saved] table_top_polysemous_latex.tex\n",
            "  [saved] polysemy_profile_comparison.csv\n",
            "\n",
            "Language  Total lemmas  Mean k (HDBSCAN)  Median k (HDBSCAN)  Std k (HDBSCAN)  % Monosemous (k=1)  % Polysemous (k>1)  Max k\n",
            " English           636             1.569                 1.0            0.705                52.8                47.2      5\n",
            " Chinese           574             1.138                 1.0            0.418                88.2                11.8      5\n",
            "\n",
            "✓ Step 6 complete.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r output_niv_cht_hdbscan.zip /content/output/"
      ],
      "metadata": {
        "id": "wnUe-3lgedhv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b1e2101-e78e-4256-cb2b-a3d7637b36f2"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/output/ (stored 0%)\n",
            "  adding: content/output/polysemy_profile_comparison.csv (deflated 24%)\n",
            "  adding: content/output/validation_correlation_zh.csv (deflated 47%)\n",
            "  adding: content/output/qualitative_english_top_polysemous.txt (deflated 67%)\n",
            "  adding: content/output/statistical_comparison.csv (deflated 30%)\n",
            "  adding: content/output/figures/ (stored 0%)\n",
            "  adding: content/output/figures/sense_distribution_english.png (deflated 18%)\n",
            "  adding: content/output/figures/wordnet_correlation_en.png (deflated 12%)\n",
            "  adding: content/output/figures/comparison_violinplot.png (deflated 9%)\n",
            "  adding: content/output/figures/sense_distribution_chinese.png (deflated 19%)\n",
            "  adding: content/output/validation_correlation_en.csv (deflated 54%)\n",
            "  adding: content/output/qualitative_chinese_top_polysemous.txt (deflated 65%)\n",
            "  adding: content/output/table_top_polysemous_latex.tex (deflated 43%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r data_niv_cht_hdbscan.zip /content/bible_data/"
      ],
      "metadata": {
        "id": "Pj8aPu8Cfp1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14395f8d-737e-4120-c242-8344b4e64451"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/bible_data/ (stored 0%)\n",
            "  adding: content/bible_data/english_embeddings.npz (deflated 0%)\n",
            "  adding: content/bible_data/chinese_nouns.csv (deflated 85%)\n",
            "  adding: content/bible_data/chinese_sense_labels.csv (deflated 82%)\n",
            "  adding: content/bible_data/english_wsi_results.csv (deflated 54%)\n",
            "  adding: content/bible_data/english_verses.csv (deflated 68%)\n",
            "  adding: content/bible_data/chinese_wsi_results.csv (deflated 50%)\n",
            "  adding: content/bible_data/chinese_embeddings.npz (deflated 1%)\n",
            "  adding: content/bible_data/chinese_verses.csv (deflated 65%)\n",
            "  adding: content/bible_data/chinese_noun_freq.csv (deflated 53%)\n",
            "  adding: content/bible_data/english_noun_freq.csv (deflated 56%)\n",
            "  adding: content/bible_data/english_sense_labels.csv (deflated 81%)\n",
            "  adding: content/bible_data/english_nouns.csv (deflated 88%)\n",
            "  adding: content/bible_data/jieba_biblical_dict.txt (deflated 70%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf output\n",
        "# !rm -rf bible_data"
      ],
      "metadata": {
        "id": "Xco5z30hrNhT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ac45311"
      },
      "source": [
        "# Visualize Word Sense Clusters\n",
        "To visualize the word sense clusters for 'bread' and '餅', I will perform the following steps:\n",
        "\n",
        "1.  **Install UMAP**: Ensure the `umap-learn` library is installed for dimensionality reduction.\n",
        "2.  **Load Data**: Load the English and Chinese word embeddings (`english_embeddings.npz`, `chinese_embeddings.npz`) and sense labels (`english_sense_labels.csv`, `chinese_sense_labels.csv`).\n",
        "3.  **Filter by Lemma**: Select data specifically for the lemma 'bread' in English and '餅' (bǐng, meaning 'cake' or 'flatbread') in Chinese.\n",
        "4.  **Dimensionality Reduction**: Apply UMAP to reduce the high-dimensional embeddings of these lemmas to two dimensions (2D) for easier plotting.\n",
        "5.  **Plot Sense Clusters**: Create scatter plots for both 'bread' and '餅', where each point represents an occurrence of the word, and its color indicates the induced sense cluster. Add a legend to distinguish between senses.\n",
        "6.  **Review Plots**: Examine the generated plots to visually assess the separation and distinctness of the induced sense clusters for both lemmas.\n",
        "\n",
        "This visualization will help in qualitatively understanding how well the clustering algorithm separated the different meanings of these words based on their contextual embeddings.\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import umap\n",
        "from pathlib import Path\n",
        "\n",
        "# Configuration\n",
        "DATA_DIR = Path(\"/content\") / \"bible_data\"\n",
        "OUTPUT_DIR = Path(\"/content\") / \"output\"\n",
        "FIG_DIR = OUTPUT_DIR / \"figures\"\n",
        "FIG_DIR.mkdir(exist_ok=True) # Ensure figures directory exists\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "# --- Function to load data and filter by lemma ---\n",
        "def load_lemma_data(lang: str, lemma: str):\n",
        "    \"\"\"Loads embeddings and sense labels for a specific lemma.\"\"\"\n",
        "    embeddings_file = DATA_DIR / f\"{lang}_embeddings.npz\"\n",
        "    labels_file = DATA_DIR / f\"{lang}_sense_labels.csv\"\n",
        "\n",
        "    print(f\"Loading data for {lang} lemma '{lemma}'...\")\n",
        "\n",
        "    # Load embeddings and lemmas from .npz\n",
        "    data = np.load(embeddings_file, allow_pickle=True)\n",
        "    all_embeddings = data[\"embeddings\"]\n",
        "    all_lemmas = data[\"lemmas\"]\n",
        "    all_verse_ids = data[\"verse_ids\"]\n",
        "\n",
        "    # Filter for the specific lemma\n",
        "    lemma_mask = (all_lemmas == lemma)\n",
        "    lemma_embeddings = all_embeddings[lemma_mask]\n",
        "    lemma_verse_ids = all_verse_ids[lemma_mask]\n",
        "\n",
        "    # Load sense labels\n",
        "    labels_df = pd.read_csv(labels_file)\n",
        "    # Filter labels for the specific lemma and relevant verse_ids\n",
        "    lemma_labels_df = labels_df[\n",
        "        (labels_df[\"lemma\"] == lemma) & (labels_df[\"verse_id\"].isin(lemma_verse_ids))\n",
        "    ].copy()\n",
        "\n",
        "    # Ensure the order of labels matches the order of embeddings\n",
        "    # This assumes verse_ids in embeddings are unique enough or can be joined reliably\n",
        "    # If not, a more robust merge is needed using original index or full context\n",
        "    # For now, we'll align by verse_id, assuming embeddings and labels are generated from the same source order\n",
        "    lemma_labels_df = lemma_labels_df.set_index('verse_id').loc[lemma_verse_ids].reset_index()\n",
        "\n",
        "    if len(lemma_embeddings) != len(lemma_labels_df):\n",
        "        print(f\"Warning: Mismatch in count for '{lemma}'. Embeddings: {len(lemma_embeddings)}, Labels: {len(lemma_labels_df)}\")\n",
        "        # Attempt to reconcile by matching contexts if available, or just proceed with common.\n",
        "        # For simplicity, proceeding assuming verse_ids align after filtering\n",
        "        # If this causes issues, more rigorous data alignment might be needed.\n",
        "\n",
        "    # Only keep Ward clustering labels for visualization as per plan\n",
        "    sense_labels = lemma_labels_df[\"cluster_ward\"].values\n",
        "\n",
        "    print(f\"Found {len(lemma_embeddings)} occurrences for '{lemma}'.\")\n",
        "    return lemma_embeddings, sense_labels, lemma_verse_ids\n",
        "\n",
        "# --- Function for UMAP dimensionality reduction ---\n",
        "def apply_umap(embeddings: np.ndarray, n_components: int = 2):\n",
        "    \"\"\"Applies UMAP for dimensionality reduction.\"\"\"\n",
        "    print(f\"Applying UMAP to reduce embeddings to {n_components}D...\")\n",
        "    reducer = umap.UMAP(\n",
        "        n_components=n_components,\n",
        "        random_state=RANDOM_STATE,\n",
        "        metric='cosine', # Good for high-dimensional data like embeddings\n",
        "        n_jobs=-1 # Use all available cores\n",
        "    )\n",
        "    reduced_embeddings = reducer.fit_transform(embeddings)\n",
        "    return reduced_embeddings\n",
        "\n",
        "# --- Function to plot sense clusters ---\n",
        "def plot_sense_clusters(\n",
        "    reduced_embeddings: np.ndarray,\n",
        "    sense_labels: np.ndarray,\n",
        "    lemma: str,\n",
        "    lang: str,\n",
        "):\n",
        "    \"\"\"Generates a scatter plot of 2D embeddings, colored by sense cluster.\"\"\"\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    unique_senses = sorted(np.unique(sense_labels))\n",
        "\n",
        "    # Use a color palette suitable for categorical data\n",
        "    palette = sns.color_palette(\"tab10\", n_colors=len(unique_senses))\n",
        "\n",
        "    for i, sense_id in enumerate(unique_senses):\n",
        "        mask = (sense_labels == sense_id)\n",
        "        plt.scatter(\n",
        "            reduced_embeddings[mask, 0],\n",
        "            reduced_embeddings[mask, 1],\n",
        "            label=f\"Sense {sense_id + 1}\",\n",
        "            color=palette[i],\n",
        "            alpha=0.7,\n",
        "            s=50, # size of points\n",
        "            edgecolors='w', # white edge for better visibility\n",
        "            linewidth=0.5\n",
        "        )\n",
        "\n",
        "    plt.title(f\"UMAP Visualization of {lang.capitalize()} '{lemma}' Sense Clusters\", fontsize=14)\n",
        "    plt.xlabel(\"UMAP Component 1\", fontsize=12)\n",
        "    plt.ylabel(\"UMAP Component 2\", fontsize=12)\n",
        "    plt.legend(title=\"Sense Clusters\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    plt.grid(True, linestyle='--', alpha=0.6)\n",
        "    plt.tight_layout() # Adjust layout to prevent labels from overlapping\n",
        "    plot_filename = FIG_DIR / f\"{lang}_{lemma}_sense_clusters_umap.png\"\n",
        "    plt.savefig(plot_filename, dpi=300)\n",
        "    plt.show()\n",
        "    print(f\"Saved plot to {plot_filename}\")\n",
        "\n",
        "# --- Main execution ---\n",
        "print(\"--- Starting Sense Cluster Visualization ---\")\n",
        "\n",
        "# 1. English 'bread'\n",
        "en_lemma = \"bread\"\n",
        "en_embeddings, en_labels, _ = load_lemma_data(\"english\", en_lemma)\n",
        "if len(en_embeddings) > 0:\n",
        "    en_reduced_embeddings = apply_umap(en_embeddings)\n",
        "    plot_sense_clusters(en_reduced_embeddings, en_labels, en_lemma, \"english\")\n",
        "else:\n",
        "    print(f\"No embeddings found for English lemma '{en_lemma}'. Skipping visualization.\")\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "# 2. Chinese '餅'\n",
        "zh_lemma = \"餅\"\n",
        "zh_embeddings, zh_labels, _ = load_lemma_data(\"chinese\", zh_lemma)\n",
        "if len(zh_embeddings) > 0:\n",
        "    zh_reduced_embeddings = apply_umap(zh_embeddings)\n",
        "    plot_sense_clusters(zh_reduced_embeddings, zh_labels, zh_lemma, \"chinese\")\n",
        "else:\n",
        "    print(f\"No embeddings found for Chinese lemma '{zh_lemma}'. Skipping visualization.\")\n",
        "\n",
        "print(\"\\n--- Sense Cluster Visualization Complete ---\")\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import umap\n",
        "from pathlib import Path\n",
        "import matplotlib.font_manager as fm\n",
        "import os\n",
        "\n",
        "# Configuration\n",
        "DATA_DIR = Path(\"/content\") / \"bible_data\"\n",
        "OUTPUT_DIR = Path(\"/content\") / \"output\"\n",
        "FIG_DIR = OUTPUT_DIR / \"figures\"\n",
        "FIG_DIR.mkdir(exist_ok=True) # Ensure figures directory exists\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "# --- Function to load data and filter by lemma ---\n",
        "def load_lemma_data(lang: str, lemma: str):\n",
        "    \"\"\"\n",
        "    Loads embeddings and sense labels for a specific lemma, ensuring proper alignment.\n",
        "    This version slices directly from the full embeddings and labels arrays using global indices.\n",
        "    \"\"\"\n",
        "    embeddings_file = DATA_DIR / f\"{lang}_embeddings.npz\"\n",
        "    labels_file = DATA_DIR / f\"{lang}_sense_labels.csv\"\n",
        "\n",
        "    print(f\"Loading data for {lang} lemma '{lemma}'...\")\n",
        "\n",
        "    if not embeddings_file.exists():\n",
        "        raise FileNotFoundError(f\"Embeddings file not found: {embeddings_file}\")\n",
        "    if not labels_file.exists():\n",
        "        raise FileNotFoundError(f\"Labels file not found: {labels_file}\")\n",
        "\n",
        "    # Load all data from the .npz file\n",
        "    npz_data = np.load(embeddings_file, allow_pickle=True)\n",
        "    all_embeddings_npz = npz_data[\"embeddings\"]\n",
        "    all_lemmas_npz = npz_data[\"lemmas\"]\n",
        "\n",
        "    # Load all sense labels from the .csv file\n",
        "    labels_df_full = pd.read_csv(labels_file)\n",
        "    all_cluster_ward_labels_full = labels_df_full['cluster_ward'].values\n",
        "\n",
        "    # Identify the global indices of occurrences for the target lemma\n",
        "    lemma_indices = np.where(all_lemmas_npz == lemma)[0]\n",
        "\n",
        "    if len(lemma_indices) == 0:\n",
        "        print(f\"No occurrences found for lemma '{lemma}' in {lang} data. Skipping.\")\n",
        "        return np.array([]), np.array([]), np.array([])\n",
        "\n",
        "    # Slice the embeddings and labels using these global indices\n",
        "    lemma_embeddings = all_embeddings_npz[lemma_indices]\n",
        "    lemma_sense_labels = all_cluster_ward_labels_full[lemma_indices]\n",
        "\n",
        "    # Final sanity check on lengths\n",
        "    if len(lemma_embeddings) != len(lemma_sense_labels):\n",
        "        raise ValueError(\n",
        "            f\"Final alignment failed for lemma '{lemma}'. \"\n",
        "            f\"Embeddings count: {len(lemma_embeddings)}, Labels count: {len(lemma_sense_labels)}. \"\n",
        "            \"This indicates a deep inconsistency in data generation which should be investigated in Step 2/3/4.\"\n",
        "        )\n",
        "\n",
        "    print(f\"Found {len(lemma_embeddings)} occurrences for '{lemma}', with aligned labels.\")\n",
        "    # Return None for verse_ids as they are not needed for plotting and the new method doesn't explicitly track them here.\n",
        "    return lemma_embeddings, lemma_sense_labels, None\n",
        "\n",
        "# --- Function for UMAP dimensionality reduction ---\n",
        "def apply_umap(embeddings: np.ndarray, n_components: int = 2):\n",
        "    \"\"\"Applies UMAP for dimensionality reduction.\"\"\"\n",
        "    print(f\"Applying UMAP to reduce embeddings to {n_components}D...\")\n",
        "    reducer = umap.UMAP(\n",
        "        n_components=n_components,\n",
        "        random_state=RANDOM_STATE,\n",
        "        metric='cosine', # Good for high-dimensional data like embeddings\n",
        "        n_jobs=-1 # Use all available cores\n",
        "    )\n",
        "    reduced_embeddings = reducer.fit_transform(embeddings)\n",
        "    return reduced_embeddings\n",
        "\n",
        "# --- Function to plot sense clusters ---\n",
        "def plot_sense_clusters(\n",
        "    reduced_embeddings: np.ndarray,\n",
        "    sense_labels: np.ndarray,\n",
        "    lemma: str,\n",
        "    lang: str,\n",
        "):\n",
        "    \"\"\"Generates a scatter plot of 2D embeddings, colored by sense cluster.\"\"\"\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    unique_senses = sorted(np.unique(sense_labels))\n",
        "\n",
        "    # Use a color palette suitable for categorical data\n",
        "    palette = sns.color_palette(\"tab10\", n_colors=len(unique_senses))\n",
        "\n",
        "    for i, sense_id in enumerate(unique_senses):\n",
        "        mask = (sense_labels == sense_id)\n",
        "        plt.scatter(\n",
        "            reduced_embeddings[mask, 0],\n",
        "            reduced_embeddings[mask, 1],\n",
        "            label=f\"Sense {sense_id + 1}\",\n",
        "            color=palette[i],\n",
        "            alpha=0.7,\n",
        "            s=50, # size of points\n",
        "            edgecolors='w', # white edge for better visibility\n",
        "            linewidth=0.5\n",
        "        )\n",
        "\n",
        "    plt.title(f\"UMAP Visualization of {lang.capitalize()} '{lemma}' Sense Clusters\", fontsize=14)\n",
        "    plt.xlabel(\"UMAP Component 1\", fontsize=12)\n",
        "    plt.ylabel(\"UMAP Component 2\", fontsize=12)\n",
        "    plt.legend(title=\"Sense Clusters\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    plt.grid(True, linestyle='--', alpha=0.6)\n",
        "    plt.tight_layout() # Adjust layout to prevent labels from overlapping\n",
        "    plot_filename = FIG_DIR / f\"{lang}_{lemma}_sense_clusters_umap.png\"\n",
        "    plt.savefig(plot_filename, dpi=300)\n",
        "    plt.show()\n",
        "    print(f\"Saved plot to {plot_filename}\")\n",
        "\n",
        "# --- Main execution ---\n",
        "print(\"--- Starting Sense Cluster Visualization ---\")\n",
        "\n",
        "# --- Font Configuration for Chinese Characters ---\n",
        "# Install a Chinese font (e.g., Noto Sans CJK)\n",
        "!apt-get update -qq > /dev/null\n",
        "!apt-get install -qq fonts-noto-cjk > /dev/null\n",
        "\n",
        "# Clear and rebuild matplotlib font cache\n",
        "# This is crucial for newly installed fonts to be recognized\n",
        "fm._load_fontmanager(try_read_cache=False)\n",
        "fm._fmcache = {} # Clear the font cache manually\n",
        "print(\"Matplotlib font cache cleared.\")\n",
        "\n",
        "# Find the path to a Noto Sans CJK font\n",
        "font_files = fm.findSystemFonts(fontpaths=None, fontext='ttf')\n",
        "chosen_font_path = None\n",
        "for fpath in font_files:\n",
        "    if 'NotoSansCJK' in fpath:\n",
        "        chosen_font_path = fpath\n",
        "        break\n",
        "\n",
        "if chosen_font_path:\n",
        "    print(f\"Found Noto Sans CJK font at: {chosen_font_path}\")\n",
        "    # Add this specific font to font manager\n",
        "    fm.fontManager.addfont(chosen_font_path)\n",
        "\n",
        "    # Get the actual font name from the file\n",
        "    prop = fm.FontProperties(fname=chosen_font_path)\n",
        "    actual_font_name = prop.get_name()\n",
        "    print(f\"Actual font family name: {actual_font_name}\")\n",
        "\n",
        "    # Set generic font family to 'sans-serif'\n",
        "    plt.rcParams['font.family'] = ['sans-serif']\n",
        "    # Add the Chinese font to the beginning of the 'sans-serif' font list\n",
        "    # This makes it the preferred font for sans-serif text.\n",
        "    plt.rcParams['font.sans-serif'] = [actual_font_name] + plt.rcParams['font.sans-serif']\n",
        "    plt.rcParams['axes.unicode_minus'] = False # Fix minus sign display issues\n",
        "    # Set font size for better readability\n",
        "    plt.rcParams[\"font.size\"] = 11\n",
        "    plt.rcParams[\"axes.titlesize\"] = 13\n",
        "    plt.rcParams[\"axes.labelsize\"] = 12\n",
        "    print(f\"Matplotlib configured to use '{actual_font_name}'.\")\n",
        "else:\n",
        "    print(f\"Warning: No Noto Sans CJK font found after install. Chinese characters might not display correctly.\")\n",
        "\n",
        "# 1. English 'bread'\n",
        "en_lemma = \"bread\"\n",
        "en_embeddings, en_labels, _ = load_lemma_data(\"english\", en_lemma)\n",
        "if len(en_embeddings) > 0:\n",
        "    en_reduced_embeddings = apply_umap(en_embeddings)\n",
        "    plot_sense_clusters(en_reduced_embeddings, en_labels, en_lemma, \"english\")\n",
        "else:\n",
        "    print(f\"No embeddings found for English lemma '{en_lemma}'. Skipping visualization.\")\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "# 2. Chinese '禮'\n",
        "# The user requested '禮' instead of '餅' in the previous run, so I'll keep '禮' for consistency with the traceback.\n",
        "zh_lemma = \"禮\"\n",
        "zh_embeddings, zh_labels, _ = load_lemma_data(\"chinese\", zh_lemma)\n",
        "if len(zh_embeddings) > 0:\n",
        "    zh_reduced_embeddings = apply_umap(zh_embeddings)\n",
        "    plot_sense_clusters(zh_reduced_embeddings, zh_labels, zh_lemma, \"chinese\")\n",
        "else:\n",
        "    print(f\"No embeddings found for Chinese lemma '{zh_lemma}'. Skipping visualization.\")\n",
        "\n",
        "print(\"\\n--- Sense Cluster Visualization Complete ---\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfoirPbZ4jrf",
        "outputId": "77a84a8a-bf6a-4e42-e7e8-7cb8a250a675"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Sense Cluster Visualization ---\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Matplotlib font cache cleared.\n",
            "Found Noto Sans CJK font at: /usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc\n",
            "Actual font family name: Noto Sans CJK JP\n",
            "Matplotlib configured to use 'Noto Sans CJK JP'.\n",
            "Loading data for english lemma 'bread'...\n",
            "Found 254 occurrences for 'bread', with aligned labels.\n",
            "Applying UMAP to reduce embeddings to 2D...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved plot to /content/output/figures/english_bread_sense_clusters_umap.png\n",
            "\n",
            "\n",
            "Loading data for chinese lemma '禮'...\n",
            "Found 147 occurrences for '禮', with aligned labels.\n",
            "Applying UMAP to reduce embeddings to 2D...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved plot to /content/output/figures/chinese_禮_sense_clusters_umap.png\n",
            "\n",
            "--- Sense Cluster Visualization Complete ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "950f8253",
        "outputId": "daf3ab9e-af82-40ab-ec47-ed070ac59885"
      },
      "source": [
        "print(\"--- Quantitative Metrics Comparison ---\")\n",
        "\n",
        "# Metrics for English 'bread'\n",
        "en_bread_metrics = en[en['lemma'] == 'bread']\n",
        "print(\"\\nEnglish 'bread' metrics:\")\n",
        "display(en_bread_metrics[['lemma', 'n_instances', 'k_ward', 'k_kmeans', 'silhouette_ward', 'agreement_pct']])\n",
        "\n",
        "# Metrics for Chinese '禮'\n",
        "zh_li_metrics = zh[zh['lemma'] == '禮']\n",
        "print(\"\\nChinese '禮' metrics:\")\n",
        "display(zh_li_metrics[['lemma', 'n_instances', 'k_ward', 'k_kmeans', 'silhouette_ward', 'agreement_pct']])\n",
        "\n",
        "print(\"\\n--- End of Quantitative Metrics Comparison ---\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Quantitative Metrics Comparison ---\n",
            "\n",
            "English 'bread' metrics:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "    lemma  n_instances  k_ward  k_kmeans  silhouette_ward  agreement_pct\n",
              "59  bread          254       8         5           0.3638           56.3"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f488b025-3d62-48e0-9191-046126846fd6\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>lemma</th>\n",
              "      <th>n_instances</th>\n",
              "      <th>k_ward</th>\n",
              "      <th>k_kmeans</th>\n",
              "      <th>silhouette_ward</th>\n",
              "      <th>agreement_pct</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>bread</td>\n",
              "      <td>254</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>0.3638</td>\n",
              "      <td>56.3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f488b025-3d62-48e0-9191-046126846fd6')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f488b025-3d62-48e0-9191-046126846fd6 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f488b025-3d62-48e0-9191-046126846fd6');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"print(\\\"\\\\n--- End of Quantitative Metrics Comparison ---\\\")\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"lemma\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"bread\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"n_instances\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 254,\n        \"max\": 254,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          254\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"k_ward\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 8,\n        \"max\": 8,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          8\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"k_kmeans\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 5,\n        \"max\": 5,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"silhouette_ward\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.3637999892234802,\n        \"max\": 0.3637999892234802,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.3637999892234802\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"agreement_pct\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 56.3,\n        \"max\": 56.3,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          56.3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chinese '禮' metrics:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "    lemma  n_instances  k_ward  k_kmeans  silhouette_ward  agreement_pct\n",
              "385     禮          147       2         7           0.3775          39.46"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-80c91c0b-1efe-4b75-a076-7e2a887b8226\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>lemma</th>\n",
              "      <th>n_instances</th>\n",
              "      <th>k_ward</th>\n",
              "      <th>k_kmeans</th>\n",
              "      <th>silhouette_ward</th>\n",
              "      <th>agreement_pct</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>385</th>\n",
              "      <td>禮</td>\n",
              "      <td>147</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>0.3775</td>\n",
              "      <td>39.46</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-80c91c0b-1efe-4b75-a076-7e2a887b8226')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-80c91c0b-1efe-4b75-a076-7e2a887b8226 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-80c91c0b-1efe-4b75-a076-7e2a887b8226');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"print(\\\"\\\\n--- End of Quantitative Metrics Comparison ---\\\")\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"lemma\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"\\u79ae\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"n_instances\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 147,\n        \"max\": 147,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          147\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"k_ward\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 2,\n        \"max\": 2,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"k_kmeans\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 7,\n        \"max\": 7,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          7\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"silhouette_ward\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.3774999976158142,\n        \"max\": 0.3774999976158142,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.3774999976158142\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"agreement_pct\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 39.46,\n        \"max\": 39.46,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          39.46\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- End of Quantitative Metrics Comparison ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = pd.read_csv(\"/content/bible_data/chinese_wsi_results.csv\")\n",
        "polysemous = results[results['k_hdbscan'] > 1].sort_values('n_instances', ascending=False)\n",
        "print(polysemous[['lemma', 'n_instances', 'k_hdbscan']].head(20))"
      ],
      "metadata": {
        "id": "SPzgYOhpf8ps",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7245a686-35d3-4c14-ae39-d4be5b73ae6b"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    lemma  n_instances  k_hdbscan\n",
            "16      人         9228          2\n",
            "111     地         1378          2\n",
            "378    祭司          857          2\n",
            "363    眾人          663          2\n",
            "162    子孫          429          4\n",
            "236    支派          374          2\n",
            "534    門徒          335          2\n",
            "159    婦人          312          2\n",
            "559    首領          276          2\n",
            "439     肘          269          4\n",
            "133     壇          267          2\n",
            "414    罪孽          241          2\n",
            "214    惡人          224          2\n",
            "105     國          221          2\n",
            "444     腳          202          2\n",
            "9      主人          182          2\n",
            "51     全地          174          2\n",
            "142    天使          173          2\n",
            "423    義人          166          2\n",
            "318     燈          131          2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_en = pd.read_csv(\"/content/bible_data/english_wsi_results.csv\")\n",
        "polysemous_en = results_en[results_en['k_hdbscan'] > 1].sort_values('n_instances', ascending=False)\n",
        "print(polysemous_en[['lemma', 'n_instances', 'k_hdbscan']].head(20))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4iiRKxib6pI",
        "outputId": "59576ca7-349b-45da-a027-c71caa893b7f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        lemma  n_instances  k_hdbscan\n",
            "322       man         3927          2\n",
            "513       son         2929          2\n",
            "288      king         2533          4\n",
            "131       day         2037          2\n",
            "248      hand         1296          3\n",
            "186    father         1289          3\n",
            "364  offering         1026          2\n",
            "624      word          984          2\n",
            "563      time          950          4\n",
            "416    priest          903          2\n",
            "632      year          857          4\n",
            "392     place          852          4\n",
            "603       way          818          3\n",
            "88       city          814          2\n",
            "63    brother          784          2\n",
            "254     heart          781          2\n",
            "480   servant          767          2\n",
            "557     thing          745          3\n",
            "350    nation          708          2\n",
            "500       sin          668          2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Rxq7hlf0cuzU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}