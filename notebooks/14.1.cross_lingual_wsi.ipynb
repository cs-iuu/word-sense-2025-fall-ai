{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMLmnoTUe4TAGnzkwoImXG8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cs-iuu/word-sense-2025-fall-ai/blob/main/notebooks/14.1.cross_lingual_wsi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cross-Lingual Word Sense Induction (WSI)\n",
        "\n",
        "\n",
        "POS tagging using A1: \"lincoln/multilingual-xlm-roberta-base-ud-pos\"\n",
        "\n",
        "Features:\n",
        "- POS tagging (A1 multilingual XLM-R UD POS)\n",
        "- Context extraction filtered by POS\n",
        "- Contextual token embeddings using xlm-roberta-base\n",
        "- Clustering (KMeans) for Word Sense Induction (WSI)\n",
        "- Automatic sense descriptions & LLM (optional) glosses\n",
        "- Cross-lingual gloss alignment (Hungarian matching + many-to-one)\n",
        "- Visualizations (UMAP)\n",
        "- Polysemy metrics and paired comparisons\n",
        "- Mongolian ↔ English using XLM-R contextual embeddings"
      ],
      "metadata": {
        "id": "NgPXcHvZCVVC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "GQiPH8b9CgS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers sentencepiece sentence-transformers umap-learn scikit-learn matplotlib scipy tqdm stanza"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbB6QiKXCik6",
        "outputId": "632191ff-62d4-4426-9e25-a01e281d8429"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.6/1.7 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/608.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m608.4/608.4 kB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Optional: OpenAI for high-quality glosses (set use_openai=True and provide API key via env)"
      ],
      "metadata": {
        "id": "80cLLqSPttB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q openai"
      ],
      "metadata": {
        "id": "g7lEhXAQtntL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "HRHN61k6Cl3U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import math\n",
        "import random\n",
        "from collections import Counter, defaultdict\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForTokenClassification\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import umap\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "\n",
        "\n",
        "# For optional stanza-based fallback\n",
        "import stanza\n",
        "\n",
        "\n",
        "# Optional OpenAI\n",
        "import openai"
      ],
      "metadata": {
        "id": "EafCU4FxCo69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Settings"
      ],
      "metadata": {
        "id": "kZnVCuAEt6lZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RANDOM_SEED = 42\n",
        "random.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "\n",
        "# POS model choice: A1 (user selected)\n",
        "# POS_MODEL = \"lincoln/multilingual-xlm-roberta-base-ud-pos\"\n",
        "POS_MODEL = \"jordigonzm/mdeberta-v3-base-multilingual-pos-tagger\"\n",
        "# Embedding encoder (contextual): XLM-R\n",
        "EMB_MODEL = \"xlm-roberta-base\"\n",
        "\n",
        "\n",
        "# Optional: use OpenAI for gloss generation\n",
        "USE_OPENAI = False\n",
        "OPENAI_MODEL = \"gpt-4o-mini\" # example\n",
        "\n",
        "\n",
        "# Target words (user-provided)\n",
        "MONGOLIAN_TARGETS = ['зам', 'гэр', 'амар', 'сайн', 'хүн', 'ам']\n",
        "ENGLISH_TARGETS = ['road', 'home', 'rest', 'good', 'person', 'mouth']\n",
        "TARGET_PAIRS = list(zip(MONGOLIAN_TARGETS, ENGLISH_TARGETS))\n",
        "\n",
        "\n",
        "# Clustering defaults\n",
        "DEFAULT_K = 2\n",
        "SIMILARITY_THRESHOLD = 0.35"
      ],
      "metadata": {
        "id": "h7kkKyuRt8co"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load models"
      ],
      "metadata": {
        "id": "Q4kCIV9cCu41"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Access the HF_TOKEN from Colab secrets\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "\n",
        "print(\"Loading POS model (A1):\", POS_MODEL)\n",
        "pos_tokenizer = AutoTokenizer.from_pretrained(POS_MODEL, token=hf_token)\n",
        "pos_model = AutoModelForTokenClassification.from_pretrained(POS_MODEL, token=hf_token)\n",
        "label_map = pos_model.config.id2label # mapping id->label\n",
        "\n",
        "\n",
        "print(\"Loading embedding encoder:\", EMB_MODEL)\n",
        "emb_tokenizer = AutoTokenizer.from_pretrained(EMB_MODEL, token=hf_token)\n",
        "emb_model = AutoModel.from_pretrained(EMB_MODEL, token=hf_token)\n",
        "emb_model.eval()\n",
        "\n",
        "\n",
        "# Optional: small sentence-transformers model for sentence-level fallback (not required)\n",
        "from sentence_transformers import SentenceTransformer\n",
        "sbert_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_B3qFCNAC3ZR",
        "outputId": "fcaa4fcb-8616-4a6e-a489-ef9aa6f860a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading POS model (A1): jordigonzm/mdeberta-v3-base-multilingual-pos-tagger\n",
            "Loading embedding encoder: xlm-roberta-base\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define target words and load the corpus"
      ],
      "metadata": {
        "id": "UpHpTzTKDPeG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import math\n",
        "import random\n",
        "from collections import Counter, defaultdict\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForTokenClassification\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import umap\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "\n",
        "\n",
        "# For optional stanza-based fallback\n",
        "import stanza\n",
        "\n",
        "\n",
        "# Optional OpenAI\n",
        "import openai\n",
        "# ---------------------------\n",
        "# UTILITY: POS tagging with token-classification model\n",
        "# We'll align wordpieces back to textual words using SentencePiece marker ' '.\n",
        "# This assumes the tokenizer is a SentencePiece-based tokenizer with   markers (true for XLM-R).\n",
        "# ---------------------------\n",
        "\n",
        "def pos_tag_sentence(sentence):\n",
        "    \"\"\"\n",
        "    Returns list of (word, UPOS) for the sentence using the token-classification model.\n",
        "    \"\"\"\n",
        "    tokens = pos_tokenizer(sentence, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        outputs = pos_model(**tokens)\n",
        "    preds = outputs.logits.argmax(-1).squeeze().tolist()\n",
        "    token_ids = tokens.input_ids[0].tolist()\n",
        "    token_strs = pos_tokenizer.convert_ids_to_tokens(token_ids)\n",
        "\n",
        "    results = []\n",
        "    current_word = None\n",
        "    current_labels = []\n",
        "\n",
        "    # Iterate over tokens ignoring special tokens\n",
        "    for tok, lab_id in zip(token_strs, preds):\n",
        "        if tok in pos_tokenizer.all_special_tokens:\n",
        "            continue\n",
        "        if tok.startswith(\" \"):\n",
        "            # start of new wordpiece\n",
        "            if current_word is not None:\n",
        "                # consolidate label: majority label\n",
        "                if current_labels:\n",
        "                    lbl = Counter(current_labels).most_common(1)[0][0]\n",
        "                else:\n",
        "                    lbl = None\n",
        "                results.append((current_word, lbl))\n",
        "            current_word = tok[1:]\n",
        "            current_labels = [label_map[lab_id]]\n",
        "        else:\n",
        "            # continuation\n",
        "            if current_word is None:\n",
        "                current_word = tok\n",
        "                current_labels = [label_map[lab_id]]\n",
        "            else:\n",
        "                current_word += tok.replace(' ', '')\n",
        "                current_labels.append(label_map[lab_id])\n",
        "\n",
        "    # flush\n",
        "    if current_word is not None:\n",
        "        if current_labels:\n",
        "            lbl = Counter(current_labels).most_common(1)[0][0]\n",
        "        else:\n",
        "            lbl = None\n",
        "        results.append((current_word, lbl))\n",
        "\n",
        "    return results\n",
        "\n",
        "# Quick test (comment/uncomment for quick run)\n",
        "# print(pos_tag_sentence(\"Тэр хүн гэртээ очсон.\"))\n",
        "\n",
        "# ---------------------------\n",
        "# CONTEXT EXTRACTION FILTERED BY POS\n",
        "# ---------------------------\n",
        "\n",
        "def extract_pos_filtered_contexts(corpus, target_word, desired_pos):\n",
        "    \"\"\"Return list of sentences from corpus where target_word appears with desired_pos tag.\"\"\"\n",
        "    out = []\n",
        "    for sent in corpus:\n",
        "        try:\n",
        "            tags = pos_tag_sentence(sent)\n",
        "        except Exception as e:\n",
        "            # fallback: simple substring match\n",
        "            if target_word in sent:\n",
        "                out.append(sent)\n",
        "            continue\n",
        "        for tok, pos in tags:\n",
        "            if tok == target_word and pos == desired_pos:\n",
        "                out.append(sent)\n",
        "                break\n",
        "    return out\n",
        "\n",
        "# ---------------------------\n",
        "# CONTEXTUAL TOKEN EMBEDDING EXTRACTION\n",
        "# We'll extract token-level embeddings by matching token ids of target word.\n",
        "# ---------------------------\n",
        "\n",
        "def get_contextual_embedding(sentence, target_word):\n",
        "    \"\"\"Return averaged contextual embedding for target_word in sentence, or None if not found.\"\"\"\n",
        "    inputs = emb_tokenizer(sentence, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        outputs = emb_model(**inputs)\n",
        "    last_hidden = outputs.last_hidden_state.squeeze(0)  # seq_len x dim\n",
        "\n",
        "    # Get tokenized target and ids\n",
        "    target_tokens = emb_tokenizer.tokenize(target_word)\n",
        "    target_ids = emb_tokenizer.convert_tokens_to_ids(target_tokens)\n",
        "\n",
        "    ids = inputs['input_ids'].squeeze(0).tolist()\n",
        "\n",
        "    # find matching spans\n",
        "    positions = []\n",
        "    for i in range(len(ids) - len(target_ids) + 1):\n",
        "        if ids[i:i+len(target_ids)] == target_ids:\n",
        "            positions.append(range(i, i+len(target_ids)))\n",
        "\n",
        "    if not positions:\n",
        "        return None\n",
        "\n",
        "    vecs = []\n",
        "    for pos in positions:\n",
        "        subvecs = last_hidden[list(pos)]  # len_sub x dim\n",
        "        vecs.append(subvecs.mean(dim=0).numpy())\n",
        "\n",
        "    return np.mean(np.stack(vecs, axis=0), axis=0)\n",
        "\n",
        "# ---------------------------\n",
        "# CLUSTERING / WSI\n",
        "# ---------------------------\n",
        "\n",
        "def cluster_embeddings(embeddings, k=2):\n",
        "    if len(embeddings) == 0:\n",
        "        return None, None\n",
        "    k = min(k, len(embeddings))\n",
        "    if k <= 1:\n",
        "        labels = np.zeros(len(embeddings), dtype=int)\n",
        "        centers = np.array([embeddings.mean(axis=0)])\n",
        "        return labels, centers\n",
        "    km = KMeans(n_clusters=k, random_state=RANDOM_SEED).fit(embeddings)\n",
        "    return km.labels_, km.cluster_centers_\n",
        "\n",
        "# ---------------------------\n",
        "# SENSE DESCRIPTION: keywords + representative sentence\n",
        "# ---------------------------\n",
        "\n",
        "def describe_sense(sentences, embeddings, labels, cluster_id, top_k=6):\n",
        "    idxs = [i for i, lbl in enumerate(labels) if lbl == cluster_id]\n",
        "    cluster_sents = [sentences[i] for i in idxs]\n",
        "    cluster_embs = embeddings[idxs]\n",
        "\n",
        "    centroid = cluster_embs.mean(axis=0)\n",
        "    sims = cosine_similarity([centroid], cluster_embs)[0]\n",
        "    rep_rel = np.argmax(sims)\n",
        "    rep_sentence = cluster_sents[rep_rel]\n",
        "\n",
        "    # simple keyword extraction: token frequency excluding stopwords (language-agnostic)\n",
        "    words = []\n",
        "    for s in cluster_sents:\n",
        "        clean = re.sub(r\"[^\\w\\s]\", \" \", s)\n",
        "        words.extend([w.lower() for w in clean.split() if len(w) > 1])\n",
        "    most_common = [w for w, c in Counter(words).most_common(top_k)]\n",
        "\n",
        "    return {\n",
        "        'cluster_id': cluster_id,\n",
        "        'keywords': most_common,\n",
        "        'example': rep_sentence,\n",
        "        'sentences': cluster_sents,\n",
        "        'centroid': centroid\n",
        "    }\n",
        "\n",
        "# ---------------------------\n",
        "# GLOSS GENERATION (heuristic or OpenAI)\n",
        "# ---------------------------\n",
        "\n",
        "def heuristic_gloss(keywords, example):\n",
        "    kw = \", \".join(keywords[:6])\n",
        "    return f\"A sense related to: {kw}. Example: \\\"{example}\\\"\"\n",
        "\n",
        "\n",
        "def generate_gloss_openai(keywords, example, language='en'):\n",
        "    if not USE_OPENAI:\n",
        "        return heuristic_gloss(keywords, example)\n",
        "    prompt = f\"Write a one-sentence dictionary-style gloss in {language}.\\nKeywords: {keywords}\\nExample: \\\"{example}\\\"\\nReturn only a short gloss.\"\n",
        "    try:\n",
        "        resp = openai.ChatCompletion.create(\n",
        "            model=OPENAI_MODEL,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=0.0,\n",
        "            max_tokens=60\n",
        "        )\n",
        "        gloss = resp.choices[0].message['content'].strip()\n",
        "        return gloss\n",
        "    except Exception as e:\n",
        "        print(\"OpenAI error:\", e)\n",
        "        return heuristic_gloss(keywords, example)\n",
        "\n",
        "# ---------------------------\n",
        "# EMBED GLOSS TEXT (using embedding encoder by averaging token vectors)\n",
        "# ---------------------------\n",
        "\n",
        "def embed_text_with_xlm(text):\n",
        "    inputs = emb_tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=128)\n",
        "    with torch.no_grad():\n",
        "        outs = emb_model(**inputs)\n",
        "    vec = outs.last_hidden_state.mean(dim=1).squeeze(0).numpy()\n",
        "    return vec\n",
        "\n",
        "# ---------------------------\n",
        "# ALIGNMENT between MO and EN clusters\n",
        "# ---------------------------\n",
        "\n",
        "def align_clusters(mo_descs, en_descs, mode='gloss', similarity_threshold=SIMILARITY_THRESHOLD):\n",
        "    mo_ids = list(mo_descs.keys())\n",
        "    en_ids = list(en_descs.keys())\n",
        "\n",
        "    if mode == 'gloss':\n",
        "        mo_mat = np.vstack([mo_descs[i]['gloss_emb'] for i in mo_ids])\n",
        "        en_mat = np.vstack([en_descs[j]['gloss_emb'] for j in en_ids])\n",
        "    else:\n",
        "        mo_mat = np.vstack([mo_descs[i]['centroid'] for i in mo_ids])\n",
        "        en_mat = np.vstack([en_descs[j]['centroid'] for j in en_ids])\n",
        "\n",
        "    sim = cosine_similarity(mo_mat, en_mat)\n",
        "\n",
        "    # Hungarian optimum (maximize) => minimize negative\n",
        "    cost = -sim\n",
        "    row_ind, col_ind = linear_sum_assignment(cost)\n",
        "    one_to_one = []\n",
        "    for r, c in zip(row_ind, col_ind):\n",
        "        one_to_one.append({'mo_cluster': mo_ids[r], 'en_cluster': en_ids[c], 'similarity': float(sim[r,c])})\n",
        "\n",
        "    many_to_one = []\n",
        "    for i, mid in enumerate(mo_ids):\n",
        "        for j, eid in enumerate(en_ids):\n",
        "            if sim[i,j] >= similarity_threshold:\n",
        "                many_to_one.append({'mo_cluster': mid, 'en_cluster': eid, 'similarity': float(sim[i,j])})\n",
        "\n",
        "    return {'sim_matrix': sim, 'one_to_one': one_to_one, 'many_to_one': many_to_one, 'mo_ids': mo_ids, 'en_ids': en_ids}\n",
        "\n",
        "# ---------------------------\n",
        "# VISUALIZATION\n",
        "# ---------------------------\n",
        "\n",
        "def visualize_crosslingual(mo_word, en_word, mo_embs, en_embs, mo_labels, en_labels):\n",
        "    combined = np.vstack([mo_embs, en_embs])\n",
        "\n",
        "    if combined.shape[0] < 3: # Require at least 3 points for meaningful UMAP and to avoid scipy.linalg.eigh error\n",
        "        print(f\"Not enough points to visualize {mo_word} ↔ {en_word}. Need at least 3, got {combined.shape[0]}.\")\n",
        "        return\n",
        "\n",
        "    # Adjust n_neighbors for small datasets\n",
        "    # UMAP n_neighbors must be at least 2 for some internal calculations\n",
        "    n_neighbors_val = min(15, combined.shape[0] - 1)\n",
        "    if n_neighbors_val < 2 and combined.shape[0] > 1:\n",
        "        n_neighbors_val = 2 # If 2 points, n_neighbors can be 1, but UMAP prefers >=2.\n",
        "    elif combined.shape[0] == 1:\n",
        "        print(f\"Not enough unique neighbors for UMAP for {mo_word} ↔ {en_word}. Skipping visualization.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        reducer = umap.UMAP(random_state=RANDOM_SEED, n_neighbors=n_neighbors_val)\n",
        "        reduced = reducer.fit_transform(combined)\n",
        "    except Exception as e:\n",
        "        print(f\"UMAP visualization failed for {mo_word} ↔ {en_word} with {combined.shape[0]} points. Error: {e}. Skipping visualization.\")\n",
        "        return\n",
        "\n",
        "    n_mo = len(mo_embs)\n",
        "\n",
        "    plt.figure(figsize=(8,6))\n",
        "    # Mongolian (circles)\n",
        "    for lbl in sorted(set(mo_labels)):\n",
        "        idxs = [i for i in range(n_mo) if mo_labels[i] == lbl]\n",
        "        if not idxs: continue\n",
        "        pts = reduced[idxs]\n",
        "        plt.scatter(pts[:,0], pts[:,1], marker='o', label=f'MO {mo_word} S{lbl}')\n",
        "    # English (triangles)\n",
        "    for lbl in sorted(set(en_labels)):\n",
        "        idxs = [n_mo + i for i in range(len(en_embs)) if en_labels[i] == lbl]\n",
        "        if not idxs: continue\n",
        "        pts = reduced[idxs]\n",
        "        plt.scatter(pts[:,0], pts[:,1], marker='^', label=f'EN {en_word} S{lbl}')\n",
        "\n",
        "    plt.title(f\"WSI: {mo_word} ↔ {en_word}\")\n",
        "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# ---------------------------\n",
        "# POLYSEMY METRICS\n",
        "# ---------------------------\n",
        "\n",
        "def induce_k_for_word(embs, k_max=6):\n",
        "    if len(embs) < 2:\n",
        "        return 1\n",
        "    best_k = 1\n",
        "    best_score = -1.0\n",
        "    for k in range(2, min(k_max, len(embs))+1):\n",
        "        km = KMeans(n_clusters=k, random_state=RANDOM_SEED).fit(embs)\n",
        "        try:\n",
        "            from sklearn.metrics import silhouette_score\n",
        "            score = silhouette_score(embs, km.labels_)\n",
        "        except Exception:\n",
        "            score = -1\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_k = k\n",
        "    return best_k\n",
        "\n",
        "def compute_polysemy_for_word(embs):\n",
        "    k = induce_k_for_word(embs, k_max=6)\n",
        "    if k <= 1:\n",
        "        return {'S':1, 'k':1}\n",
        "    km = KMeans(n_clusters=k, random_state=RANDOM_SEED).fit(embs)\n",
        "    sizes = np.bincount(km.labels_)\n",
        "    centroid_sim = float(np.mean(cosine_similarity(km.cluster_centers_)))\n",
        "    dom_prop = float(sizes.max() / sizes.sum())\n",
        "    return {'S':k, 'sizes':sizes.tolist(), 'centroid_sim':centroid_sim, 'dominance':dom_prop}\n",
        "\n",
        "# ---------------------------\n",
        "# HIGH-LEVEL PER-WORD PIPELINE\n",
        "# ---------------------------\n",
        "\n",
        "def run_wsi_for_pair(mo_word, en_word, mongolian_corpus, english_corpus, desired_pos='NOUN', k=DEFAULT_K):\n",
        "    print('\\n' + '='*60)\n",
        "    print(f\"Processing pair: {mo_word} ↔ {en_word} (POS={desired_pos})\")\n",
        "\n",
        "    # 1) contexts filtered by POS\n",
        "    mo_contexts = extract_pos_filtered_contexts(mongolian_corpus, mo_word, desired_pos)\n",
        "    en_contexts = extract_pos_filtered_contexts(english_corpus, en_word, desired_pos)\n",
        "\n",
        "    print(f\"Found {len(mo_contexts)} Mongolian contexts, {len(en_contexts)} English contexts\")\n",
        "    if len(mo_contexts) < 1 or len(en_contexts) < 1:\n",
        "        print(\"Not enough contexts; skipping\")\n",
        "        return None\n",
        "\n",
        "    # 2) embeddings for each occurrence\n",
        "    mo_embs = []\n",
        "    for s in mo_contexts:\n",
        "        emb = get_contextual_embedding(s, mo_word)\n",
        "        if emb is not None:\n",
        "            mo_embs.append(emb)\n",
        "    mo_embs = np.vstack(mo_embs) if mo_embs else np.zeros((0, emb_model.config.hidden_size))\n",
        "\n",
        "    en_embs = []\n",
        "    for s in en_contexts:\n",
        "        emb = get_contextual_embedding(s, en_word)\n",
        "        if emb is not None:\n",
        "            en_embs.append(emb)\n",
        "    en_embs = np.vstack(en_embs) if en_embs else np.zeros((0, emb_model.config.hidden_size))\n",
        "\n",
        "    if len(mo_embs) == 0 or len(en_embs) == 0:\n",
        "        print(\"No valid embeddings found; skipping\")\n",
        "        return None\n",
        "\n",
        "    # 3) cluster separately (choose k by silhouette or fixed)\n",
        "    mo_k = induce_k_for_word(mo_embs, k_max=6)\n",
        "    en_k = induce_k_for_word(en_embs, k_max=6)\n",
        "    mo_labels, mo_centers = cluster_embeddings(mo_embs, k=mo_k)\n",
        "    en_labels, en_centers = cluster_embeddings(en_embs, k=en_k)\n",
        "\n",
        "    print(f\"Mongolian clusters: {mo_k}, English clusters: {en_k}\")\n",
        "\n",
        "    # 4) describe clusters + gloss generation\n",
        "    mo_descs = {}\n",
        "    for cid in sorted(set(mo_labels)):\n",
        "        d = describe_sense(mo_contexts, mo_embs, mo_labels, cid)\n",
        "        d['gloss'] = generate_gloss_openai(d['keywords'], d['example'], language='mn' if True else 'en')\n",
        "        d['gloss_emb'] = embed_text_with_xlm(d['gloss'])\n",
        "        mo_descs[cid] = d\n",
        "\n",
        "    en_descs = {}\n",
        "    for cid in sorted(set(en_labels)):\n",
        "        d = describe_sense(en_contexts, en_embs, en_labels, cid)\n",
        "        d['gloss'] = generate_gloss_openai(d['keywords'], d['example'], language='en')\n",
        "        d['gloss_emb'] = embed_text_with_xlm(d['gloss'])\n",
        "        en_descs[cid] = d\n",
        "\n",
        "    # 5) alignment\n",
        "    alignment = align_clusters(mo_descs, en_descs, mode='gloss', similarity_threshold=SIMILARITY_THRESHOLD)\n",
        "\n",
        "    # 6) visualization\n",
        "    visualize_crosslingual(mo_word, en_word, mo_embs, en_embs, mo_labels, en_labels)\n",
        "\n",
        "    # 7) polysemy metrics\n",
        "    mo_poly = compute_polysemy_for_word(mo_embs)\n",
        "    en_poly = compute_polysemy_for_word(en_embs)\n",
        "\n",
        "    result = {\n",
        "        'mo_word': mo_word,\n",
        "        'en_word': en_word,\n",
        "        'mo_contexts': mo_contexts,\n",
        "        'en_contexts': en_contexts,\n",
        "        'mo_embs': mo_embs,\n",
        "        'en_embs': en_embs,\n",
        "        'mo_labels': mo_labels,\n",
        "        'en_labels': en_labels,\n",
        "        'mo_descs': mo_descs,\n",
        "        'en_descs': en_descs,\n",
        "        'alignment': alignment,\n",
        "        'mo_poly': mo_poly,\n",
        "        'en_poly': en_poly\n",
        "    }\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "P7LwyL7v3qvF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Execute"
      ],
      "metadata": {
        "id": "3_zYOQQimC9l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ---------------------------\n",
        "# RUN EXAMPLE on toy corpora (replace with your real corpora)\n",
        "# ---------------------------\n",
        "if __name__ == '__main__':\n",
        "    # Example small corpora; replace these with file reads\n",
        "    mongolian_corpus = [\n",
        "        \"Би зам дагуу алхаж байна.\",\n",
        "        \"Энэ зам урт.\",\n",
        "        \"Манай гэр уулын орой дээр.\",\n",
        "        \"Тэр сайхан гэр барьсан.\",\n",
        "        \"Би өнөөдөр амарсан.\",\n",
        "        \"Амар амгалан орчин.\",\n",
        "        \"Тэр сайн хүн.\",\n",
        "        \"Хүн өөрөө үнэ цэнтэй.\",\n",
        "        \"Намайг ам гэдэг.\",\n",
        "        \"Ам нь жижиг.\"\n",
        "    ]\n",
        "\n",
        "    english_corpus = [\n",
        "        \"The road is long.\",\n",
        "        \"I walked along the road.\",\n",
        "        \"Our home is warm.\",\n",
        "        \"He built a beautiful home.\",\n",
        "        \"I rested today.\",\n",
        "        \"A peaceful rest is important.\",\n",
        "        \"She is a good person.\",\n",
        "        \"Every person has value.\",\n",
        "        \"His mouth is small.\",\n",
        "        \"Open your mouth.\"\n",
        "    ]\n",
        "\n",
        "    # Run for each pair (POS=NOUN as default) - change desired_pos to 'VERB' as needed\n",
        "    results = {}\n",
        "    for mo_word, en_word in TARGET_PAIRS:\n",
        "        res = run_wsi_for_pair(mo_word, en_word, mongolian_corpus, english_corpus, desired_pos='NOUN')\n",
        "        if res:\n",
        "            results[f\"{mo_word}_{en_word}\"] = res\n",
        "\n",
        "    # Save results to disk for inspection\n",
        "    with open('wsi_results_sample.json', 'w', encoding='utf8') as f:\n",
        "        json.dump({k: {\n",
        "            'mo_word': v['mo_word'],\n",
        "            'en_word': v['en_word'],\n",
        "            'mo_poly': v['mo_poly'],\n",
        "            'en_poly': v['en_poly'],\n",
        "            'alignment_one_to_one': v['alignment']['one_to_one']\n",
        "        } for k,v in results.items()}, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print('\\nSaved sample results to wsi_results_sample.json')\n",
        "\n",
        "# ---------------------------\n",
        "# NOTES / NEXT STEPS\n",
        "# ---------------------------\n",
        "# - Replace small example corpora with your real parallel Bible files (aligned by line).\n",
        "# - If you want to enable OpenAI glosses, set USE_OPENAI=True and set OPENAI_API_KEY in the environment.\n",
        "# - Consider adding lemmatization (stanza) if you want to extract lemma-level contexts rather than surface forms.\n",
        "# - For large corpora, consider batching calls to embedding model and pos model for speed.\n",
        "# - To perform full-scale experiments, save per-word results (embeddings, labels, glosses) and run the polysemy statistics module on the sampled word pairs.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SS92UdmmDZ3",
        "outputId": "333f2109-bb1c-45ea-bf36-6c8b8ae595a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "Processing pair: зам ↔ road (POS=NOUN)\n",
            "Found 0 Mongolian contexts, 0 English contexts\n",
            "Not enough contexts; skipping\n",
            "\n",
            "============================================================\n",
            "Processing pair: гэр ↔ home (POS=NOUN)\n",
            "Found 0 Mongolian contexts, 0 English contexts\n",
            "Not enough contexts; skipping\n",
            "\n",
            "============================================================\n",
            "Processing pair: амар ↔ rest (POS=NOUN)\n",
            "Found 0 Mongolian contexts, 0 English contexts\n",
            "Not enough contexts; skipping\n",
            "\n",
            "============================================================\n",
            "Processing pair: сайн ↔ good (POS=NOUN)\n",
            "Found 0 Mongolian contexts, 0 English contexts\n",
            "Not enough contexts; skipping\n",
            "\n",
            "============================================================\n",
            "Processing pair: хүн ↔ person (POS=NOUN)\n",
            "Found 0 Mongolian contexts, 0 English contexts\n",
            "Not enough contexts; skipping\n",
            "\n",
            "============================================================\n",
            "Processing pair: ам ↔ mouth (POS=NOUN)\n",
            "Found 0 Mongolian contexts, 0 English contexts\n",
            "Not enough contexts; skipping\n",
            "\n",
            "Saved sample results to wsi_results_sample.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fDAsAO4sCNyl"
      },
      "outputs": [],
      "source": [
        "# ==============================================\n",
        "# Generate automatic “sense descriptions”\n",
        "# ==============================================\n",
        "\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "def describe_sense(sentences, embeddings, labels, cluster_id, top_k=5):\n",
        "    \"\"\"\n",
        "    Returns keywords and representative sentence for a sense cluster.\n",
        "    \"\"\"\n",
        "    # Extract cluster sentences\n",
        "    idx = [i for i, lbl in enumerate(labels) if lbl == cluster_id]\n",
        "    cluster_sents = [sentences[i] for i in idx]\n",
        "    cluster_embs  = embeddings[idx]\n",
        "\n",
        "    # 1. Compute centroid\n",
        "    centroid = cluster_embs.mean(axis=0)\n",
        "\n",
        "    # 2. Find most typical (closest) sentence\n",
        "    sims = cosine_similarity([centroid], cluster_embs)[0]\n",
        "    closest_idx = idx[int(np.argmax(sims))]\n",
        "    representative_sentence = sentences[closest_idx]\n",
        "\n",
        "    # 3. Extract keywords (very simple tokenizer)\n",
        "    words = []\n",
        "    for s in cluster_sents:\n",
        "        clean = re.sub(r\"[^\\w\\s]\", \"\", s)\n",
        "        words.extend(clean.lower().split())\n",
        "\n",
        "    most_common = [w for w, c in Counter(words).most_common(top_k)]\n",
        "\n",
        "    return {\n",
        "        \"cluster_id\": cluster_id,\n",
        "        \"keywords\": most_common,\n",
        "        \"example\": representative_sentence,\n",
        "        \"sentences\": cluster_sents\n",
        "    }\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# Visualization (combined)\n",
        "# -------------------------------\n",
        "def visualize_crosslingual(word_mo, word_en,\n",
        "                           mo_embeddings, en_embeddings,\n",
        "                           mo_labels, en_labels):\n",
        "\n",
        "    reducer = umap.UMAP(random_state=42)\n",
        "    combined = np.vstack([mo_embeddings, en_embeddings])\n",
        "    reduced = reducer.fit_transform(combined)\n",
        "\n",
        "    n_mo = len(mo_embeddings)\n",
        "\n",
        "    plt.figure(figsize=(7,6))\n",
        "\n",
        "    # Mongolian points (circles)\n",
        "    for lbl in set(mo_labels):\n",
        "        idx = [i for i in range(n_mo) if mo_labels[i] == lbl]\n",
        "        plt.scatter(reduced[idx,0], reduced[idx,1],\n",
        "                    marker='o', label=f\"MO {word_mo} - Sense {lbl}\")\n",
        "\n",
        "    # English points (triangles)\n",
        "    for lbl in set(en_labels):\n",
        "        idx = [i+n_mo for i in range(len(en_embeddings)) if en_labels[i] == lbl]\n",
        "        plt.scatter(reduced[idx,0], reduced[idx,1],\n",
        "                    marker='^', label=f\"EN {word_en} - Sense {lbl}\")\n",
        "\n",
        "    plt.title(f\"Cross-Lingual WSI: '{word_mo}' ↔ '{word_en}'\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# RUN CROSS-LINGUAL WSI\n",
        "# -------------------------------\n",
        "for mo_word, en_word in targets:\n",
        "\n",
        "    print(\"\\n=======================================\")\n",
        "    print(f\"WORD PAIR: {mo_word} ↔ {en_word}\")\n",
        "    print(\"=======================================\")\n",
        "\n",
        "    # 1. Collect embeddings\n",
        "    mo_sents, mo_embs = collect_embeddings(mongolian_corpus, mo_word)\n",
        "    en_sents, en_embs = collect_embeddings(english_corpus, en_word)\n",
        "\n",
        "    if len(mo_embs) == 0 or len(en_embs) == 0:\n",
        "        print(\"Not enough contexts. Skipping.\")\n",
        "        continue\n",
        "\n",
        "    # 2. Cluster separately\n",
        "    mo_kmeans, mo_labels = cluster_senses(mo_embs, k=2)\n",
        "    en_kmeans, en_labels = cluster_senses(en_embs, k=2)\n",
        "\n",
        "    # 3. Compute cluster similarity\n",
        "    sim = compare_clusters(mo_kmeans.cluster_centers_,\n",
        "                           en_kmeans.cluster_centers_)\n",
        "\n",
        "    print(\"\\nCosine similarity between senses (MO x EN):\")\n",
        "    print(sim)\n",
        "\n",
        "    # 4. Show which Mongolian sense matches which English sense\n",
        "    best_align = sim.argmax(axis=1)\n",
        "    for i, j in enumerate(best_align):\n",
        "        print(f\" Mongolian Sense {i} ↔ English Sense {j} (similarity={sim[i,j]:.3f})\")\n",
        "\n",
        "    # 5. Visualize combined embeddings\n",
        "    visualize_crosslingual(mo_word, en_word, mo_embs, en_embs, mo_labels, en_labels)\n"
      ]
    }
  ]
}